Responses to Review #81A
===========================================================================

Responses to Weaknesses
-----------------------

> - The exact rewrite strategy produced for a given rewrite system
>   is not entirely clear to me.

The rewrite strategy produced is a slight variant on running a single
bottom-up pass with eager βι-reduction:

- Any rule tagged with `do_again`, on success, triggers an extra
  bottom-up pass on the output of that rewrite rule.  This repetition
  is fueled, with the fuel currently hard-coded to the number of
  rewrite rules (which works well when the rules form a dag), but this
  could easily be a parameter of the rewriter.

> - The supported language of goals and rewrite rules is rather
>   restricted compared to the one supported by Malecha and Bengston's
>   reflexive rewriter, which can deal with quantified formulas for
>   example and more general side conditions.

In fact our rewriter could support both of these if the user is
willing to put in the work to adjust the encoding by hand.  We do not
advertise support for these because the convenience machinery, which
allows users to make use of our rewriter with nearly the same ease
that they could use `autorewrite`, does not support them.

There are in fact a couple of features of the rewriter of Malecha and
Bengston which would require fundamental design changes for us to support:

- setoid relations (we only use pointwise equality)

- supporting dependent types and computation at the type level (I'm
  actually not sure if they support this, but it seems plausible)

- matching application patterns that are not in eta-long form (such as
  rewriting `flat_map f` without having it applied to a list)

- matching application patterns where the head symbol is a unification
  variable


There are a couple more features that they support which we could
support with only relatively straightforward engineering work:

- repeatedly rewriting until there are no more matches, using fuel

- supporting non-linear patterns out-of-the-box (this would require
  some tweaking to the PHOAS types to support deciding equality of
  terms)


Finally, there are the things that we don't claim support for just
because our convenience code doesn't support them, but which are
supported in principle by the core reflective framework:

- discharging side-conditions through other reflective tactics

- supporting arbitrary type families

- support for quantified formulas

Roughly, the first one merely requires reifying the rewriting lemma by
hand, rather than using the convenience machinery; the success or
failure of rewrite replacement can then be conditioned on the result
of running another reflective procedure which decides the
side-condition.

Supporting arbitrary type families just requires extending the
datatype of non-base-types; the parts which we did not know how to
automate were (a) automatically turning the eliminator on a container
type into an eliminator in the UnderLets monad (more precisely, in the
NbE-value type augmeted with the UnderLets monad), and (b) generating
the section-retraction commutation of the container type and the expr
type (i.e., automatically generating the functions `list (expr T) →
expr (list T) → option (list (expr T))`, `expr A * expr B → expr (A *
B) → option (expr A * expr B)`, `option (expr T) → expr (option T) →
option (option (expr T))`, etc).  We'd be excited to know about a
general theory that handles this sort of thing.

Supporting quantified formulas would merely require tweaking the
current convenience code to treat `Prop`-valued things as term-like
rather than type-like, and adding a bit of pre-processing for `forall`
to turn it into a constant analogous to `ex`.  This would not be very
useful, though, unless users were assuming propositional
extensionality, because we can only rewrite with pointwise equality.
Nevertheless, we're not aware of any barriers in theory.


To sum up, as I understand it, Malecha and Bengston's rewriter is
parameterized over a user-supplied type of symbols for types and
terms, and the user needs to reify the lemmas themselves.  If rather
than relying on our plugin, our user writes the type codes by hand,
then we can quite easily support a much richer language of types.
Similarly, if the user codes the reified rewrite rule by hand rather
than relying on our preprocessor, then it's quite trivial to solve
side conditions with arbitrary other reflective tactics (though we
don't provide infrastructure for sequencing them).  So the differences
here are that (a) we provide much more convenience to the user, and
the cost of this is that we lose some of the expressivity that is
available by writing the lower-level versions by hand, and (b) their
rewriter fits into a much larger framework of reflective tactics that
they can use for discharging side-conditions, while our framework is
just the rewriter.


Finally, there are a couple of features that we support that we
believe Malecha and Bengtson do not:

- sharing preservation

- a rewriting strategy that allows fine-grained control of which
  subterms are re-walked looking for more rewriting

- automatically specializing the reflective language to the rewriting
  problem in a way that allows drastic speedups (I haven't seen this
  anywhere in the literature)

- constructing the reflective tactic and applying it with syntax
  that's approximately as simple as the built-in autorewrite and
  doesn't require that the user know about reflection

- interleaving βι reduction with rewriting (I'm actually not sure if
  they support this; the paper doesn't seem to say)

The first one, in particular, is quite essential in the Fiat-Crypto
example, as not having native support for lifting let-binders results
in asymptotically worse performance.  This is already apparent in the
LiftLetsMap / "Binders and Recursive Functions" example in the paper.
We achieve linear performance in `n×m` when you ignore the final cbv;
`setoid_rewrite` and `rewrite_strat` are both cubic.  The rewriter of
Malecha and Bengtson cannot possibly achieve better than O(n×m²)
unless it can be sublinear in the number of rewrites.  This is because
our rewriter gets away with a constant number of rewrites (four), plus
evaluating recursion principles for a total amount of work O(n×m).
But without primitive support for let-lifting, you instead need to
lift the lets by rewrite rules, and you end up needing to perform
O(n×m^2) rewrites just to lift the lets.  The analysis is thus:
running `make` simply gives us `m` nested applications of `map_dbl` to
a length `n` list.  To reduce a given call to `map_dbl`, you must
first lift all existing let-binders (there are n×k of them on the
k-innermost-call) across map_dbl, one at a time.  Then the map_dbl
adds another n let binders.  So we end up doing $\sum_{k=0}^{m} n×k$
lifts, i.e., n×m(m+1)/2 rewrites just to lift the lets.



Responses to Comments for author
--------------------------------
> L93 "apostropher mark" does that refer to l' on L97?

We mistakenly left this sentence in from a previous version of the
example.  The apostrophe mark in fact does not show up until the
example on line 737--738, where `?l` and `?u` are prefixed with `'` to
denote that they are literals.  It shows up again on line 1371 where
we prefix `n` with `'` to indicate that `List.repeat` should only be
simplified when the length of the output list is a ground term.

> L219: "before reaching normal forms".
>    By strong normalization, you can always reach the normal form
>    of an open term w.r.t. the reduction rules making up
>    definitional equality.
>    You seem to be referring to normal forms w.r.t. a larger rewrite
>    system here, please be more specific.

We had a mistaken understanding of the technical meaning of "normal
forms"; the setence should read "before reaching ground terms" or
"before reaching ground terms".

> L623: Maybe you can just say that reify and reflect are structurally recursive
>   on types and reduce on the expression?

Sure.

> L557 Let binders are not included here, unlike the grammar L359, why?

This is an oversight, they should be included, thanks.

> L762 Explains that you use a constant to represent them, it would be
>   good to give a hint earlier about this encoding trick.

We actually use this constant on the Gallina side, not on the expr
side.  They are, in fact, hard-coded into the expr type.


> L709 Again we did not see any example of this syntax. Maybe you
>   can provide an example here?

This is the apostrophe mark described first on L93.  There is an
example on L737--738 and L1371, but indeed we should provide the
example here of `∀ n1 m n2, 'n1 = 'n2 → 'n1 + m - 'n2 = m`.

> L710. If I understand correctly this means that `Expr t` contains
>     a boolean type among the base types, and certainly if-then-else and
>     constants as well. Can this be added simply by updating the denotation
>     of base types and constants (`denote_base_type` and `denoteI`)?

This is actually not required.  The boolean expression and the
if-then-else lives at the Gallina level.  Rewrite rules return an
optional expression, so, for example, te aforementioned `∀ n1 m n2,
'n1 = 'n2 → 'n1 + m - 'n2 = m` rule would be encoded in pseudo-Gallina
as something like:

```
{| pattern := '?1 + ?2 - '?3
 ; replacement := λ (n1 : nat) (m : expr nat) (n2 : nat),
       if Nat.eqb n1 n2
       then Some m
       else None |}
```

The transformation into a boolean statement happens entirely during
reification, and the reflective procedure cannot inspect the condition
in any way.  (This, too, is how general side-conditions could be
handled, by replacing the condition of the `if` statement with any
reflective procedure.)


> L762 If Coq's `let ... in ..` are not reified as `Let_In`, are they then
>     reduced during reification?

Yes, but this behavior is tweakable using a preprocessing tactic
(which can, for example, replace `let ... in ...` with `Let_In
... ...`).

>     I was a bit confused here because it is not clear in which world `Let_In`
>     is leaving, as a Coq identifier or as something used only by the reification/
>     interpretation. IIUC, you define a Coq constant
>     `Let_In: forall {A B} (a : A) (b : A -> B) := b a` that is used to
>     represent all lets in your goals and definitions to be reified?

Yes.

>     This `Let_In` application is then reified simply as an `App (Ident "Let_In") ...`
>     in the deeply embedded language?

No, there is a special constructor of `expr` that encodes this
constant.  But the fact that this is not just an `Ident` is largely a
convenience choice, and is not integral to the design.

>     Interpretation of `UnderLets` in the semantic domain will hence target that
>     constant too?

Yes.

>     Please provide a more detailed presentation of the treatments of lets, it
>     sounds unnecessarily vague as it is.

Undoubtedly some of this confusion is due to our attempt to present a
sufficiently simplified version of the rewriter so as to be
comprehensible.  It seems we simplified too far, or else
inconsistently.


> L883: I'd like to know here what `cbv` does when it encounters
>     `Let_In`, do you make it so that it does not reduce it,
>     and only recurses in the value and body of the let independently?

We pass `cbv` an explicit whitelist which does not include `Let_In`,
hence indeed recursion is performed independently in the value and
body.  The source of quadratic behavior is explained by Pierre-Marie
Pédrot in
https://github.com/coq/coq/issues/11151#issuecomment-557435656.


> L929: What exactly happens in the extracted OCaml version? You are
>   running the compiled rewriter extracted to OCaml applied to the
>   extraction of the deeply-embedded language?

That's correct.  We define a constant which is the application of the
rewriter to a particular reified constant, and then apply a
transformation which produces a string or other flat term (as the
OCaml runtime does not normalize under binders).  We then extract this
constant, compile it, and run the program.

>   Why didn't you also compare the native compilation mode for
>   the prime example? It would also avoid the problem of doubly
>   interpreting of the VM.

The reification of the prime examples is quite large, and the native
compiler is quite bad at large input terms, and spends most of its
time in compilation.  The comparison is not worth making without
having access to the fine-grained details of native timing (available
with `Set NativeCompute Timing` in 8.12 but not before).  For a point
of comparison, synthesizing the approximately 5 lines of code for
unsaturated addition on the prime 2^255-19 with 5 limbs on a 64-bit
machine takes 0.225s in the VM, 4.6s in cbv, 6.8s in lazy, and fails
with a stack overflow after running for 168s in the native compiler.
The smaller example of addition for 2^127-1 with 2 limbs on 64 bits
takes 0.056s in the VM, 0.519s in cbv, 0.57s in lazy, and the native
compiler again fails with a stack overflow after 173s.


> Typos:
>
> L883: refied

Thanks!  Now fixed.


Responses to Questions for the response period
----------------------------------------------
> - There seems to be no handling of "alien" terms (of "alien" type)
>   in the reification/interpretation setup? Is it a difficulty that you
>   side-step or can it be supported easily? Is it the purpose of the
>   extra idents parameter of the Make command? In that case, the tactic
>   would not be able to reify goals mentioning variables of alien types
>   then which cannot be mentioned as extras?

That's correct.  One of the selling points and, we believe,
innovations of this framework is that (a) reflective syntax can be
automatically specialized to a particular domain on-the-fly with minor
but non-trivial overhead, and (b) performing this specialization can
be looked at as part of a form of pre-compilation of rewrite databases
and allows significant speed-ups (about 2×, in our experience) by
reducing identifier lookup to Gallina pattern matching.

We could parameterize the entire framework on extra "alien" terms and
types.  Indeed this is the point of the `extra idents` parameter of
`Make`.  We don't currently have any examples where there are types in
the goal which cannot be added to the `extra ident` list, and hence
don't currently see a pressing need to add direct support for this.
Note also that supporting alien terms and types would would result in
significant overhead unless very carefully engineered (to take a
trivial example of this problem, we require decidable equality between
constants/identifiers, and `decide equality` is cubic, if I recall
correctly).

> - The handling of LetIn is a bit strange: we switch between
>   a grammar with a LetIn constructor to a presentation of `expr`
>   which doesn't, but looking at the code (L588 of Rewriter/Language.v),
>   `LetIn` does appear in the `expr` type which is interpreted to `dlet`,
>   a notation for a `Let_In` constant. In the same file, L774, the
>   preprocessing step of reification apparently inlines dependent lets
>   or turn them into beta-redexes (depending on the dependency),
>   so let-ins are not preserved by reification? Please clarify the situation.

Appologies for the lack of clarity.  The presentation of `expr` in the
paper should contain a `LetIn` constructor.  Indeed currently `let
... in ...` are inlined and `Let_In`s / `dlet ... in ...` are kept.
This is quite easily configurable, though, by changing the relevant
line of `reify_preprocess` in `src/Rewriter/Language/Language.v`, or
indeed just overwriting the preprocesser with a different one using
Ltac's reassignment (`::=`) feature.

> - The framework requires no properties of the rewrite system
>   except for semantics preservation, as far as I understand.
>   In particular you don't guarantee that the final terms
>   cannot be rewritten anymore by the rewrite rules, as
>   `rewrite_head` only performs one rewrite at the head.
>   Similarly, you use the `do_again` option to force the
>   rewriter/reducer to perform an additional bottom-up pass.
>   So, do I understand correctly that the NbE algorithm
>   produces normal forms w.r.t. β, but it does not necessarily
>   compute a normal form w.r.t. the rewrite rules?
>   Does it also mean that the rewriter is resistant to looping
>   rewrite rules like commutativity, e.g. `?x + ?y -> y + x`?
>   Finally, if my assumption is correct, to compare with
>   other rewriting strategies like `rewrite !` which loop
>   until no rewrite matches appear, don't you need to
>   repeatedly call your rewriter to obtain the same simplified
>   terms in general?

Yes, that is all correct.  (And commutativity rules will only loop if
you mark them with `do_again`.)  On the particular examples where we
compare, a single bottom-up pass (modulo the `do_again` rules) is
indeed sufficient to fully normalize the term.  For goals like
quantifier-lifting or reassociation, where a single bottom-up pass is
not sufficient, you would indeed need to repeatedly call the rewriter
(guarded by some form of progress check).  This is not too hard to
implement, but we have not implemented it.


Responses to Review #81B
===========================================================================

Responses to Comments for author
-------------------
> The paper presents a Coq library for building verified partial evaluators and shows that it performs better than primitive Coq reduction on examples such as FIAT crypto. The library is carefully designed to preserve subterm sharing.

> Though I understand the scheme at a high-level, I find it hard to understand the research contributions and how all the pieces in the paper fit together. NBE has been implemented in Coq (Boespflug et al.) and in other proof assistants, so is the contribution here to make it usable from a tactic in a way it can be proof-checked?

> Most of the techniques used in the paper are already well-known (pattern matching using decision trees, Nbe, PHOAS), the paper has put them together to make them perform better in a verified way, however, it is not evaluated well.There is only a single macro-benchmark, leaving open its applicability to other domains. The tweaks for discharging side-conditions in 4.3 and 4.4 also seems specific to that single domain.

> There are other details that the paper glosses over, e.g. around line 618, what is the correctness theorem for pattern matching and how does it show up in the proof of this lemma? There are several key places in the paper where the details are left to the Appendix, which is surprising given that the paper does not use its full budget.

> Overall I think the paper could do a better job at explaining the main contributions and the technique. It is not quite convincing in its current form.

TODO: I'm not sure what to say, I basically agree with this reviewer.
The new contributions, as I see them, are (a) actually assembling all
the pieces, and (b) the approach of creating on-the-fly a datatype of
constants as a sort of early compilation phase.  The aforementioned
paper of Boespflug et al. ("Efficient normalization by evaluation") in
fact discusses that representing constructors of the term language as
constructors of the metalanguage is one of the best optimizations they
found, but seems to go on to misinterpret their own performance
results.  Section 3.2 says "Replacing the constructor names with
integers rather than strings to avoid string comparison cost does
spare some computation, but it is better to avoid the \emph{Const}
constructor altogether. Rather than representing a datatype as an
in-memory tree, with \emph{App} constructors at branch nodes and
\emph{Const} constructors at the leaves, each in its own memory cell,
it is much more memory efficient to add all data constructors found in
the syntax as additional constructors to the metalanguage
interpretation, effectively flattening the representation in memory."
However, as I understand it, this memory optimization is already
effectively present in the evalapply benchmark, which in most cases
shows not nearly as significant a gain as the constructors benchmark.
Instead, as suggested by the performance numbers on page 14 of the
Aehlig et al. paper, I expect that most of this gain actually comes
from not using strings as constant names.  Alas, Boespflug et
al. neither include benchmarking results for replacing strings with
integers without going all the way to constructors, and apparently nor
did they include this optimization when benchmarking the strategy of
Aehlig et al.  While they correctly describe how making constructors
of the object language into constructors of the metalanguage breaks
modularity, their proposal to recover modularity is about the memory
layout, not about the encoding of constructor identity.  Furthermore,
the proposal of using integers works fine for untyped NbE, but incurs
significant overhead (proportional to the size of the term times the
number of constants) when defining typed NbE.  (Whether rewriting is
better performed with typed or untyped NbE is a different question,
not addressed in our paper.)  Our contribution (b) from the beginning
of this paragraph is to provide a way to have constructors in the
object language be represented as constructors in the metalanguage, in
a typed NbE setting, without breaking modularity nor incurring extra
work on the user, for a minor but non-trivial up-front time cost.

> Some other comments:
>
> - Please explain the notation [| E |] used on line 618 (or better show the Coq statement of the lemma?).

The notation `[| E |]` stands for `DenoteE E`, with `DenoteE` defined
on lines 582--583.  Sorry for not making this clear.

> - The paper could also elaborate/simplify the steps in 2.1 with the help of a small running example.

We agree that a small running example would help elaborate the steps
in 2.1, but we disagree that this would be a simplification.  In
particular, the framework is sufficiently intricate that even an
example with no lemmas to rewrite involves emitting nearly a hundred
definitions.  Nevertheless, we will consider including it.  Here is
such an example:

We use the example of subsection 1.1, where we write:
\begin{minted}[fontsize=\small]{coq}
Make rewriter := Rewriter For (zero_plus, plus_zero, times_zero, times_one, eval_map,
  eval_fold_left, do_again eval_length, do_again eval_combine,
  eval_rect nat, eval_rect list, eval_rect prod) (with delta) (with extra idents (seq)).
\end{minted}

\begin{enumerate}
\item
  The given lemma statements are scraped for which named functions and types the rewriter package will support.
  In this case, the lemma statements are \mintinline{coq}{zero_plus}, \mintinline{coq}{plus_zero}, etc.; the types are \mintinline{coq}{nat}, \mintinline{coq}{list}, and \mintinline{coq}{prod}; and the named functions are \mintinline{coq}{Nat.add}, \mintinline{coq}{Nat.mul}, \mintinline{coq}{List.map}, \mintinline{coq}{List.fold_left}, \mintinline{coq}{List.length}, \mintinline{coq}{List.combine}, \mintinline{coq}{nat_rect}, \mintinline{coq}{list_rect}, \mintinline{coq}{prod_rect}, and the extra identifier given \mintinline{coq}{List.seq}.
\item
  Inductive types enumerating all available primitive types and functions are emitted.
  In this example, the inductive types are approximately
\begin{minted}{coq}
Inductive base := nat | list (_ : base).
Inductive ident := Nat_add | Nat_mul | List_map | List_fold_left | List_length | List_combine | nat_rect | list_rect | prod_rect | List_seq.
\end{minted}
  In fact this list is not accurate, because some types are forced to be included (\mintinline{coq}{nat} and \mintinline{coq}{bool}) while others have hard-coded support separate from the list of base types (\mintinline{coq}{list}, \mintinline{coq}{option}, \mintinline{coq}{unit}), and the list of identifiers is pre-filled with some hard-coded support necessary for making the rewriter work at all.
  The true inductive types generated are:
\begin{minted}{coq}
Inductive base : Set :=  base_nat : base | base_bool : base.
Inductive ident : type (base.type base) → Type :=
| ident_seq : ident (type.base (base.type.type_base base_nat) -> type.base (base.type.type_base base_nat) -> type.base (base.type.list (base.type.type_base base_nat)))
| ident_combine : ∀ t t0 : base.type base, ident (type.base (base.type.list t) -> type.base (base.type.list t0) -> type.base (base.type.list (t * t0)))
| ident_length : ∀ t : base.type base, ident (type.base (base.type.list t) -> type.base (base.type.type_base base_nat))
| ident_fold_left : ∀ t t0 : base.type base, ident ((type.base t -> type.base t0 -> type.base t) -> type.base (base.type.list t0) -> type.base t -> type.base t)
| ident_map : ∀ t t0 : base.type base, ident ((type.base t -> type.base t0) -> type.base (base.type.list t) -> type.base (base.type.list t0))
| ident_succ : ident (type.base (base.type.type_base base_nat) -> type.base (base.type.type_base base_nat))
| ident_mul : ident (type.base (base.type.type_base base_nat) -> type.base (base.type.type_base base_nat) -> type.base (base.type.type_base base_nat))
| ident_O : ident (type.base (base.type.type_base base_nat))
| ident_add : ident (type.base (base.type.type_base base_nat) -> type.base (base.type.type_base base_nat) -> type.base (base.type.type_base base_nat))
| ident_literal : ∀ t : base, match t with | base_nat => nat | base_bool => bool end → ident (type.base (base.type.type_base t))
| ident_nil : ∀ t : base.type base, ident (type.base (base.type.list t))
| ident_cons : ∀ t : base.type base, ident (type.base t -> type.base (base.type.list t) -> type.base (base.type.list t))
| ident_Some : ∀ t : base.type base, ident (type.base t -> type.base (base.type.option t))
| ident_None : ∀ t : base.type base, ident (type.base (base.type.option t))
| ident_pair : ∀ t t0 : base.type base, ident (type.base t -> type.base t0 -> type.base (t * t0))
| ident_tt : ident (type.base ())
| ident_prod_rect_nodep : ∀ t t0 t1 : base.type base, ident ((type.base t -> type.base t0 -> type.base t1) -> type.base (t * t0) -> type.base t1)
| ident_bool_rect : ∀ t : base.type base, ident ((type.base () -> type.base t) -> (type.base () -> type.base t) -> type.base (base.type.type_base base_bool) -> type.base t)
| ident_list_case : ∀ t t0 : base.type base, ident ((type.base () -> type.base t0) -> (type.base t -> type.base (base.type.list t) -> type.base t0) -> type.base (base.type.list t) -> type.base t0)
| ident_option_rect : ∀ t t0 : base.type base, ident ((type.base t -> type.base t0) -> (type.base () -> type.base t0) -> type.base (base.type.option t) -> type.base t0)
| ident_nat_rect : ∀ t : base.type base, ident ((type.base () -> type.base t) -> (type.base (base.type.type_base base_nat) -> type.base t -> type.base t) -> type.base (base.type.type_base base_nat) -> type.base t)
| ident_eager_nat_rect : ∀ t : base.type base, ident ((type.base () -> type.base t) -> (type.base (base.type.type_base base_nat) -> type.base t -> type.base t) -> type.base (base.type.type_base base_nat) -> type.base t)
| ident_nat_rect_arrow : ∀ t t0 : base.type base, ident ((type.base t -> type.base t0) -> (type.base (base.type.type_base base_nat) -> (type.base t -> type.base t0) -> type.base t -> type.base t0) -> type.base (base.type.type_base base_nat) -> type.base t -> type.base t0)
| ident_eager_nat_rect_arrow : ∀ t t0 : base.type base, ident ((type.base t -> type.base t0) -> (type.base (base.type.type_base base_nat) -> (type.base t -> type.base t0) -> type.base t -> type.base t0) -> type.base (base.type.type_base base_nat) -> type.base t -> type.base t0)
| ident_list_rect : ∀ t t0 : base.type base, ident ((type.base () -> type.base t0) -> (type.base t -> type.base (base.type.list t) -> type.base t0 -> type.base t0) -> type.base (base.type.list t) -> type.base t0)
| ident_eager_list_rect : ∀ t t0 : base.type base, ident ((type.base () -> type.base t0) -> (type.base t -> type.base (base.type.list t) -> type.base t0 -> type.base t0) -> type.base (base.type.list t) -> type.base t0)
| ident_list_rect_arrow : ∀ t t0 t1 : base.type base, ident ((type.base t0 -> type.base t1) -> (type.base t -> type.base (base.type.list t) -> (type.base t0 -> type.base t1) -> type.base t0 -> type.base t1) -> type.base (base.type.list t) -> type.base t0 -> type.base t1)
| ident_eager_list_rect_arrow : ∀ t t0 t1 : base.type base, ident ((type.base t0 -> type.base t1) -> (type.base t -> type.base (base.type.list t) -> (type.base t0 -> type.base t1) -> type.base t0 -> type.base t1) -> type.base (base.type.list t) -> type.base t0 -> type.base t1)
| ident_List_nth_default : ∀ t : base.type base, ident (type.base t -> type.base (base.type.list t) -> type.base (base.type.type_base base_nat) -> type.base t)
| ident_eager_List_nth_default : ∀ t : base.type base, ident (type.base t -> type.base (base.type.list t) -> type.base (base.type.type_base base_nat) -> type.base t)
.
\end{minted}
  We believe that this is long enough as to be confusing rather than illuminating.
\item
  Tactics generate all of the necessary definitions and prove all of the necessary lemmas for dealing with this particular set of inductive codes.
  Definitions include operations like Boolean equality on type codes and lemmas like ``all representable primitive types have decidable equality.''
  There are actually 69 definitions and lemmas generated during this step, and it's needlessly confusing to list all of them.
  They can be found by inspecting the records \mintinline{coq}{Rewriter.Language.IdentifiersBasicLibrary.Compilers.Basic.GoalType.package} and \mintinline{coq}{Rewriter.Language.IdentifiersLibrary.Compilers.pattern.ident.GoalType.package}.
  Note that the first of these has a couple of fields which are themselves records, and the definition of these records must also be inspected for the full picture.
\item
  The statements of rewrite rules are reified and soundness and syntactic-well-formedness lemmas are proven about each of them.
  Each instance of the former involves wrapping the user-provided proof with the right adapter to apply to the reified version.
  It is not clear that examples would help here.
\item
  The definitions needed to perform reification and rewriting and the lemmas needed to prove correctness are assembled into a single package that can be passed by name to the rewriting tactic.
  This step is semantically trivial but extremely verbose.
\end{enumerate}

Overall, it's not clear what this reviewer was hoping to get from a
worked out example.






> - The micro-benchmark 4(a) does not include the reification time, which seems a little strange given that reification is very much part of the technique.

The micro-benchmark 4(a) includes lines both with and without
reification.  The former is labeled "Our approach including
reification, cbv, etc." while the latter is labeled "Our approach
(only rewriting)".  The lines are quite close, and both are
significantly better than other approaches.  Moreover, as explained in
the benchmark of 4(c) (the only benchmark where the time of just
rewriting diverges significantly from the full time), more than 99% of
the overhead is due to *interpretation* in Coq's cbv tactic.

In all microbenchmarks, we have constructed the input so that it is
trivial to reify, and the overhead is negligable when there is any
real rewriting work to be done.  Reification time in the
macrobenchmark is significant due to the fact that we reify modulo δ
without sharing of subterms, but this is an artifact of the way that
we've integrated the rewriter into Fiat-Crypto.

Finally, as we explain at the end of the first paragraph of 5.1.1 on
lines 785--788, the particular method of reification is not integral
to the framework, and we have prior work demonstrating how to achieve
extremely fast reification.

> - Does the technique allow for compilation of P-384 in Fiat Crypto? The introduction mentions it, but the experiments never come back to it.

Yes.  P-384 is a prime close to 2^384, and figure 4(d) shows that our
benchmarks extend well past 2^400.  In fact, though, as described in
section 5.2, "in the course of running this experiment, we found a way
to improve the old approach for a fairer comparison."  This
improvement allows the old approach to compile P-384 (32-bit) in about
55 seconds.  We achieve about 16 seconds if you extract and compile
our code, or about 117 seconds if you run our code in Coq's VM.




Responses to Review #81C
===========================================================================

> That said, I doubt that even an expert could reproduce what they have
> done, based on this paper.  There's an overview in 2.1, aspects of
> which are fleshed out later, but a great deal is unspecified.  In a
> paper of this length that's probably inevitable.  But it's still sad.

This is true.  We have another 28 pages of content merely describing
the trickiest implementation and design choices, and the document
describing the full design of the core of the framework (not including
the code that makes the rewriter convenient to use, i.e., only
including the rewriter itself and the correctness theorem used in step
(3) of the second half of our nine-step approach in section 2.1) is a
bit over 40 pages.  Note that this document does not describe any of
the proof techniques, only the definitions and proof statements.
Documenting the full rewriter is undoubtedly a much larger effort, and
would not fit in a paper such as this.

> It's a description of a substantial and sophisticated engineering
> achivement, not a crisp, pearly idea.

Indeed, and this is unfortunate.  If there is a crisp, pearly idea to
be found, beyond assembling preexisting techniques, it is that in the
tradeoff between slow generic procedures and fast domain-specific
code, we can have our pie and eat it too by specializing the generic
reflective method to our specific domain and partially evaluating it
in a pre-compilation phase.  However, this paper is not about this
pearl, but about the larger system in which it is applied.

Responses to Comments for author
-------------------
Some small notes

> * Fiat Crypto.  Line 897 says you replicate Erbsen; and yet on line 55
>   you say you can do something they can't.  Which is right?  If they
>   can't do P-384, how is it done in deployed systems?  Do the deployed
>   systems use verified code?  If not, how does the verified code
>   compared with the (presumably hand-written) deployed code?  Please
>   be clearer about all this.

We say on lines 910--911, "[a]ctually, in the course of running this
experiment, we found a way to improve the old approach for a fairer
comparison."  This modification of their approach allows synthesis of
P-384.

Perhaps the following modifications would make the text clearer?  To
the first sentence ("[...] replicating the generation of
performance-competitive finite-field-arithmetic code for all popular
elliptic curves by Erbsen et al. [2019]"), there is an implicit "as
well as generating code for a number of curves they were unable to
generate code for".  The second sentence, "In all cases, we generate
essentially the same code as they did," should instead read "In all
cases where they succeeded in generating code, [...]".

The point of this comparison is that we are generating code from the
same starting point and with the same ending point, but we can handle
more and larger primes than could be handled in Erbsen et al. [2019].

> * Line 93.  Special marker.  Is this part of Coq already, or are you
>   inventing it?
>
> * Line 96 "Further marker".  Same question.

Both of these are just the identity function under a different name.  That is, we write
```
Definition literal {T} (v : T) := v.
Notation "' x" := (literal x).
```
for the apostrophe marker and we write
```
Definition eagerly {T} (v : T) := v.
```
for the further marker.
```

> * Line 106. I'm already.  I'm not a Coq user so I don't know
>   what plugins do.  Is do_again defined by you or built-in?  "request
>   that some rules trigger a bottom-up pass.."  Who implements that
>   request.  It's all very baffling.

Appologies for the confusion.

A Coq plugin allows us to extend not just the tactic language but the
vernacular language with new commands; we use it to create a single
command that the user can type which will define the needed
definitions and inductive types.

When we speak of "requests" and "triggers", we are speaking of
annotations which the user gives and which our rewrite procedure
inspects to determine what to do.  As a very simple example, you could
write a tactic which takes in a pair of a boolean and a lemma, and
rewrites with the lemma once when the boolean is false, but rewrites
repeatedly when the boolean is true.  This procedure could be written
in Ltac as
```
Ltac my_procedure argument :=
  lazymatch argument with
  | (true, ?lemma) => repeat rewrite lemma
  | (false, ?lemma) => rewrite lemma
  end.
```
You could say that passing the true boolean is a request to the
procedure which triggers repeated rewriting, and this is in fact
almost exactly how our rewriter functions.

To answer the last part of this question in full detail, we define
`do_again` as another identity function:
```
Notation do_again := (@id (@snd bool Prop (pair true _))) (only parsing).
```
(Note the use of `pair true`, which is us annotating the lemma with a
boolean indicating that rewriting should be repeated.  We perform the
annotation as an argument to the identity function merely for
convenience of recognition by our tactic.)


> * Line 226 little examples like this, illustrating what the perf
>   challenges are, are really helpful.  More please.

Glad to hear this!  We like small examples like this too, but they
tend to be notoriously hard to come by, especially in Coq and other
proof assistants where there is a great deal going on under the hood,
and it's not always clear where the bottlenecks are.  We'd be
interested in knowing if there are specific other parts of the paper
that could benefit from examples like these.
