PLDI 2020 Paper #226 Reviews and Comments
===========================================================================
Paper #226 A Framework for Building Verified Partial Evaluators


Review #226A
===========================================================================

Overall merit
-------------
B. I support accepting this paper but will not champion it.

Reviewer expertise
------------------
Z. Some familiarity

Paper summary
-------------
This paper presents a new approach for partial evaluation in Coq. The
goal is to generate more efficient extracted code using partial
evaluation, but most existing partial evaluators for verified code
either increase the size of the trusted code base or cannot scale to
large programs. The new approach combines a different term encoding with
verified (potentially conditional) optimizations. The new framework is
capable of extracted efficient implementations of larger elliptic curve
crypto primitives than previous work.

Comments for author
-------------------
This is a well-written paper that was relatively easy to follow for me,
a total non-expert in interactive theorem proving. It walks the reader
through the motivation for a better partial evaluator (Fiat Crypto
couldn't extract code for larger curves), the desiderata, and the
solutions for each problem that arises. It's hard to argue with the
results -- the new extraction process is much faster, allowing it to
generate code for curves that the old implementation couldn't.

As a non-expert, one challenge I had was that this paper reads more like
a Coq experience report than a new conceptual contribution. There were a
number of places in the paper where I (and, it sounds like, the authors)
wondered if all this effort was just to work around some limitations of
the Coq implementation. Concretely, there are several points (like the
end of Section 3) where it seems like things would have been much easier
if not for termination checking; then, later on, it's revealed that we
got lucky(?) and the chosen PHOAS representation satisfies the
termination checker without any effort. I wonder how much those in the
PLDI audience who don't hack a lot of Coq will be able to learn from
this paper.

Section 3.1 discusses the need for efficient pattern matching for a
partial evaluator, and how pattern matching in Coq's logic isn't very
sophisticated, so the authors rolled their own based on [20]. That makes
sense. However, Section 5.2 concedes that actually, the performance of
the partial evaluator running inside Coq is pretty poor. To get that
performance back up, we need to extract the evaluator to OCaml and run
it natively. It seems like if we're going to make that concession, we
don't need to worry much about the efficiency of the pattern matching
engine -- we can just delegate to OCaml's, just like [1]?

Something I was unclear about throughout the paper was what the user
experience is for a tool like this. One of the cool things about LMS was
that it was trivial to adopt -- just changing the types of the code you
wanted to stage was enough to get the partial evaluation effect. I'm
less clear on the story here -- if I've written and proven an
implementation of some code, and now want to specialize it to a
particular input, what exactly do I have to write and/or do? Section 4.4
left me wondering how often I would actually need to do a lot more than
just write down some rewrite rules -- it seems like depending on the
problem, I would need to (1) write a new abstract interpretation, (2)
prove that correct, and (3) write new rules with the results of that
interpretation in mind. How laborious is that effort?

Post author-response comments
-----------------------------
Thanks for a well-written and helpful response. Ultimately, as a non-expert, I needed more help understanding how this paper would appeal to the PLDI audience broadly. In the paper's current form, I had a hard time distinguishing conceptual insights from Coq engineering tricks.



Review #226B
===========================================================================

Overall merit
-------------
A. I will champion accepting this paper.

Reviewer expertise
------------------
Y. Knowledgeable

Paper summary
-------------
The paper addresses the problem of performing rewrite-intensive program transformation in the context of a provably semantics-preserving system, with proofs generated in checked with a proof assistant. Since type theory-based proof assistants, by design, have limited kernels, and the primitive operations therein are small (e.g. beta-reduction and unfolding of definitions), the proofs quickly become too large too manage and generating them infeasible. The approach combines several techniques to avoid expensive rewriting under binders (such as Coq’s setoid_rewrite) but still use the native kernel for lambda calculus normalization.

Comments for author
-------------------
This work takes a very practical standpoint: if verified toolchains and proof-carrying code are to become mainstream, then someone has to do something about those proof term sizes. The solution is very creative, too. In some sense, it’s even counterintuitive: implement another layer of embedding when what you’re striving for is performance? — but the paper eventually delivers and the way that it does that is clever, by capitalizing on existing normalization mechanics that are already built into Coq.

I find the treatment of the problem systematic and one that presents several different facets of proof-assistant engineering. Also, the writing is superb.

= Specific comments =

p.1 [75] “fancier reduction strategies grow the trusted code base” — this is a bit over-generalizing. They don’t necessarily do so, and I understand after reading this paper that it’s either that or really blowing up the proof size, but IMHO it should be stated as a tradeoff.

p.2 [213] “reduction included in .. need not be requested explicitly.” What does it mean for a reduction to be “requested”?

p.5 [457] “rely on no such embeddings” — it’s perhaps besides the point, but Coq’s extraction mechanism must rely on the semantics of OCaml, and probably any attempt to build a certified extraction procedure will need to have such embedding, right?

p.5 [498] “set of inductive codes” — what are these codes? Is it just an implementation detail?

p.5 [545] “does not need to look into definitions” — so this rewriting scheme never unfolds the definitions of symbols? Why is that an acceptable limitation?

p.6 [630] “Wildcard”. The holes are not named nor indexed. Is it irrelevant or are they implicitly indexed left-to-right?
p.6 [636] “App” — I am guessing the edge in the decision tree should read “Switch” (as in [609])? Or is “Switch” supposed to be written inside the circle?

p.6 [654] “match e with etc.” — Where did the vectors go in this expression? Were they all eliminated by Coq’s Compute or similar?

p.11 [1169] “including reification, cbv, etc.” — It is not clear how important these are, and whether the reader should consider them as part of the running time or not. Surely reification is a vital part of the process, so what is the significance of “only rewriting”?

p.11 [1205] “Macrobenchmark”: maybe it’s just me but I read this as “Microbenchmark” (following 5.1) and was confused.

Post author-response comments
-----------------------------
*Outcome of PC discussion*

While all the reviewers appreciated the effort involved in leveraging the agreeably weak beta-reduction in Coq’s kernel without resorting to OCaml implementation, the contribution is found to be too narrow and the vast majority of the PLDI audience will not be able to relate to it.



Review #226C
===========================================================================

Overall merit
-------------
B. I support accepting this paper but will not champion it.

Reviewer expertise
------------------
Z. Some familiarity

Paper summary
-------------
Partial evaluation is an optimization technique that aims at generating faster, specialized variants of a function by reducing its application domain. An example in cryptography is a specialization of a function to a certain key size. The resulting function is faster than the more generic function, but a proof is required to demonstrate that the behaviour of the new function is the same as the general function. This paper presents a way to efficiently prove the correctness of partial evaluators. Although previous approaches exist, they introduced a trusted component in the Trusted Computing Base (TCB), and had performance issue due to the lack of common sub-expression handling. This paper proposes an approach that does not add to the TCB and that takes common sub-expression into account to reduce the redundancy in generated proofs.

In order to prove correctness of partial evaluators, the authors have developed a Coq library, which works in two steps: first a rewriter is generated from equality lemmas, and then it can be applied to prove the equality of two functions or synthesize a specialized version of a function with its correctness proof. The paper presents a quick overview of the features of the library, followed by a more in-depth explanation of the techniques used to build it, comparing with existing implementations, as their work is heavily inspired by work by Aehlig et al. The paper evaluates the library speed compared to coq standard tactics on two microbenchmarks, and on a macrobenchmark involving code generation for the Fiat cryptography library.

Comments for author
-------------------
Positives
----------- 
- The paper is well-structured and easy to follow. 
- The authors are positioning well their work in the context of existing approaches and techniques that they use. The paper explains when they adapted them and justifies all of their choices. 
They also compared how their own use-case impacts existing approaches, and how their implementation overcomes the difficulty. Overall, the formalization flows almost naturally.

Negatives
---------------

- Through the paper there is a constant comparison between the introduced  approach and the work of Aehlig et al [1]. One expects that those two approaches will also be empirically compared and evaluated. 
- Although important to the presented work, side conditions (in Sections 4.3 and 4.4) are not as well explained and justified as the rest of the paper. 
- The abstract interpreter is not evaluated.


Suggestions
----------------------------------------------
- Figure 3 (benchmark results) is really hard to read and evaluate since the text is small and the graphs are barely readable. Slightly reducing the size of Figure 1 might help you get enough space for Figure 3.



Review #226D
===========================================================================

Overall merit
-------------
B. I support accepting this paper but will not champion it.

Reviewer expertise
------------------
Y. Knowledgeable

Paper summary
-------------
This paper proposes a new approach to do partial evaluation in Coq.  A large component of it is a term rewriter, which is used to simplify terms as knowledge about some parameters is given.  Unlike previous work by Achlig etal [1], where an operational semantics of an ML-like language for terms was defined via a deep embedding and had to be trusted as part of the overall proof checking, here the authors propose an approach where the language is defined at the core level via translation back and forth to Coq itself.  While this translation-based definition still needs to be given, the advantage is that Coq can now take over a lot of the checking work (like termination) and thus the overall trust base stays the same, without any need to modify or extend the proof checker.

Coq being Coq, a lot of challenges had to be overcome in order to achieve usable performance.  First, a less-than-naive rewrite engine had to be implemented, where some limited indexing across rewrite rules was used, to avoid doing work from scratch each time a rule is attempted to be matched.    Second, translations to Coq and normalization of evaluation of terms had to be implemented in a certain way, to convince Coq that everything terminates.  Third, a shallow encoding of terms' binders into Coq's existing binders had to be implemented, to avoid the cost of de Bruijn encodings.  Then subterm sharing and side conditions to rules were necessary, which are also considered basic features of and implemented in other rewrite engines outhere.

Evaluation was done both on microbenchmarks and on a larger example, the Fiat Crytocurrency.  The results show good results.

Comments for author
-------------------
The paper is overall well-written.  The choice of what to include in the paper and what in the appendix is fair, although there is a lot of material in the appendix that is required to appreciate the contribution.  Not sure what you can do about it, though, but at least make sure that it will be all available in a companion report if the paper is accepted.  I do not have any major questions, only a few minor ones that made me have some reservations about the paper and not give it an A.

Why not implement, once and for all in Coq, the efficient rewriting engine algorithms, based on indexing and pattern match automata, that the term rewriting community has developed over the years and implemented in truly fast rewrite engines, performing millions, or even tens of millions, or rewrite steps per second?  That would benefit the entire Coq community.

Why should one trust the rewrite rules as correct?  Don't they become part of the trust base anyway?  Also, isn't your term translation back and forth to Coq the same as like giving a semantics to an ML-like language like in [1]?  One still has to trust these translations, including the fact that the ML-like language and Coq share the semantics of their similar constructs., which to me counts as the same level of trust that [1] requires from their users.  You should clarify that in the paper.

Lines 701-704: this becomes part of the trust base, no?

How is the approach in [11] different from the approach here?  Is it only a performance improvement?  How about the trust-base?  I was a bit disappointed to read that you generate essentially the same code as [11] and you only care about the performance of the partial evaluator; it would have been a really nice application where correctness is guaranteed through Coq, but if the application already had all these guarantees, then the work in this paper feels less critical and more incremental.



Response by Jason Gross <jgross@mit.edu>
---------------------------------------------------------------------------
Thank you all for your feedback and comments.  We'll start by
clarifying a few most crucial elements of our approach and its
advantages, after which we'll return to answer further questions on a
review-by-review basis (some of which are forward-referenced by our
initial highlights).

First, a quick response to Review A's questions about what the user
experience is really like.  Section 1.1 does provide a complete
example; we are not eliding any additional user overhead of applying
our rewrite engine.  (Abstract interpretation is a separate activity
that we say a bit more about later, but this paper is not about
abstract interpretation, but rather a flexible framework that can
interface with abstract intepretation.)

Next: Review D revealed a number of places where we left it unclear
how we avoid growing the trusted base, beyond what one accepts anyway
in using Coq, and we're glad to have the opportunity to clarify.
Briefly:
- *Must one trust that the registered rewrite rules are correct*?
   No, the verified rewriting engine requires a proof that the
   interpretation of the AST of each rewrite rule is a correct
   proposition in the logic.
- *Must one trust the translations back and forth between normal Coq and our ASTs?*
  No, the translation *into* ASTs is inherently untrusted, as we ask
  the Coq kernel to "run it backwards" through interpretation of ASTs,
  for instance in formulating the semantic correctness condition for a
  rewrite rule. The kernel naturally checks that interpretation takes
  us back to the terms (e.g., rewrite-rule statements) we started
  with.
- *Are the central NbE function definitions (lines 701-704) trusted?*
  No, they receive full proofs of functional correctness (which are
  used as lemmas in the overall engine proof) and thus need not be
  trusted.
Summing up, in any particular use of our rewrite engine, the final
theorem statement proved truly does not mention or depend on any part
of our framework; only the *proof* depends on our framework.  Since we
do not add any axioms in this project, the trusted base is not
expanded.  And see our more detailed Review D response below for more.

Finally, a few brief remarks on performance, which indeed is at the
heart of our project's motivation. Review D expresses disappointment
that, in comparison to [11], our project is only about improving the
performance of a compilation pipeline; we should have emphasized that
the tool from [11] timed out (running longer than 100 hours) on many
important inputs, while our new version handles all known real-world
elliptic curves usually within a few minutes or at most two hours. Also,
Review A summarizes Section 5.2 as concluding that our engine's
performance is poor. We want to emphasize again that there we compare
against reduction on programs (from [11]) that have been
tailor-written in CPS just to improve reduction performance, while
our engine works on much more readable, direct-style code, in addition
to integrating flexible rewriting with reduction.  Still, our
within-Coq evaluation doesn't pass 90 seconds of running time until
primes grow to about 10 times as many bits as native machine words.

Responses to Review A
=====================

Regarding termination checking, indeed, normalization by evaluation in
PHOAS is, in some sense, specifically crafted to appease the
termination checker in an elegant way.

Broadly, none of the individual techniques are new.  What's new is
that we combine all of them to get a flexible verified tool that
operates efficiently at scale, without extending the trusted code
base.

> However, Section 5.2 concedes that actually, the performance of the
> partial evaluator running inside Coq is pretty poor.

While the performance is poor compared to the extracted version and is
not stellar compared to the "rewrite all the code in CPS" version
(which is inflexible), the performance is still quite good relative to
the prior state-of-the-art equational reasoning in Coq of
`setoid_rewrite` / `rewrite_strat`, which take around 90 seconds for
primes equal to the bitwidth, and about 3 hours for primes with twice
as many bits as the machine architecture.  By contrast, even within
Coq, our approach only approaches 90 seconds for primes with about 10
times as many bits as the machine architecture.  We'll aim to
emphasize this more in future versions of the paper.

> It seems like if we're going to make that concession, we don't need
> to worry much about the efficiency of the pattern matching engine --
> we can just delegate to OCaml's, just like [1]?

If we delegate to OCaml's pattern matching engine, then we can't do
conditional rewrite rules, because there's not currently a way to
extract Coq pattern matches to OCaml's conditional matches.
Additionally, we tried eliminating pattern-matching compilation from
our engine.  This results in out-of-memory errors during compile-time
(because we end up being quadratic in the number of rewrite rules for
reasons that are slightly hard to explain), and if we disable the
partial evaluation of rewrite rule selection (trusting in OCaml's
pattern matching engine), we lose approximately a factor of 8 in
performance of extracted code.


The intended user experience for a tool like this is detailed at the
end of section 1.1.  The couple lines of code described there are all
that is necessary for equational reasoning.  The work of abstract
interpretation was required for Fiat Cryptography to handle automatic
specialization from unbounded integers to fixed-width integers.  This
is more than just equational reasoning, and, indeed, was a somewhat
laborious process.  We have yet to find any better alternative to
this, however, when abstract interpretation is, in fact, required.


Responses to Review B
=====================

Thank you for your detailed comments about places where we were
unclear or misstating things.

Responding to the questions:

> p.2 [213] “reduction included in .. need not be requested
> explicitly.” What does it mean for a reduction to be “requested”?

Imagine a proof assistant without any computational reduction rules,
only axioms.  In such a system, every reduction step needs to
correspond to a use of an axiom in the proof term.  This is what we
mean by "requested".

> p.5 [457] “rely on no such embeddings” — it’s perhaps besides the
> point, but Coq’s extraction mechanism must rely on the semantics of
> OCaml, and probably any attempt to build a certified extraction
> procedure will need to have such embedding, right?

Yes.

> p.5 [498] “set of inductive codes” — what are these codes? Is it
> just an implementation detail?

The inductive codes are the inductive types from step (2), "Inductive
types enumerating all available primitive types and functions are
emitted."  For example, we might have the inductive codes for types
`Inductive base := nat_code | bool_code | Z_code` and the inductive
codes for functions `Inductive ident := nat_add | Z_add | bool_if`.
Details beyond this are basically just implementation details.

> p.5 [545] “does not need to look into definitions” — so this
> rewriting scheme never unfolds the definitions of symbols? Why is
> that an acceptable limitation?

The rewriter itself has no native support for unfolding definitions,
but you can include a rewrite rule that corresponds to an "unfolding",
such as `?n + S ?m = S (n + m)` or `fst (?x, ?y) = x`.

> p.6 [630] “Wildcard”. The holes are not named nor indexed. Is it
> irrelevant or are they implicitly indexed left-to-right?

Holes are indexed left-to-right, but this is an implementation detail;
the machinery for reifying Gallina rewrite rules into patterns and
replacements takes care of indexing.

> p.6 [636] “App” — I am guessing the edge in the decision tree should
> read “Switch” (as in [609])? Or is “Switch” supposed to be written
> inside the circle?

We should have been more clear that every un-labeled node is a
"Switch" node.  The "App" edges are for the `app_case` on [609], and
the identifier-labeled edges are part of `icases`.

> p.6 [654] “match e with etc.” — Where did the vectors go in this
> expression? Were they all eliminated by Coq’s Compute or similar?

Yes, they are all eliminated under `cbv` / `cbn` during the partial
evaluation described on lines [644]-[646].

> p.11 [1169] “including reification, cbv, etc.” — It is not clear how
> important these are, and whether the reader should consider them as
> part of the running time or not. Surely reification is a vital part
> of the process, so what is the significance of “only rewriting”?

The response to this is hinted at on [1120]-[1123]: "As the
reification method was not especially optimized, and there exist fast
reification methods [13]".  However, refication is not actually the
bottleneck.  The bottleneck is the final cbv at the end, and this is
only due to a bug in Coq's implementation of NbE reduction that makes
substitution quadratic in the number of
binders (we wish we could link to the bug tracker, but it might reveal
something about our identities).
While we felt it would be misleading to give only the "only rewriting"
numbers, we do believe this is really the benchmark that matters, Coq
bugs aside.  (And we hope this bug in Coq will be fixed soon.)


Responses to Review C
=====================

Thank you for your points; we will aim to incorporate your feedback in
the next revision of the paper.

One clarification we'd like to make is that the abstract interpreter
itself is beyond the scope of this project; we merely want to
demonstrate that it is possible to integrate separate abstract
interpretation passes with the partial evaluator and rewriting
framework.


Responses to Review D
=====================

> Why not implement, once and for all in Coq, the efficient rewriting
> engine algorithms, based on indexing and pattern match automata,
> that the term rewriting community has developed over the years and
> implemented in truly fast rewrite engines, performing millions, or
> even tens of millions, or rewrite steps per second?  That would
> benefit the entire Coq community.

Indeed, this would be a great next step.  We started with a somewhat
more modest goal, and already found the proof effort significant.

> Why should one trust the rewrite rules as correct? Don't they become
> part of the trust base anyway?

To be accepted by the engine, the rewrite rules must be proven in Coq.
So no unsound rewrites are permitted (unless you use unsound axioms to
prove them). The checking happens as part of the step from line 502 of
the paper, where we bring all the rewrite rules together in a list of
dependently typed records, each one pairing the syntax of a rewrite
rule with a required proof of its soundness, stated in terms of
interpretation of the reified syntax tree.

> Also, isn't your term translation back and forth to Coq the same as
> like giving a semantics to an ML-like language like in [1]?  One
> still has to trust these translations, including the fact that the
> ML-like language and Coq share the semantics of their similar
> constructs., which to me counts as the same level of trust that [1]
> requires from their users.  You should clarify that in the paper.

The translations in our tool are *not* trusted. This is because every
time our code reifies a term, the Coq kernel checks that the embedded
AST our code produced has the same semantics as the original term by
unfolding the interpreter applied to the AST and observing that it
results in the original term. Our rewriting transformation is proven to
be semantics-preserving, so the theorem we get at the end is that the
new term, post-rewriting, has the same semantics as the original term.
Since the kernel checks the correctness of the semantics of the initial
term, there's nothing additional here to trust. We should perhaps
clarify this in the paper.

> Lines 701-704: this becomes part of the trust base, no?

Thankfully, it does not.  In this case, reify and reflect are internal
operations which are not exposed in the end-to-end theorem (any moreso
than code for CompCert's intermediate phases appears in a generated
theorem witnessing correct compilation of a concrete C program), and
we prove the properties that we need of these operations.

> How is the approach in [11] different from the approach here?

> Is it only a performance improvement?

[11] relies on a delicate ad-hoc combination of hand-writing all of
the code in CPS to take advantage of built-in reduction for partial
evaluation, and then using a hand-rolled domain-specific rewriter to do
the necessary algebraic simplifications. The authors of [11] describe
the need to write all algorithms in CPS as "arbitrary and unfortunate."
Further, this two-stage approach requires reifying large blocks of code,
and reifying repeatedly for every prime, which was prohibitively slow in
many cases ([11] Appendix B).

The approach described here lets us reify the generic code once, and
combine all of the rewriting and partial evaluation into the same
generic framework. We believe this is much easier to use. Probably the
clearest example of this is that our rewriting and partial evaluation
can be applied to code that was written without considering our tool as
a target, whereas the code repository accompanying [11] contains two
versions of many functions: one human-readable, and one for partial
evaluation (which they prove equivalent each time).

The performance improvement is also essential, in the sense that some of
the primes that did not finish even after waiting nearly a thousand
hours in [11] finished for us in seconds, minutes, or, at most, an hour
or two.

> How about the trust-base?

Running our rewriter in Coq has the same trusted base as [11].  The
faster version of our code also relies on extraction to OCaml, but this
is not essential.

> I was a bit disappointed to read that you generate essentially the
> same code as [11] and you only care about the performance of the
> partial evaluator; it would have been a really nice application
> where correctness is guaranteed through Coq, but if the application
> already had all these guarantees, then the work in this paper feels
> less critical and more incremental.

Again, we would like to emphasize that many primes could not be
handled at all by [11] even with hundreds of hours and 64 GB of RAM.
The "performance improvement" here allows us to not just handle the
same primes faster, but also allows us to handle primes that could not
be handled at all.
