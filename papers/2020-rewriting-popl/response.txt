Thank you both for your feedback; we look forward to incorporating it
into future versions of this paper.

Responses to Reviewer A:
================

> How should one interpret ¬Wildcard p = something?

Ah, we clearly need to parenthesize better here.  The correct reading
of "if first ¬Wildcard p = something" is "if the index of the first
non-wildcard in p is something".

> Would it make sense to compare also the extracted code from the old
> approach?

This is quite hard.  OCaml does not reduce under functions.  Coq has a
`native_compute` strategy which is a modification of the OCaml runtime
to reduce under binders, but I have not yet found a way to get the
native compiler to report only the execution time, rather than also
the compilation time, and the compilation time is significant (often
incurring a 5x--10x overhead over the VM).

> Is not the `Let` constructor missing a binder in e₂?

The `v` in the `Let` constructor is bound in `e₂`, just as the `v`
bound via `λv` in the `Abs` constructor is bound in `e`.

Responses to Reviewer B:
================

> What is the speed of the generated code? Do you have performance
> results for that as well?

It's a bit unclear what is meant by "generated" here.  We have a table
describing the performance of reduction/rewriting, though we only
include relative performances when we should perhaps include absolute
numbers as well (which range from about a thousandth of a second all
the way up to about twenty seconds).  If you mean the runtime
performance of the code that results from performing
reduction/rewriting, we shall refer you to the Fiat Cryptography
paper; the code we produce with the new rewriting system does not
differ substantially from the code that was previously produced.  The
rewriting system is configurable on what rewrite rules you reduce
with, so the performance of the resulting Gallina code is not an
interesting metric.

> The evaluation section does not seem to show results for P-384.

P-384 is a prime close to $2^{384}$ and is included in the table,
though it is not obviously labeled.  On 64-bit P-384, the new method
takes a bit over half-a-second in extracted OCaml code, and about 8
seconds in the VM.  The old method in the VM takes a bit under 5
seconds, while the method actually previously used by Fiat
Cryptography (which used `cbv` because making the constants that were
to be held opaque was not an option) takes just under two minutes.  On
32-bit P-384 (the one where the old method timed out), the new method
takes a bit under 20 seconds in extracted OCaml code and about 107
seconds in the VM.  The old method stack overflows, but very naive
extrapolation suggests it would take a couple of minutes in the VM and
perhaps around an hour in `cbv`.

> "it takes Coq over 800 hours to verify that two terms are equal,
> even when our partially reduced code is only a dozen or so lines
> long. In our new framework, it takes only about 0.3 seconds to do
> the partial reduction on the given example"
>
> This doesn't seem to be an apples-to-apples comparison. You're
> comparing Coq checking for term equality to running your partial
> evaluator... those seem like different things... if they are really
> achieving the same thing, that deserves some explanation.

Indeed, they are different things, though quite related.  In order to
verify that two terms are equal, Coq will often partially unfold one
or the other.  If you are not principled in how you generate the terms
you are comparing, then Coq may get lost in the weeds unfolding terms
in the wrong order.  By having a principled system for reduction, we
can side-step most of the performance issues that arise when doing
partial reduction in an ad-hoc manner.

> Why don't you prove completeness? That seems rather important for
> correctness.

It is important for correctness of the rewriting system as a whole,
but it is not required for correctness of the code that is output.
Said another way, "do nothing" is always a sound rewriting strategy
(albeit a totally incomplete one), and to get correctness theorems
about the code we emit, we do not require (nor could we make use of) a
completeness theorem.

> I find disturbing this presentation style of giving wrong versions
> of things and then slowing correcting them. I'd prefer to see just
> one version that's correct.

Indeed, this was a conscious choice that we knew came with downsides.
The concern that we had is that the actually completely correct
version includes numerous subtle details which, while important for
things like performance of reduction during precomputation of the
rewriter itself, do not play an important role in understanding the
essential makeup of the system.  We may try to strike a better balance
in the next version of this paper, and we appreciate this comment, as
well as all the other comments and reactions.
