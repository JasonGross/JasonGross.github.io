@PhdThesis{zimmermann:tel-02451322,
  author = {Zimmermann, Théo},
  school = {{Universit{\'e} de Paris}},
  title  = {{Challenges in the collaborative evolution of a proof language and its ecosystem}},
  year   = {2019},
  url    = {https://hal.inria.fr/tel-02451322},
}

@Misc{coqpl-15-coq-bug-minimizer,
  author           = {Jason Gross},
  month            = jan,
  note             = {Presented at \href{https://coqpl.cs.washington.edu/2014/07/31/}{The First International Workshop on Coq for PL (CoqPL'15)}},
  title            = {Coq Bug Minimizer},
  year             = {2015},
  abstract         = {Are bugs the bane of your existence? Do you dread Coq upgrades, because they mean you'll have to spend days tracking down subtle failures deep in your developments? Have you ever hit an anomaly that just wouldn't go away, and wished you understood what triggered it? Have you ever been tormented by two blocks of code that looked identical, but behaved differently? Do you wish you submit more helpful error reports, but don't want to put in the time to construct minimal examples? If you answered ``yes'' to any of these questions, then the Coq Bug Minimizer is for you! Clone your own copy at \url{https://github.com/JasonGross/coq-bug-finder}.},
  modificationdate = {2014-10-07T00:00:00},
  owner            = {Jason},
  reviews          = {https://jasongross.github.io/papers/2015-coq-bug-minimizer-reviews.txt},
  url              = {https://jasongross.github.io/papers/2015-coq-bug-minimizer.pdf},
}

@InProceedings{Cleve2000,
  author        = {Holger Cleve and Andreas Zeller},
  booktitle     = {Proceedings of the Fourth International Workshop on Automated Debugging, {AADEBUG} 2000, Munich, Germany, August 28-30th, 2000},
  title         = {Finding Failure Causes through Automated Testing},
  year          = {2000},
  editor        = {Mireille Ducassé},
  archiveprefix = {arxiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/conf/aadebug/CleveZ00.bib},
  eprint        = {cs/0012009},
  primaryclass  = {cs},
  timestamp     = {Thu, 09 Jan 2020 16:10:06 +0100},
}

@Book{zeller2009programs,
  author    = {Andreas Zeller},
  publisher = {Elsevier},
  title     = {Why Programs Fail: A Guide to Systematic Debugging},
  year      = {2009},
}

@InProceedings{Burger2005,
  author    = {Burger, Martin and Lehmann, Karsten and Zeller, Andreas},
  booktitle = {Companion to the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  title     = {Automated Debugging in {Eclipse}},
  year      = {2005},
  address   = {New York, NY, USA},
  pages     = {184--185},
  publisher = {Association for Computing Machinery},
  series    = {OOPSLA '05},
  abstract  = {Your program fails. What is the cause of this failure? In this demo, we present two delta debugging plug-ins for the Eclipse environment which isolate failure causes in the program history and in the program's execution.},
  doi       = {10.1145/1094855.1094926},
  isbn      = {1595931937},
  keywords  = {testing, programming environments, program comprehension, debugging, version control},
  location  = {San Diego, CA, USA},
  numpages  = {2},
}

@Misc{delta,
  author   = {Daniel S. Wilkerson and Scott McPeak},
  month    = feb,
  note     = {Presented at \href{https://web.archive.org/web/20071224085116/http://www.codecon.org/2006/program.html}{CodeCon 2006}},
  title    = {{d}elta - Delta assists you in minimizing ``interesting'' files subject to a test of their interestingness},
  year     = {2006},
  abstract = {History: Scott and I were working on various static-analysis projects (our research group: http://osq.cs.berkeley.edu/). We had large inputs that would cause our tools to fail and minimizing by hand was hopeless, so I we wrote delta. With delta, no matter how big you start with, you always end up with about a page or two of code, even a quarter-million line input we tried once. Microsoft Research heard about it through the grapevine and asked someone to come to my office and ask me if I would release it as open source, so I did. Maybe they didn't want to write an email to me endorsing an open source project on record? I don't know. Now this thing is everywhere: it is taught in the Stanford and Berkeley software engineering classes and the gcc people use it. See the website.

Our implementation is based on the Delta Debugging algorithm: http://www.st.cs.uni-sb.de/dd/

Demo: I will probably minimize a file while wearing no clothes. Just kidding; I wouldn't actually minimize a file in public.

Future Plans: The gcc people seem to like it and one of them has checkin privileges so I suppose it will keep getting better. I have an idea to generalize the algorithm. Most people have ideas on how to make it work better for *their* use of it, but these schemes tend to make it worse in general.},
  url      = {https://github.com/dsw/delta},
}

@InProceedings{Zeller2002,
  author    = {Zeller, Andreas},
  booktitle = {Proceedings of the 10th ACM SIGSOFT Symposium on Foundations of Software Engineering},
  title     = {Isolating Cause-Effect Chains from Computer Programs},
  year      = {2002},
  address   = {New York, NY, USA},
  pages     = {1--10},
  publisher = {Association for Computing Machinery},
  series    = {SIGSOFT '02/FSE-10},
  abstract  = {Consider the execution of a failing program as a sequence of program states. Each state induces the following state, up to the failure. Which variables and values of a program state are relevant for the failure? We show how the Delta Debugging algorithm isolates the relevant variables and values by systematically narrowing the state difference between a passing run and a failing run—by assessing the outcome of altered executions to determine wether a change in the program state makes a difference in the test outcome. Applying Delta Debugging to multiple states of the program automatically reveals the cause-effect chain of the failure—that is, the variables and values that caused the failure.In a case study, our prototype implementation successfully isolated the cause-effect chain for a failure of the GNU C compiler: "Initially, the C program to be compiled contained an addition of 1.0; this caused an addition operator in the intermediate RTL representation; this caused a cycle in the RTL tree—and this caused the compiler to crash."},
  doi       = {10.1145/587051.587053},
  isbn      = {1581135149},
  keywords  = {tracing, testing, program comprehension, automated debugging},
  location  = {Charleston, South Carolina, USA},
  numpages  = {10},
}

@Article{coq-coq-correct,
  author     = {Sozeau, Matthieu and Boulier, Simon and Forster, Yannick and Tabareau, Nicolas and Winterhalter, Théo},
  journal    = {Proc. ACM Program. Lang.},
  title      = {Coq {Coq} {Correct}! Verification of Type Checking and Erasure for {Coq}, in {Coq}},
  year       = {2019},
  month      = dec,
  number     = {POPL},
  volume     = {4},
  abstract   = {Coq is built around a well-delimited kernel that perfoms typechecking for definitions in a variant of the Calculus of Inductive Constructions (CIC). Although the metatheory of CIC is very stable and reliable, the correctness of its implementation in Coq is less clear. Indeed, implementing an efficient type checker for CIC is a rather complex task, and many parts of the code rely on implicit invariants which can easily be broken by further evolution of the code. Therefore, on average, one critical bug has been found every year in Coq. This paper presents the first implementation of a type checker for the kernel of Coq (without the module system and template polymorphism), which is proven correct in Coq with respect to its formal specification and axiomatisation of part of its metatheory. Note that because of Gödel's incompleteness theorem, there is no hope to prove completely the correctness of the specification of Coq inside Coq (in particular strong normalisation or canonicity), but it is possible to prove the correctness of the implementation assuming the correctness of the specification, thus moving from a trusted code base (TCB) to a trusted theory base (TTB) paradigm. Our work is based on the MetaCoq project which provides metaprogramming facilities to work with terms and declarations at the level of this kernel. Our type checker is based on the specification of the typing relation of the Polymorphic, Cumulative Calculus of Inductive Constructions (PCUIC) at the basis of Coq and the verification of a relatively efficient and sound type-checker for it. In addition to the kernel implementation, an essential feature of Coq is the so-called extraction: the production of executable code in functional languages from Coq definitions. We present a verified version of this subtle type-and-proof erasure step, therefore enabling the verified extraction of a safe type-checker for Coq.},
  address    = {New York, NY, USA},
  articleno  = {8},
  doi        = {10.1145/3371076},
  issue_date = {January 2020},
  keywords   = {type checker, certification, proof assistants},
  numpages   = {28},
  publisher  = {Association for Computing Machinery},
}


@inproceedings{ochoa2022breakbot,
   author    = {Lina Ochoa and Thomas Degueule and Jean-Rémy Falleri},
   title     = {{BreakBot}: Analyzing the Impact of Breaking Changes to
Assist Library Evolution},
   booktitle = {44th {IEEE/ACM} International Conference on Software
Engineering: New Ideas and Emerging Results, {ICSE} {(NIER)} 2022},
   publisher = {{IEEE}},
   year      = {2022}
}

@Unpublished{zimmermann:hal-03479327,
  author      = {Zimmermann, Théo and Coolen, Julien and Gross, Jason and Pédrot, Pierre-Marie and Gilbert, Gaëtan},
  note        = {working paper},
  title       = {Advantages of maintaining a multi-task project-specific bot: an experience report},
  year        = {2022},
  hal_id      = {hal-03479327},
  hal_version = {v2},
  pdf         = {https://hal.inria.fr/hal-03479327/file/paper.pdf},
  url         = {https://hal.inria.fr/hal-03479327},
}

@InProceedings{quickchick,
  author      = {Paraskevopoulou, Zoe and Hriţcu, Cătălin and Dénès, Maxime and Lampropoulos, Leonidas and Pierce, Benjamin C.},
  booktitle   = {{ITP} 2015 - 6th conference on Interactive Theorem Proving},
  title       = {Foundational Property-Based Testing},
  year        = {2015},
  address     = {Nanjing, China},
  month       = aug,
  publisher   = {Springer},
  series      = {Lecture Notes in Computer Science},
  volume      = {9236},
  doi         = {10.1007/978-3-319-22102-1_22},
  hal_id      = {hal-01162898},
  hal_version = {v1},
  pdf         = {https://hal.inria.fr/hal-01162898/file/main.pdf},
  url         = {https://hal.inria.fr/hal-01162898},
}

@Article{chen_survey_compiler_testing,
  author     = {Chen, Junjie and Patra, Jibesh and Pradel, Michael and Xiong, Yingfei and Zhang, Hongyu and Hao, Dan and Zhang, Lu},
  journal    = {ACM Comput. Surv.},
  title      = {A Survey of Compiler Testing},
  year       = {2020},
  issn       = {0360-0300},
  month      = feb,
  number     = {1},
  volume     = {53},
  abstract   = {Virtually any software running on a computer has been processed by a compiler or a compiler-like tool. Because compilers are such a crucial piece of infrastructure for building software, their correctness is of paramount importance. To validate and increase the correctness of compilers, significant research efforts have been devoted to testing compilers. This survey article provides a comprehensive summary of the current state-of-the-art of research on compiler testing. The survey covers different aspects of the compiler testing problem, including how to construct test programs, what test oracles to use for determining whether a compiler behaves correctly, how to execute compiler tests efficiently, and how to help compiler developers take action on bugs discovered by compiler testing. Moreover, we survey work that empirically studies the strengths and weaknesses of current compiler testing research and practice. Based on the discussion of existing work, we outline several open challenges that remain to be addressed in future work.},
  address    = {New York, NY, USA},
  articleno  = {4},
  doi        = {10.1145/3363562},
  issue_date = {January 2021},
  keywords   = {test optimization, test program generation, Compiler testing, compiler debugging, test oracle},
  numpages   = {36},
  publisher  = {Association for Computing Machinery},
}

@InProceedings{herfert2017automatically,
  author    = {Herfert, Satia and Patra, Jibesh and Pradel, Michael},
  booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
  title     = {Automatically Reducing Tree-Structured Test Inputs},
  year      = {2017},
  address   = {Urbana-Champaign, IL, USA},
  pages     = {861--871},
  publisher = {IEEE Press},
  series    = {ASE 2017},
  abstract  = {Reducing the test input given to a program while preserving some property of interest is important, e.g., to localize faults or to reduce test suites. The well-known delta debugging algorithm and its derivatives automate this task by repeatedly reducing a given input. Unfortunately, these approaches are limited to blindly removing parts of the input and cannot reduce the input by restructuring it. This paper presents the Generalized Tree Reduction (GTR) algorithm, an effective and efficient technique to reduce arbitrary test inputs that can be represented as a tree, such as program code, PDF files, and XML documents. The algorithm combines tree transformations with delta debugging and a greedy backtracking algorithm. To reduce the size of the considered search space, the approach automatically specializes the tree transformations applied by the algorithm based on examples of input trees. We evaluate GTR by reducing Python files that cause interpreter crashes, JavaScript files that cause browser inconsistencies, PDF documents with malicious content, and XML files used to tests an XML validator. The GTR algorithm reduces the trees of these files to 45.3\%, 3.6\%, 44.2\%, and 1.3\% of the original size, respectively, outperforming both delta debugging and another state-of-the-art algorithm.},
  doi       = {10.1109/ase.2017.8115697},
  isbn      = {9781538626849},
  numpages  = {11},
}

@InProceedings{regehr2012test,
  author    = {Regehr, John and Chen, Yang and Cuoq, Pascal and Eide, Eric and Ellison, Chucky and Yang, Xuejun},
  booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
  title     = {Test-Case Reduction for {C} Compiler Bugs},
  year      = {2012},
  address   = {New York, NY, USA},
  pages     = {335--346},
  publisher = {Association for Computing Machinery},
  series    = {PLDI '12},
  abstract  = {To report a compiler bug, one must often find a small test case that triggers the bug. The existing approach to automated test-case reduction, delta debugging, works by removing substrings of the original input; the result is a concatenation of substrings that delta cannot remove. We have found this approach less than ideal for reducing C programs because it typically yields test cases that are too large or even invalid (relying on undefined behavior). To obtain small and valid test cases consistently, we designed and implemented three new, domain-specific test-case reducers. The best of these is based on a novel framework in which a generic fixpoint computation invokes modular transformations that perform reduction operations. This reducer produces outputs that are, on average, more than 25 times smaller than those produced by our other reducers or by the existing reducer that is most commonly used by compiler developers. We conclude that effective program reduction requires more than straightforward delta debugging.},
  doi       = {10.1145/2254064.2254104},
  isbn      = {9781450312059},
  keywords  = {test-case minimization, automated testing, compiler defect, random testing, bug reporting, compiler testing},
  location  = {Beijing, China},
  numpages  = {12},
}

@InProceedings{chen2013taming,
  author    = {Chen, Yang and Groce, Alex and Zhang, Chaoqiang and Wong, Weng-Keen and Fern, Xiaoli and Eide, Eric and Regehr, John},
  booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  title     = {Taming Compiler Fuzzers},
  year      = {2013},
  address   = {New York, NY, USA},
  pages     = {197--208},
  publisher = {Association for Computing Machinery},
  series    = {PLDI '13},
  abstract  = {Aggressive random testing tools ("fuzzers") are impressively effective at finding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine.},
  doi       = {10.1145/2491956.2462173},
  isbn      = {9781450320146},
  keywords  = {compiler testing, test-case reduction, automated testing, compiler defect, random testing, fuzz testing, bug reporting},
  location  = {Seattle, Washington, USA},
  numpages  = {12},
}

@InProceedings{holmes2016mitigating,
  author    = {Holmes, Josie and Groce, Alex and Alipour, Mohammad Amin},
  booktitle = {Proceedings of the 7th International Workshop on Automating Test Case Design, Selection, and Evaluation},
  title     = {Mitigating (and Exploiting) Test Reduction Slippage},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {66--69},
  publisher = {Association for Computing Machinery},
  series    = {A-TEST 2016},
  abstract  = {Reducing the size of tests, typically by delta debugging or a related algorithm, is a critical component of effective automated testing and debugging. Automatically generated or user-submitted tests are often far longer than required, full of unnecessary components that make debugging difficult. Test reduction algorithms automatically remove components of such tests, while preserving the property that the test fails. Unfortunately, reduction can sometimes transform a failing test that detects a subtle, critical, and previously unknown fault into a test that detects a trivial-to-find, unimportant, and already known fault. When reducing a test detecting fault(s) F produces a test that does not detect the same F, this is known as slippage. In the case where an interesting fault slips to an uninteresting fault, slippage is a problem, and must be avoided. However, slippage can also be beneficial, when a long test can be reduced to detect a fault that has not otherwise been detected (including by the original test). While traditional delta debugging only produces one reduced test, the concept of slippage suggests an alternative approach, where the output of reduction is a set of reduced tests, in order to avoid problematic slippage and induce beneficial slippage. In this paper, we present preliminary efforts to understand slippage, and compare two approaches to slippage mitigation.},
  doi       = {10.1145/2994291.2994301},
  isbn      = {9781450344012},
  keywords  = {delta debugging. slippage, test manipulation and reduction},
  location  = {Seattle, WA, USA},
  numpages  = {4},
}

@InProceedings{emi,
  author    = {Le, Vu and Afshari, Mehrdad and Su, Zhendong},
  booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  title     = {Compiler Validation via Equivalence modulo Inputs},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {216--226},
  publisher = {Association for Computing Machinery},
  series    = {PLDI '14},
  abstract  = {We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations.To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed.Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.},
  doi       = {10.1145/2594291.2594334},
  isbn      = {9781450327848},
  keywords  = {miscompilation, equivalent program variants, compiler testing, automated testing},
  location  = {Edinburgh, United Kingdom},
  numpages  = {11},
}

@InProceedings{Nitpick,
  author    = {Blanchette, Jasmin Christian and Nipkow, Tobias},
  booktitle = {Interactive Theorem Proving},
  title     = {Nitpick: A Counterexample Generator for Higher-Order Logic Based on a Relational Model Finder},
  year      = {2010},
  address   = {Berlin, Heidelberg},
  editor    = {Kaufmann, Matt and Paulson, Lawrence C.},
  pages     = {131--146},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Nitpick is a counterexample generator for Isabelle/HOL that builds on Kodkod, a SAT-based first-order relational model finder. Nitpick supports unbounded quantification, (co)inductive predicates and datatypes, and (co)recursive functions. Fundamentally a finite model finder, it approximates infinite types by finite subsets. As case studies, we consider a security type system and a hotel key card system. Our experimental results on Isabelle theories and the TPTP library indicate that Nitpick generates more counterexamples than other model finders for higher-order logic, without restrictions on the form of the formulas to falsify.},
  doi       = {10.1007/978-3-642-14052-5_11},
  isbn      = {978-3-642-14052-5},
}

@InProceedings{yang2011finding,
  author    = {Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
  booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
  title     = {Finding and Understanding Bugs in {C} Compilers},
  year      = {2011},
  address   = {New York, NY, USA},
  pages     = {283--294},
  publisher = {Association for Computing Machinery},
  series    = {PLDI '11},
  abstract  = {Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.},
  doi       = {10.1145/1993498.1993532},
  isbn      = {9781450306638},
  keywords  = {automated testing, compiler testing, random program generation, random testing, compiler defect},
  location  = {San Jose, California, USA},
  numpages  = {12},
}

@Comment{jabref-meta: databaseType:bibtex;}
