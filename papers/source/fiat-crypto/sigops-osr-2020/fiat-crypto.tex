%Page: single-spaced 8.5″ x 11″ pages
%Font and Columns: 10-point type on 12-point (single-spaced) leading, two-column format, Times Roman or a similar font, within a text block 6.5″ wide x 9″ deep.
%Number of pages: OSR does not enforce strict minimum or maximum page limits. We suggest 2-6 pages as a rough guideline, but authors should feel free to devote more space as needed..
%Other considerations: Pages should NOT be numbered, and figures and tables should be legible in black and white, without requiring magnification.
%Please feel free to use any Systems conference template (e.g., USENIX ATC, OSDI, SOSP, NSDI, etc.) to prepare drafts.

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}

\bibliographystyle{plain}
\pagestyle{empty}

\usepackage[utf8]{inputenc}
\usepackage{newunicodechar}
\newunicodechar{λ}{$\lambda$}
\newunicodechar{∀}{$\forall$}
\newunicodechar{⌊}{$\lfloor$}
\newunicodechar{⌋}{$\rfloor$}

\usepackage{cite}
\usepackage{etex}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}
\usepackage{ifthen}
\usepackage{textcomp}
\usepackage{makecell}
\renewcommand{\cellalign}{cl}
\usepackage{balance}

%% Some recommended packages.
\usepackage{listings}
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

% packages for the carry-chain diagram
\usepackage{wrapfig}
\usepackage[all, frame]{xy}
\usepackage{mathtools}

\usepackage{tikz}
\usepackage{tkz-graph}
\usetikzlibrary{shapes.misc,positioning,arrows,snakes,backgrounds}
\usepackage{bm}

\usepackage{pgfplots}
\usepackage{xtab}

\newcommand{\kw}[1]{\mathsf{#1}}
\newcommand{\id}[1]{\bm{\mathsf{#1}}}
\newcommand{\nat}[0]{\mathbb N}
\newcommand{\intZ}[0]{\mathbb Z}
\newcommand{\floor}[1]{{\left \lfloor #1 \right \rfloor}}
\newcommand{\bunderline}[1]{\underline{#1\mkern-2mu}\mkern2mu }

% ~ godbolt.org coloring
\newcommand{\docolor}[2]{{\setlength{\fboxsep}{0pt}%
  \ifthenelse{\equal{#1}{A}}{\colorbox{green!15}{#2}}{%
  \ifthenelse{\equal{#1}{B}}{\colorbox{blue!15}{#2}}{%
  \ifthenelse{\equal{#1}{C}}{\colorbox{yellow!15}{#2}}{%
  \ifthenelse{\equal{#1}{D}}{\colorbox{red!15}{#2}}{%
    #1#2%
  }}}}%
}}%
{
\catcode`\!\active
\global\def\enableverbatimcolors{%
  \catcode`\!\active
  \def!##1[[##2]]{\docolor{##1}{##2}}%
}
}

\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO: #1}}}

\hyphenation{Comp-Cert}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Simple High-Level Code For Cryptographic Arithmetic: \\ With Proofs, Without Compromises}

% The template didn't include email addresses; do we want to?
\author{
{\rm Andres Erbsen} \\
MIT CSAIL \\
{\rm \texttt{andreser@mit.edu}}
\and
{\rm Jade Philipoom} \\
MIT CSAIL \\
{\rm \texttt{jadep@mit.edu}}
\and
{\rm Jason Gross} \\
MIT CSAIL \\
{\rm \texttt{jgross@mit.edu}}
\and
{\rm Robert Sloan} \\
MIT CSAIL \\
{\rm \texttt{rob.sloan@alum.mit.edu}}
\and
{\rm Adam Chlipala} \\
MIT CSAIL \\
{\rm \texttt{adamc@csail.mit.edu}}
} % end author

\maketitle

\begin{abstract}
  We introduce an unusual approach for implementing cryptographic arithmetic in short high-level code with machine-checked proofs of functional correctness.
  We further demonstrate that simple partial evaluation is sufficient to transform such initial code into highly competitive C code, breaking the decades-old pattern that the only fast implementations are those whose instruction-level steps were written out by hand.

These techniques were used to build an elliptic-curve library that achieves competitive performance for a wide range of prime fields and multiple CPU architectures, showing that implementation and proof effort scales with the number and complexity of conceptually different algorithms, not their use cases.
As one outcome, we present the first verified high-performance implementation of P-256, the most widely used elliptic curve.
Implementations from our library were included in BoringSSL to replace existing specialized code, for inclusion in several large deployments for Chrome, Android, and CloudFlare.

This is an abridged version of the full paper originally presented in IEEE S\&P 2019~\cite{FiatCryptoSP19}.
We have omitted most proof-engineering details in favor of a focus on the system's functional capabilities.
\end{abstract}

\section{Introduction}

As verification makes the transition from research topic to component of real-world software ecosystems, the field must face challenges of scale, in terms of both amount of code and conceptual complexity.
Each system, of course, has its own constraints and objectives, and these needs will best be met with a broad toolbox of approaches.
In this paper, we sketch a case study following a relatively unusual approach, in the hope that it will help illuminate the broad array of possibilities for verification projects.
The full paper was originally presented in IEEE S\&P 2019~\cite{FiatCryptoSP19}; what follows is a version abridged, selectively updated, and adapted for a more systems-oriented audience.

That audience is probably most familiar with projects fitting the classic mold of proving theorems about programs written by hand, as exemplified by sEL4~\cite{seL4SOSP09}, IronClad~\cite{hawblitzel2014ironclad,hawblitzel2015ironfleet}, FSCQ~\cite{FscqSOSP15,FscqSOSP17,FscqOSDI18}, CertiKOS~\cite{CertiKOS}, and various case studies in push-button verification~\cite{Pbfs,Hyperkernel,Nickel,Serval}.
Often the implementers of these projects do rewrite systems from scratch in tandem with proofs, but the final code artifacts implement very similar pseudocode to off-the-shelf systems.
A moderately widely known alternative is \emph{program synthesis}, which writes a program automatically from its logical specification.
Most colloquially, this idea is associated with relatively intensive combinatorial search through spaces of possibilities, often taking advantage of SMT solvers like Z3~\cite{Z3}.
However, some systems-code domains are focused enough that we can write very predictable and \emph{verified} generic implementations.
With the help of a verified domain-specific compiler, we can translate these general problem descriptions into practical, performance-competitive code.
This paper considers a successful example of the template-based code-generation strategy.

Our project verifies efficient modular-arithmetic routines for elliptic-curve cryptography, a domain to which much optimization effort has been applied.
This means that competitive implementations use highly specialized techniques, previously accessible only to a small number of experts.
We aimed to create a system that generalized these approaches and proved, with minimal trusted code, that the detailed low-level routines implemented their high-level arithmetic specifications.
To that end, we first studied handwritten implementations and extracted fully general descriptions of the complex design choices they required.
Then, we encoded these algorithms in the Coq proof assistant~\cite{coq} as templates, generic over compile-time parameters like the modulus and integer width of the underlying architecture.
Once these parameters are plugged in, a domain-specific compiler transforms the templates into efficient low-level code.

The output code is encoded in a specialized low-level abstract syntax tree (AST) that includes only very basic fixed-width integer operations, at the approximate abstraction level of assembly instructions, and is easily translatable to a number of output languages.
At current count we can produce C, Go, and Rust, along with two other unusual specialized languages for specific applications.
The C code, our primary target, is competitive with existing handwritten C, although it still falls short of the performance of handwritten assembly.
However, our toolchain, with no input other than the compile-time parameters, produces efficient and completely verified code, and we believe it presents a very interesting case study in a verification approach that is unusual in \emph{generating} code, rather than working backwards from existing code.
(It is worthwhile to note here that one can still verify specific existing code by proving it equivalent to the generated code, and the equivalence is much more straightforward than equivalence with a high-level specification, and we have used this technique to verify existing production code.)
Our verification stretches all the way from very low-level C to the natural high-level specifications of modular arithmetic (e.g. $\forall x, y.\ \text{add}(x, y) = (x + y) \bmod p$).
The output code uses a restricted set of instructions by construction, preventing timing side channels.
The overall trusted computing base includes the Coq proof checker, a simple pretty-printer, and the C language toolchain.

In addition to being interesting research, our approach has proven to be practically applicable, as it is already used in widely deployed code.
Most applications relying on the implementations described in this paper are using them through BoringSSL, Google's OpenSSL-derived crypto library that includes our implementations of Curve25519 and P-256, the two most widely used elliptic curves via Internet standards.
A high-profile user is the Chrome Web browser, so today about half of HTTPS connections opened by Web browsers worldwide use our fast verified code (Chrome has about 60\% market share~\cite{browserMarketShare2020}, and 90\% of connections use X25519 or P-256~\cite{davidenTLSECC}).
% todo
% now Edge is just rebranded chrome, so +9% if we can find a citation
% Safari also uses BoringSSL, so +15%, but again nothing to cite, and Apple also may have put their own crypto implementations into BoringSSL and just kept the TLS parts
BoringSSL is also used in Android and on a majority of Google's servers, so a Google search from Chrome would likely be relying on our code on both sides of the connection.
Maintainers appreciate the reduced need to worry about bugs.

We believe our approach is particularly suited to design domains that a) involve methodical low-level transformations that are nonetheless finicky and easy for humans to get wrong, and b) apply these transformations to a wide range of setups with different parameters.
Elliptic-curve field arithmetic fits this description, because most efficient implementations use the same family of optimization techniques, while there are hundreds of combinations of moduli and native-integer sizes.
A change to either one usually dramatically changes the resulting code, even though the underlying techniques are similar.
Other opportunities to apply these ideas may include families of very closely related data structures being used in OS kernels and other systems software.

This abridged paper will be a whirlwind tour through the most interesting and informative aspects of our approach.
First, Section \ref{templates} will go over a common optimization technique for modular reduction and integer representation, as an example of the kinds of algorithms our templates encode.
Then, in Section \ref{compiler}, we will explain the stages of our domain-specific compiler and its capabilities.
Finally, Section \ref{experiments} will show a sample of our benchmarks.
For more details, particularly for proof-engineering techniques, explorations of related work, and full benchmarks, please look to the full-length paper~\cite{FiatCryptoSP19}.
For even \emph{more} details, the entire development is called \textbf{Fiat Cryptography}, and is available under the MIT license at:
\begin{center}
  \texttt{\url{https://github.com/mit-plv/fiat-crypto}}
\end{center}

\begin{figure*}
  \begin{tabular}{|p{4cm}|l|l|l|l|}
\hline
    prime & architecture & \# limbs & base & representation (distributing large $x$ into $x_0...x_n$)\\
\hline
    $2^{256} - 2^{224} + 2^{192} + 2^{96} - 1$ (P-256) & 64-bit & 4 & $2^{64}$ & $x = x_0 + 2^{64}x_1 + 2^{128}x_2 + 2^{192}x_3$ \\
\hline
  $2^{255}-19$ (Curve25519) & 64-bit & 5 & $2^{51}$ & $x = x_0 + 2^{51}x_1 + 2^{102}x_2 + 2^{153}x_3 + 2^{204}x_4$ \\
\hline
  $2^{255}-19$ (Curve25519) & 32-bit & 10 & $2^{25.5}$ & $x = x_0 + 2^{26}x_1 + 2^{51}x_2 + 2^{77}x_3 + ... + 2^{204}x_8 + 2^{230}x_9$ \\
\hline
  $2^{448} - 2^{224} - 1$ (p448) & 64-bit & 8 & $2^{56}$ & $x = x_0 + 2^{56}x_1 + 2^{112}x_2 + ... + 2^{392}x_7$ \\
\hline
    $2^{127} - 1$ & 64-bit & 3 & $2^{42.5}$ & $x = x_0 + 2^{43}x_1 + 2^{85}x_2$ \\
\hline
\end{tabular}
  \caption{\label{fig:representationexamples}Examples of big-integer representations for different primes and integer widths}
\end{figure*}


\begin{figure*}
\begin{alignat*}{5}
    s \times t & = 1 \times s_0t_0 && + 2^{43} \times s_0t_1 && + 2^{85} \times s_0t_2 \\
    & && + 2^{43} \times s_1t_0 && + 2^{86} \times s_1t_1 && + 2^{128} \times s_1t_2 \\
    & && && + 2^{85} \times s_2t_0 && + 2^{128} \times s_2t_1 && + 2^{170} \times s_2t_2 \\
    & = s_0t_0 && + 2^{43}(s_0t_1 + s_1t_0) && + 2^{85}(s_0t_2 + 2s_1t_1 + s_2t_0) && + 2^{127}(2s_1t_2 + 2s_2t_1) && + 2^{170}\times s_2t_2
\end{alignat*}
    \caption{\label{distribute}Distributing terms for multiplication mod $2^{127}-1$}
\end{figure*}


\section{Arithmetic Template Library}\label{templates}

In this section, we will describe the arithmetic optimization strategies we encoded in our template library.
For those who prefer to read code, we suggest \texttt{src/Demo.v} in the framework's source code, which contains a succinct standalone development of the unsaturated-arithmetic library up to and including an implementation of modular reduction specialized to take advantage of the specific shape of its fixed modulus.
The concrete examples derived in this section are within established implementation practice, and an expert would be able to reproduce them given an informal description of the strategy.
Our contribution is to encode this general wisdom in concrete algorithms and provide correctness certificates.

\subsection{Multi-Limbed Arithmetic}\label{unsaturated}

Cryptographic modular arithmetic implementations distribute very large numbers across smaller \emph{limbs} of 32- or 64-bit integers.
Fig. \ref{fig:representationexamples} shows a small sample of fast representations for different primes.
Notice that many of these implementations use bases other than $2^{32}$ or $2^{64}$, leaving bits unused in each limb: these are called \emph{unsaturated} representations.
Conversely, the ones using all available bits are called \emph{saturated}.
This abridged paper will only describe the unsaturated arithmetic in our development; for saturated arithmetic, please refer to the full paper.

Another interesting feature shown in the examples is that the exponents of some
bases, such as the original 32-bit Curve25519 representation~\cite{curve25519}, are not whole numbers.
In the actual representation, this choice corresponds to an alternating pattern, so ``base $2^{25.5}$'' uses 26 bits in the first limb, 25 in the second, 26 in the third, and so on.
Because of the alternation, these are called \emph{mixed-radix} bases, as opposed to \emph{uniform-radix} ones.

Why accept all of this added complication, and waste available bits, instead of just using saturated representations for everything?
These unorthodox big integers are fast primarily because of a specific modular-reduction algorithm, pseudo-Mersenne reduction, which becomes extremely fast when the number of bits in the prime \emph{corresponds to a limb boundary}.
For instance, reduction modulo $2^{255}-19$ is fastest when the 255th bit of a bignum is the first bit of a limb.
In a saturated implementation, limb boundaries would fall on multiples of the integer size (for instance, a 64-bit saturated representation would have limb boundaries at bits 0, 64, 128, etc.).
But in an unsaturated implementation, with a funky base like $2^{25.5}$, we get limb boundaries at bits 0, 26, 51, 77, ... and 255.
The vast speed improvement in modular reduction is worth keeping a few extra registers around to represent each large number.

\subsection{A Note on Carrying}

In unsaturated representations, it is not necessary to carry immediately after every addition.
For example, with 51-bit limbs on a 64-bit architecture, it would take 13 additions to risk overflow.
Choosing which limbs to carry and when is part of the design and is critical for keeping the limb values bounded.
Generic operations are easily parameterized on carry strategies, for example ``after each multiplication carry from limb 4 to limb 5, then from limb 0 to limb 1, then from limb 5 to limb 6,'' etc.
The library includes a conservative default.

\subsection{Example: Multiplication Modulo $2^{127}-1$}

To give a more concrete sense of how the representation described above works in practice, we will walk through a modular multiplication procedure specialized to the (relatively) small modulus $2^{127} - 1$ and a 64-bit architecture.
Say we want to multiply two numbers $s$ and $t$ in its field, with those inputs broken up into limbs as $s = s_0 + 2^{43} s_1 + 2^{85} s_2$ and $t = t_0 + 2^{43} t_1 + 2^{85} t_2$.
Distributing multiplication repeatedly over addition gives us the answer form shown in Fig. \ref{distribute}.

We have formatted the calculation suggestively: down each column, the powers of two are very close together, differing by at most one.
Therefore, it is easy to add down the columns to form our final answer, split conveniently into digits with integral bit widths.
%The number of primitive addition and multiplication operations is reasonable (and the primitive multiplications can be implemented as self-additions, which are often faster in hardware).

At this point the multiplication step is complete, but we still need to do modular reduction; we don't want our answer to have five limbs when it should need only three.
Here we use the pseudo-Mersenne reduction trick: $2^k \bmod {(2^k -c)} = c$, so $a + 2^kb \equiv a + cb \pmod{2^k - c}$.
We ``divide'' the last two limbs of the product, the terms with weights $2^{127}$ and $2^{170}$, by $2^{127}$.
Because we chose our base system wisely, this division is actually a no-op; we just now consider these limbs to have different weights.
This is the big trick that makes pseudo-Mersenne reduction fast: eliminating division by clever choice of base system.
Then we can multiply the high limbs by $c$, which in this case is 1, and add them to the first three digits, providing the following final formula:
\begin{align*}
  (s_0t_0 + 2s_1t_2 + 2s_2t_1) &+ 2^{43}(s_0t_1 + s_1t_0 + s_2t_2) \\
  &+ 2^{85}(s_0t_2 + 2s_1t_1 + s_2t_0)
\end{align*}

Note that this is a \emph{partial} modular-reduction algorithm; it is not guaranteed that the result is less than $2^{127}-1$, just that the result fits in three limbs.

In the code, \texttt{src/Demo.v} includes another example that walks through modular multiplication with pseudo-Mersenne reduction, using the modulus from Curve25519~\cite{curve25519}. The full generic template library is found under \texttt{src/Arithmetic}.

\section{Domain-Specific Compiler}\label{compiler}

\subsection{Partial Evaluation For Specific Parameters}\label{partial}

It is impossible to achieve competitive performance with arithmetic code that manipulates dynamically allocated lists at runtime.
The fastest code will implement, for example, a single numeric addition with straightline code that keeps as much state as possible in registers.
Expert implementers today write that straightline code manually, applying various rules of thumb.
Our alternative is to use \emph{partial evaluation} in Coq to generate all such specialized routines, beginning with our library of high-level functional implementations that generalize the patterns lurking behind hand-written implementations today.

Consider the case where we know statically that each number we add will have 3 digits.
A particular addition in our top-level algorithm may have the form $\id{add} \; [a_1, a_2, a_3] \; [b_1, b_2, b_3]$, where the $a_i$s and $b_i$s are unknown program inputs.
While we cannot make compile-time simplifications based on the values of the digits, we \emph{can} reduce away all the overhead of dynamic allocation of lists.
We could use Coq's term-reduction machinery, which allows us to choose $\lambda$-calculus-style reduction rules to apply until reaching a normal form.
Here is what happens with our example, when we ask Coq to leave $\kw{let}$ and $+$ unreduced but apply other rules.
(Note that :: is a notation for appending to the front of a list.)

\noindent {\small \begin{eqnarray*}
  \id{add} \; [a_1, a_2, a_3] \; [b_1, b_2, b_3] &\Downarrow& \kw{let} \; n_1 = a_1 + b_1 \; \kw{in} \; n_1 :: \\
  && \kw{let} \; n_2 = a_2 + b_2 \; \kw{in} \; n_2 :: \\
  && \kw{let} \; n_3 = a_3 + b_3 \; \kw{in} \; n_3 :: []
\end{eqnarray*}}%

\noindent We have made progress: no run-time case analysis on lists remains.
Unfortunately, $\kw{let}$ expressions are intermixed with list constructions, leading to code that looks rather different than assembly.
To work around this issue, we chose to implement this phase of our pipeline as a certified compiler\footnote{The compiler was ongoing work at the time of our original publication and was only briefly discussed as a future improvement; in the original paper, we described using Coq's built-in term-reduction machinery.
However, although the proof-engineering techniques have changed dramatically, the function is the same.
}.
That is, we define a type of abstract syntax trees (ASTs) for the sorts of programs that earlier phases produce, we reify those programs into our AST type, and we run compiler passes written in Coq's Gallina functional programming language.
Each pass is proved correct once and for all.

Our certified compiler handles partial evaluation and $\kw{let}$-lifting, turning this function into the straightline code
\noindent {\small \begin{eqnarray*}
  && \kw{let} \; n_1 = a_1 + b_1 \; \kw{in} \\
  && \kw{let} \; n_2 = a_2 + b_2 \; \kw{in} \\
  && \kw{let} \; n_3 = a_3 + b_3 \; \kw{in} \\
  && [n_1, n_2, n_3]
\end{eqnarray*}}%

Chaining together sequences of function calls leads to idiomatic and efficient straightline code, preserving sharing of $\kw{let}$-bound variables.
This level of inlining is common for the inner loops of crypto primitives, and it will also simplify the static analysis described in the next subsection.

\subsection{Word-Size Inference}\label{ex-bounds}

Up to this point, we have derived code that looks almost exactly like the C code we want to produce.
The code is structured to avoid overflows when run with fixed-precision integers, but so far it is only proven correct for natural numbers.
The final major step is to infer a range of possible values for each variable, allowing us to assign each one a register or stack-allocated variable of the appropriate bit width.

The bounds-inference pass works by standard abstract interpretation with intervals.
As inputs, we require lower and upper bounds for the integer values of all arguments of a function.
These bounds are then pushed through all operations to infer bounds for temporary variables.
Each temporary is assigned the smallest bit width that can accommodate its full interval.

As an artificial example, assume the input bounds $a_1, a_2, a_3, b_1 \in [0, 2^{31}]$; $b_2, b_3 \in [0, 2^{30}]$.
The analysis concludes $n_1 \in [0, 2^{32}]$; $n_2, n_3 \in [0, 2^{30} + 2^{31}]$.
The first temporary is just barely too big to fit in a 32-bit register, while the second two will fit just fine.
Therefore, assuming the available temporary sizes are 32-bit and 64-bit, we can transform the code with precise size annotations.

\noindent {\small \begin{align*}
  &\kw{let} \; n_1 : \nat_{2^{64}} = a_1 + b_1 \; \kw{in} \\
  &\kw{let} \; n_2 : \nat_{2^{32}} = a_2 + b_2 \; \kw{in} \\
  &\kw{let} \; n_3 : \nat_{2^{32}} = a_3 + b_3 \; \kw{in} \\
  &[n_1, n_2, n_3]
\end{align*}}%

\noindent Note how we may infer different temporary widths based on different bounds for arguments.
As a result, the same primitive inlined within different larger procedures may get different bounds inferred.
World-champion code for real algorithms takes advantage of this opportunity.

This phase of our pipeline is systematic enough that we chose to implement it too as another phase in our certified compiler.

\subsection{Compilation To Constant-Time Machine Code}\label{ex-asm}

What results is straightline code very similar to that written by hand by experts, represented as ASTs in a simple language with arithmetic and bitwise operators.
Our correctness proofs connect this AST to specifications in terms of integer arithmetic, such as the one for $\id{add}$ above.
All operations provided in our lowest-level AST are implemented with input-independent execution time in many commodity compilers and processors, and if so, our generated code is trivially free of timing leaks.
Each function is pretty-printed as C code and compiled with a normal C compiler, ready to be benchmarked or included in a library.
We are well aware that top implementation experts can translate C to assembly better than the compilers, and we do not try to compete with them: while better instruction scheduling and register allocation for arithmetic-heavy code would definitely be valuable, it is outside the scope of this project.
But this is the entire extent of the compromise: as described in the next section, we have been able to match or exceed the performance of all C code we sought to replicate.

\section{Experimental Results}\label{experiments}

The purpose of this section is to confirm that implementing optimized algorithms in high-level code and then separately specializing to concrete parameters actually achieves the expected performance.
Given the previous sections, this conclusion should not be surprising: as code generation is extremely predictable, it is fair to think of the high-level implementations as simple templates for low-level code.
We will detail how this works out for the two most prominent applications of our framework, the TLS ECC fields modulo $2^{255}-19$ and $2^{256}-2^{224}+2^{192}+2^{96}-1$.
In our full paper, we additionally demonstrate that the two simple templates generalized from these case studies are sufficient to achieve good performance across a broad sample of finite fields proposed for elliptic-curve-cryptography use.

\subsection{X25519 Scalar Multiplication}

We measured the number of CPU cycles different implementations take to multiply a secret scalar and a public Curve25519 point (represented by the $x$ coordinate in Montgomery coordinates).
Despite the end-to-end task posed for this benchmark, we believe the differences between implementations we compare against lie in the field-arithmetic implementation.
% They all use the same scalar-multiplication algorithm and two almost-identical variants of the Montgomery-curve $x$-coordinate differential-addition formulas.

The benchmarks were run using \texttt{gcc} 7.3 on an Intel Broadwell i7-5600U processor in a kernel module
%\footnote{Thanks to Jason Donenfeld for teaching us how and setting up benchmarks for popular Curve25519 implementations! %TODO:rephrase? Moved to acknowledgments
% TODO: should we incldue a link to his run of the benchmarks confirming a similar ranking on Skylake?
% https://www.mail-archive.com/curves@moderncrypto.org/msg00953.html
%}
with interrupts, power management, Hyper Threading, and Turbo Boost features disabled.
We are presenting them as a rather arbitrary illustration of the performance that can be achieved using our approach; it is expected that the relative speeds could differ by as much as a couple of times across different microarchitectures, compilers, and compiler flags.
% Each measurement represents the average of a batch of 15,000 consecutive trials, with time measured using the \texttt{RDTSC} instruction and converted to CPU cycles by multiplying by the ratio of CPU and timestamp-counter clock speeds.
% C code was compiled using \texttt{gcc} 7.3 with \texttt{-O3 -march=native -mtune=native -fwrapv} (we also tried \texttt{clang} 5.0, but it produced $\sim$10\% slower code for the implementations here).

% gcc 7.3 always fastest
% NAME, TSC * 2.494/2.6, usec, raw TSC
\begin{center}%
  \newcommand{\makecpu}[2][100]{{\textcolor{black!#1}{\rule{\dimexpr6em * #2 / 546000\relax}{1ex}}}}%
  \newcommand{\makethiscpu}[1]{\makecpu[50]{#1}}%
  \newcommand{\makeothercpu}[1]{\makecpu[100]{#1}}%
  \begin{tabular}[]{lrl}
  Implementation & CPU cycles & \\
\hline
% \texttt{OLHFR}\cite{oliveira_sac2017}, asm       & \texttt{117877} & \makeothercpu{117877} \\ % 122887 % TODO this is newer usermode number
\texttt{OLHFR}, asm       & \texttt{121444} & \makeothercpu{126606} \\ % % 122887 % TODO this is kbench9000 number for older version
\texttt{amd64-64}, asm       & \texttt{151586} & \makeothercpu{151586} \\ % 158029
\textit{this work}, 64-bit & \texttt{152195} & \makethiscpu{152195} \\ % 158664
%\textit{this work B}, 64-bit & \texttt{152195} & \makethiscpu{152195} \\ % 158664
\texttt{sandy2x}, asm        & \texttt{154313} & \makeothercpu{154313} \\ % 160872
\texttt{hacl-star}, 64-bit   & \texttt{154982} & \makeothercpu{154982} \\ % 161570
\texttt{donna64}, 64-bit C   & \texttt{168502} & \makeothercpu{168502} \\ % 175664
%\textit{this work A}, 64-bit & \texttt{174637} & \makethiscpu{174637} \\ % 182060
%\textit{this work, 32-bit  } & \texttt{310585} & \makethiscpu{310585} \\ % 323786
%\texttt{donna32}, 32-bit C   & \texttt{529812} & \makeothercpu{529812} \\ % 552331
\hline
  \end{tabular}

%%   \pgfplotsset{
%%     ylabel right/.style={
%%       after end axis/.append code={
%%         \node [rotate=90, anchor=north] at (rel axis cs:1.1,0.5) {#1};
%%       }
%%     }
%%   }%
%%   \begin{tikzpicture}
%%     \begin{axis}[
%% %        xlabel=Implementation,
%%         ylabel=CPU cycles,
%%         ylabel right=\textmu{}s at 2.6GHz,
%%         yticklabel style={
%%           rotate=30,
%%           anchor=east,
%%         %  /pgf/number format/fixed,
%%         %  /pgf/number format/precision=6
%%         },
%%         scaled y ticks=false,
%%  %       ybar,
%%         ymin=0,
%%         symbolic x coords={%
%%           {\texttt{amd64-64}, asm      },
%%           {\textit{this work B}, 64-bit},
%%           {\texttt{sandy2x}, asm       },
%%           {\texttt{hacl-star}, 64-bit  },
%%           {\texttt{donna64}, 64-bit C  },
%%           {\textit{this work A}, 64-bit},
%%           {\textit{this work, 32-bit  }},
%%           {\texttt{donna32}, 32-bit C  }%
%%         },
%%  %       xtick={%
%%  %         {\texttt{amd64-64}, asm      },
%%  %         {\textit{this work B}, 64-bit},
%%  %         {\texttt{sandy2x}, asm       },
%%  %         {\texttt{hacl-star}, 64-bit  },
%%  %         {\texttt{donna64}, 64-bit C  },
%%  %         {\textit{this work A}, 64-bit},
%%  %         {\textit{this work, 32-bit  }},
%%  %         {\texttt{donna32}, 32-bit C  }%
%%         %       },
%%         ytick pos=left,
%%         yticklabel={$\pgfmathprintnumber\tick$},
%%         extra y ticks = {0,130000,260000,390000,540000},
%%         extra y tick labels = {,50,100,150,200},
%%         extra y tick style={
%%           ytick pos=right,
%%           yticklabel pos=right,
%%           yticklabel style={rotate=-30,anchor=west}
%%         },
%% %        extra y tick label={$\pgfmathprintnumber\tick$},
%%         x tick label style={rotate=80,anchor=east},
%%         xtick=data,
%%         only marks,
%%       ]
%%       \addplot+[ycomb] coordinates {
%%         ({\texttt{amd64-64}, asm      }%
%%         ,151586)
%%         ({\textit{this work B}, 64-bit}%
%%         ,152195)
%%         ({\texttt{sandy2x}, asm       }%
%%         ,154313)
%%         ({\texttt{hacl-star}, 64-bit  }%
%%         ,154982)
%%         ({\texttt{donna64}, 64-bit C  }%
%%         ,168502)
%%         ({\textit{this work A}, 64-bit}%
%%         ,174637)
%%         ({\textit{this work, 32-bit  }}%
%%         ,310585)
%%         ({\texttt{donna32}, 32-bit C  }%
%%         ,529812)
%%       };
%%       \addplot+[ycomb] coordinates {
%%         ({\textit{this work B}, 64-bit}%
%%         ,152195)
%%         ({\textit{this work A}, 64-bit}%
%%         ,174637)
%%         ({\textit{this work, 32-bit  }}%
%%         ,310585)
%%       };
%%     \end{axis}
%%   \end{tikzpicture}
\end{center}

We report on our code generated using the standard representations for both 32-bit and 64-bit, though we are primarily interested in the latter, since we benchmark on a 64-bit processor.
In order, we compare against \texttt{OLHFR}, the non-precomputation-based implementation from \cite{oliveira_sac2017}; \texttt{amd64-64} and \texttt{sandy2x}, the fastest assembly implementations from SUPERCOP~\cite{SUPERCOP} that use scalar and vector instructions respectively; the verified X25519 implementation from the HACL$^*$ project~\cite{hacl}; and the best-known high-performance C implementation \texttt{curve25519-donna}, in both 64-bit and 32-bit variants.
The field arithmetic in both \texttt{amd64-64} and \texttt{hacl-star} has been verified using SMT solvers~\cite{verif25519,ECCstar}.
Our code is generated from a general template for unsaturated pseudo-mersenne arithmetic which is proven correct for all parameters.
We previously applied a few optimizations to $2^{255}-19$ specifically, but have since integrated the generalized forms of these into our template.

The results of a similar benchmark on an early prototype of our methodology were good enough to convince the maintainers of the BoringSSL library to adopt it, resulting in this Curve25519 code being shipped since Chrome 64 and used by default for TLS connection establishment in other Google products and services.
Previously, BoringSSL included the \texttt{amd64-64} assembly code and a 32-bit C implementation as a fallback, which was the first to be replaced with our generated code.
Then, the idea was raised of taking advantage of lookup tables to optimize certain point ECC multiplications.
While the BoringSSL developers had not previously found it worthwhile to modify 64-bit assembly code and review the changes, they made use of our code-generation pipeline (without even consulting us, the tool authors) and installed a new 64-bit C version.
The new code (our generated code linked with manually written code using lookup tables) was more than twice as fast as the old version and was easily chosen for adoption, enabling the retirement of \texttt{amd64-64} from BoringSSL.

% https://boringssl-review.googlesource.com/c/boringssl/+/24805
% Use 51-bit limbs from fiat-crypto in 64-bit.
%
% Our 64-bit performance was much lower than it could have been, since we
% weren't using the 64-bit multipliers. Fortunately, fiat-crypto is
% awesome, so this is just a matter of synthesizing new code and
% integration work.
%
% Functions without the signature fiat-crypto curly braces were written by
% hand and warrant more review. (It's just redistributing some bits.)
%
%
% https://boringssl-review.googlesource.com/c/boringssl/+/25524
% Now that we have 64-bit C code, courtesy of fiat-crypto, the tradeoff
% for carrying the assembly changes:
%
% Assembly:
% Did 16000 Curve25519 base-point multiplication operations in 1059932us (15095.3 ops/sec)
% Did 16000 Curve25519 arbitrary point multiplication operations in 1060023us (15094.0 ops/sec)
%
% fiat64:
% Did 39000 Curve25519 base-point multiplication operations in 1004712us (38817.1 ops/sec)
% Did 14000 Curve25519 arbitrary point multiplication operations in 1006827us (13905.1 ops/sec)
%
% The assembly is still about 9% faster than fiat64, but fiat64 gets to
% use the Ed25519 tables for the base point multiplication, so overall it
% is actually faster to disable the assembly:
%
% >>> 1/(1/15094.0 + 1/15095.3)
% 7547.324986004976
% >>> 1/(1/38817.1 + 1/13905.1)
% 10237.73016319501
%
% (At the cost of touching a 30kB table.)
%
% The assembly implementation is no longer pulling its weight. Remove it
% and use the fiat code in all build configurations.

\subsection{P-256 Mixed Addition}

Next, we benchmarked our Montgomery modular arithmetic as used for in-place point addition on the P-256 elliptic curve with one precomputed input (Jacobian += affine).
A scalar-multiplication algorithm using precomputed tables would use some number of these additions depending on the table size.
The assembly-language implementation \texttt{nistz256} was reported on by Gueron and Krasnov~\cite{nistz256} and included in OpenSSL; we also measured its newer counterpart that makes use of the ADX instruction-set extension.
% Our choice to use Montgomery reduction was inspired by the impressive performance of that implementation.
The 64-bit C code we benchmark is also from OpenSSL and uses unsaturated-style modular reduction, carefully adding a couple of multiples of the prime each time before performing a reduction step with a negative coefficient to avoid underflow.
These P-256 implementations here are unverified.
The measurement methodology is the same as for our X25519 benchmarks, except that we did not manage to get \texttt{nistz256} running in a kernel module and report userspace measurements instead.
% We also passed \texttt{-ipo} to \texttt{icc}.

\begin{center}\begin{tabular}[]{lrrrr}
% tilde means measured in userspace, everything else kbench9000
% 2.494*TSC
  Implementation        & fastest       & \texttt{clang}& \texttt{gcc}& \texttt{icc} \\
\hline
  \texttt{nistz256} +ADX &  \verb|~550|  &  &  &  \\
  \texttt{nistz256} AMD64&  \verb|~650|  &  &  &  \\
  \textit{this work A}   &  \verb|1143|  & \texttt{1811}& \texttt{1828}& \texttt{1143}\\
  OpenSSL, 64-bit C      &  \verb|1151|  & \texttt{1151}& \texttt{2079}& \texttt{1404}\\
  \textit{this work B}   &  \verb|1343|  & \texttt{1343}& \texttt{2784}& \texttt{1521}\\
\hline
\end{tabular}\end{center}

Saturated arithmetic is a known weak point of current compilers, resulting in implementors either opting for alternative arithmetic strategies or switching to assembly language.
Our programs are not immune to these issues: when we first ran our P-256 code, it produced incorrect output because \texttt{gcc} 7.1.1 had generated incorrect code%
\footnote{\url{https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81300}, \url{https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81294}}%
; \texttt{clang} 4.0 exited with a mere segmentation fault.%
\footnote{\url{https://bugs.llvm.org/show_bug.cgi?id=24943}}
Even in later compiler versions where these issues have stopped appearing, the code generated for saturated arithmetic varies a lot between compilers and is obviously suboptimal:
for example, there are ample redundant moves of carry flags, perhaps because the compilers do not consider flag registers during register allocation.
The same pattern is also present for X25519, although less pronounced: the two fastest assembly implementations listed earlier use a 4-limb saturated representation, but the speedup over 5-limb unsaturated assembly is smaller than the slowdown incurred in C code due to heavy use of carry flags.
Furthermore, expressing the same computation using intrinsics such as \verb|_mulx_u64| (variant A in the table) or using \texttt{uint128} and a bit shift (variant B) can produce a large performance difference, in different directions on different compilers.
% As arithmetic operations in our C source line up very closely with those in the \texttt{nistz256} assembly code, and as the difference between \texttt{icc} and \texttt{gcc} is bigger than the difference between our code and the fastest assembly code, we attribute most of the performance difference to low-level compilation.

The BoringSSL team had a positive enough experience with adopting our framework for Curve25519 that they decided to use our generated code to replace their P-256 implementation as well.
First, they replaced their handwritten 64-bit C implementation.
Second, while they had never bothered to write a specialized 32-bit P-256 implementation before, they also generated one with our framework.
\texttt{nistz256} remains as an option for use in server contexts where performance is critical and where patches can be applied quickly when new bugs are found.
The latter is not a purely theoretical concern -- the appendices of our full paper contain a sample of issues discovered in previous \texttt{nistz256} versions.
The two curves thus generated with our framework for BoringSSL together account for over 99\% of ECDH connections initiated by Google Chrome.
%% In fact, given that very recent versions of Microsoft Edge are based on Chromium~\cite{EdgeChromiumAnnouncement}, it's possible that connections initiated through that browser also use our generated code, although we cannot find a detailed enough source to confirm.
%% \todo{(Adam) Is the previous sentence too handwavy? Should we just not mention Edge? Yeah, I'd skip mentioning it, if we can't track down confirmation before submitting.}

% https://boringssl-review.googlesource.com/c/boringssl/+/23244

\section{Discussion}\label{conclusion}

We would like to remark on the aspects of elliptic-curve-cryptography implementation that made this approach work as well as it did, to aid future application in other contexts.
The most general (and perhaps the most important) takeaway is that effort put into structuring code in the most instructive manner possible pays off double during verification, enough to justify the development of new tooling to make that code run fast.
In cases where generalizing an algorithm makes its operation and invariants more apparent, we think it simply makes sense to prove correctness for the general version and use partial evaluation to derive the desired code, even if a specialized implementation has already been written.

Looking towards the future, we would like to extend our pipeline to bypass the C compiler and target assembly, shrinking our trusted code base.
This would require studying combined register allocation and instruction scheduling; since our output AST is very low-level, the C compiler is doing little else for us.
Alternatively, hand-optimized assembly code could be checked to implement the same machine-word-level computation as our generated code using an equality-checking engine like those found in modern SMT solvers (e.g., \cite{simplify}), allowing verification of optimizations that are out-of-reach for compilers, for example effective use of vector instructions.


% TODO: any updates here?
\section*{Acknowledgments}

This work was supported in part by a Google Research Award and National Science Foundation grants CCF-1253229, CCF-1512611, and CCF-1521584.
We benefited greatly from a fruitful collaboration with Google involving David Benjamin, Thai Duong, Adam Langley, Dominic Rizzo, and Marius Schilder.
Robert Sloan contributed to this project as a student at MIT, before joining Google.
We thank Jason Donenfeld for teaching us how to benchmark arithmetic code with Linux kernel modules, as well as for setting up benchmarks for popular Curve25519 implementations.
For comments on drafts of the paper, we thank Daniel J. Bernstein, Tej Chajed, Istvan Chung, Karl Samuel Gruetter, Ivan Kuraj, Adam Langley, Derek Leung, Devin Neal, Rahul Sridhar, Peng Wang, Ray Wang, and Daniel Ziegler.

\balance
\bibliography{fiat-crypto}

\end{document}
