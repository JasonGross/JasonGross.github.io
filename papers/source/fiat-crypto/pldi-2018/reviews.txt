PLDI 2018 Paper #2 Reviews and Comments
===========================================================================
Paper #129 Systematic Generation of Fast Elliptic Curve Cryptography
Implementations


Review #129A
===========================================================================
* Updated: 31 Jan 2018 11:35:26pm EST

Overall merit
-------------
C. I would not accept this paper but will not argue strongly against
   accepting it.

Reviewer expertise
------------------
X. Expert

Paper summary
-------------
The paper develops and evaluates a methodology for building certified
cryptographic implementations using the Coq proof assistant. The main
technical contribution of the paper is a generic verified library for
modular arithmetic. Instantiating the library on two popular curves
Ed25519 and P256 yields reasonable performance.


Strengths:

+ Alternative methodology to concurrent work on verified ECC
implementations (HACL*, Jasmin, Vale)

+ Performance of generated code is close to fastest implementations
and better than C code

- Better comparison with other approaches required, both informally
  and for evaluation purposes.

Comments for author
-------------------
The paper proposes a method for generating functionally correct and
efficient implementations for elliptic curve cryptography using the
general purpose proof assistant Coq. The central contribution of the
paper is a generic Coq library for modular arithmetic. The library can
be efficiently instantiated given some concrete choice of parameters.
According to the authors, the library complements concurrent work that
focuses on the development of cryptographic routines, although no step
for integration have been made at this stage.

While the work is pursuing an interesting direction, and the paper is
a fine example of proof engineering, the structure of the paper makes
it difficult to pinpoint the key challenges with the current approach.
The paper explains in several places tricky issues that arise in the
course of the work, but I could not always get a clear sense of how
hard solving these issues were. Similarly,  it is not always clear how
easy it would be to go beyond the current limitations---register
allocation, instruction scheduling, straightline code. I expect that
register allocation and instruction scheduling can be addressed using
translation validation, in the style of CompCert. Going beyond the
inner loops of crypto primitives might be more challenging.

Overall, I think that the problem of developing efficient and verified
libraries for arithmetic is a very important one, and the paper makes
a good case that it can be done at a reasonable cost. It remains to be
seen, though, whether this line of work can be extended in the future
to deliver fully verified arithmetic libraries---the emphasis of the
current paper is on elliptic curve arithmetic, and straightline code
programs. This said, I would suggest that the authors make explicit
the current limitations---most of it is buried in the related work
section---and also provide comparisons in the experimental section
with other recent approaches with formal guarantees.

Added after authors response
-----------------------------------
I did not find the response satisfactory on several accounts:

- Your answer about comparison with other approaches ignores the fact that other works deliver verified code that is as efficient than state-of-the-art code.

- Your answer about how to use the tool ignores that different choices of parameters may often require different algorithms altogether.

- I am also very skeptical that crypto developers will use the tool

- I am concerned that CPS has to be proved manually.  

- Your answer about straightline code is unconvincing. Supporting conditionals and loops is very important for proving correctness and safety of advanced algorithms.

Overall, my enthusiasm to accept the paper has decreased a lot after reading the authors response.

Questions for Authors
---------------------
- Do you have an intuition why the OpenSSL implementation is so slow
  compared to all other implementations?



Review #129B
===========================================================================

Overall merit
-------------
B. I support accepting this paper but will not champion it.

Reviewer expertise
------------------
Z. Some familiarity

Paper summary
-------------
This paper describes a verified compiler for elliptic curve cryptography algorithms. The algorithms, expressed at a relatively high level, are compiled using several optimization strategies specific to the application domain (e.g., the specific elliptic curve) and the target architecture, resulting in performance comparable to expert-written code.

Comments for author
-------------------
This is a great demonstration of using domain knowledge to improve compilation. Because the compiler is aware of the intended use case of its code, it can apply transformations that are highly beneficial in the context of modular arithmetic for a specific modulus that would be disadvantageous in general.

I appreciated the discussion of the data structures used to make ECC arithmetic efficient, such as the various representations of points and the modulus-specific numeric representation. It was well motivated and easy to follow.

However, I don't have a clear sense of how this compiler is *used*. When developers sit down to write some ECC code, what do they provide to the compiler? Just the algorithm and target information? Do they need to prove anything? Do they interact with it at all, or just wait for the output? What is the output of the compiler? Presumably, it's C source code that developers would incorporate into their projects, like the function in Figure 1.

What is Figure 1 showing? Section 2 talks about specifications and algorithms, none of which are provided in Figure 1, where the only inputs are a modulus and a target architecture.



Review #129C
===========================================================================
* Updated: 1 Feb 2018 2:42:25pm EST

Overall merit
-------------
B. I support accepting this paper but will not champion it.

Reviewer expertise
------------------
Z. Some familiarity

Paper summary
-------------
The paper presents a verifying compiler for optimizing arithmetic
operations used in Cryptographic implementations.
Users specify arithmetic operations in Coq and also
write example generated code to guide the compilation
process.

The compiler performs correctness preserving program transformations
until an efficient C program is derived.
These transformations are proven to be correctness preserving using Coq.

For Curve255119, the generated code is 20% within the most efficient implementations.
The code have already been adapted by a major Web browser.

The paper describes the transformations. A thorough experimental evaluation
is done in order to determine the usefulness of the method.

Comments for author
-------------------
This paper is pretty far from my research area but I was able to follow
most parts thanks to a nice presentation.

I did not like your projection that the tool will be adapted.
It seems hard for me that Crypto library hackers will use your tool.
They would probably prefer bug finding tools and Python specification.

The example in Figure 1 is not very useful for understanding.

Still I really liked this work.

I did not read the appendix.


Minor point: there are other CS areas where abstractions are not
often used, e.g., computer games.

Post rebuttal thoughts

I was not confident in my judgment and the response made me worry more.
Even if you are right that the approach has been already adapted, the PC need to be convinced that the results advance the state of art in a significant way.



Review #129D
===========================================================================

Overall merit
-------------
C. I would not accept this paper but will not argue strongly against
   accepting it.

Reviewer expertise
------------------
Z. Some familiarity

Paper summary
-------------
The paper presents a compiler for generating optimized and provably correct implementations of cryptographic primitives such as elliptic curves. The compiler starts with a unoptimized, mathematical specification of the primitives, and generates implementations that are optimized for a given prime and architecture. The compiler generated optimized code via transformations such as optimized point representation, multi-limbed arithmetic and partial evaluation that are conventionally implemented by experts. An evaluation using a few cryptographic benchmarks suggests that the compiler can generate code that is within a factor of 2 compared to hand optimized implementations.

Comments for author
-------------------
Strengths
- Identifies an interesting domain for compilation where the state of the art involves a large amount of manual effort
- Novel compiler design which encodes many optimizations conventionally hand implemented by domain experts
- Implementation adopted by a major web browser

Weaknesses
- Critical low level optimizations missing (register allocation and scheduling) 
- Limited to generating straight line code (no support for loops or conditionals)
- Side channel resistance not described
- Limited extensibility
- Poor presentation of design choices and optimizations.

At the outset I must mention that I found the paper a very interesting read. It identifies a domain where critical components continue to be hand written, hand optimized and manually proven correct. Clearly, any automation, especially in optimizations for different architecture, and different primes, is extremely useful. And its great to see the implementation of some of the primitives accepted by a major browser vendors. 

As far as the proposed compilation scheme is concerned, I am not completely convinced it generates very high quality code yet, and hence is not ready for broad adoption. As the authors themselves mention, the compiler generates C code and relies on the C/C++ compiler to generate optimized assembly. This not only puts the entire C compiler in the TCB, but also restricts optimizations to generic optimizations supported by the C compiler, not domain specifc optimizations that would be supported by a domain specfic compiler. I think you will find that generating optimized assembly requires a whole new set of techniques not covered in this paper. 
  
An aspect the paper entirely misses out is side channels. The compiler claims to generate side channel resistant code, but I dont see any reference or description. Is it the case that since the compiler is focused only on simple straight line code, side channels dont exist. If this is the case, the paper should say it

My other concern is that its unclear how the compiler deal with hardware changes such as new ISA support for cryptograpic primitives. 

On the issue of presentation, the paper presents the rationale for the design choices and optimizations, but doesnt present any algorithmic description of the optimizations themselves. For example, it mentions that a number of representations of points exist with different tradeoffs, but doesnt describe how the compiler chooses a specific representation. Is the choice left to the user (perhaps exposed as a compiler flag), or made automatically? There is a similar concern with the implementation of modular reduction.

Questions for Authors
---------------------
See questions on quality of code and side channels



Review #129E
===========================================================================
* Updated: 31 Jan 2018 9:44:08pm EST

Overall merit
-------------
B. I support accepting this paper but will not champion it.

Reviewer expertise
------------------
Z. Some familiarity

Paper summary
-------------
This paper describes a system for compiling pseudocode for simple
straight-line operators for modular big-number arithmetic into
optimized C (in the portable-assembly-language sense) that is
competitive with (but slower than) hand-written assembly code written
by experts. The optimized C is correct by construction because the
Coq-based compiler produces proofs along with C code, at least with
respect to the original pseudocode. The generated code outperforms
generic code that uses a library by 5x, but is about 2x slower than
assembly code for one of their benchmarks.

Comments for author
-------------------
The fundamental role of a compiler is to translate a high-level
description into low-level code. This paper describes a system for
doing so in a specific domain which has, to date, been the realm of
domain experts. In that sense, this paper is a classical compiler
paper. Furthermore, it effectively encodes domain knowledge from the
domain involved.

Leaving aside the questions of audience and deployment that I raise
below, the fundamental question that a review has to answer is whether
this paper contributes to our (more general) knowledge of compiler
techniques. In my judgment, it does. Because of the results in this
paper, we know that we can compile suitable elliptic-curve operations
down to straight-line primitives and that the resulting code is going
to be fast enough.

This paper covers a lot of ground: the system is ambitious and takes
code through quite a few stages. The flip side to describing an
ambitious system is that it is not possible to describe the system in
full detail. I sometimes felt that important details got glossed
over. One specific example is section 2.4.  We know how to
automatically convert to continuation-passing style, so I can imagine
that this partial evaluation is automatic, and the demo suggests it
too. But the paper doesn't explicitly say that. Who is the "we" at
line 376 ("Concretely, we rewrite add.")? And, on line 276, who makes
the choice of representing points with three coordinates? The
compiler? The programmer? Section 3 is also quite short on detail.

## Audience and deployment

There is an interesting question about who would actually write these
sorts of implementations. As a non-expert, I wouldn't write a crypto
implementation before gaining expertise, because I'm sure it would be
insecure, in addition to being slow. So the target audience for this
work is quite narrow. Within that target audience, I could imagine
that the experts would appreciate the ability to experiment with
alternate implementations, and the introduction does point that out.

On the other hand, deployment of the generated code is a strong point
in favour of the paper, in the sense that it is evidence of impact.

## General comments and questions

* The introduction is strong on the hype. Line 88 describes a
conceivable implication of this work. I can conceive it too, but the
paper hasn't quite reached the goal of producing world-champion
assembly implementations yet.

* Line 153: "The only input is a prime number". Compilers usually also
take code.  Please address Q2 in the Questions for Authors.

* line 215-217: I think that everyone's been thinking about cache side
channels recently. I don't think that your generated straight-line code
is susceptible to such attacks. You do mention that you don't support
loops and conditionals, and that would also make you susceptible to
more attacks.

* In Section 2.3, are we assuming p < 2^{64}?

* I think there is some deeper implication about having the associational
and positional representations and being able to convert between them.
I don't know what it is.

* Section 5.2 looks like bitwidth analysis (Stephenson et al, PLDI 2000).

* Section 6.2: The hand-modified version is somewhat sketchy.
I don't see how you can prove it equivalent with the infrastructure you've
described here. Do you actually work with assembler?

* Section 6.3: There is still quite a gap between this work and OpenSSL,
and the attribution to low-level optimizations is somewhat unsatisfying
in the absence of, say, tweaked code.

# Added after response

Thank you for your response. I have a better sense of what your system
accomplishes. I still believe that this paper does a lot. It seems that
the other reviewers were hoping for more, and were disappointed that their
hopes were not met.

My current understanding is:

* the system relies on implementations for straight-line crypto implementations
which have been converted partly into intermediate code already.
* given a description of a compilation target, the system can go the rest of the way
and produce C code.
* along with the C code, there is a verified proof that the translation into C code
is sound.
* the produced code can plausibly be deployed in production and helps decrease the
workload of crypto experts.

I will stick with my rating of "B".

Questions for Authors
---------------------
Please clarify my understanding of section 2.4. What is manual and what is automatic
in the CPS conversion in your system?

I'd like to know more about what primitives you actually generate. I
looked at your artifact and saw that it asked to generate primitives
like "feadd". And you write that your example code is in "some
unspecified functional language". But I don't have a good sense as to
what you can take as input code. The functions aren't hardcoded, I hope.

I also would like to know about the answers to the questions I asked 
in the paragraph that starts with "This paper covers a lot of ground".

Section 5: What guarantees exist before the certified compiler? I don't quite
understand that part of the pipeline. i.e. why do you call out a particular
part of your pipeline as a certified compiler, and not others?

I'd also like an answer to the question about Section 6.2.

(The other questions in-line in the review would, in my opinion,
improve the paper, but won't change my assessment of it).



Response by Andres Erbsen <andreser@mit.edu>
===========================================================================
Thanks for the wealth of suggestions on improving the paper's presentation.  We'll be glad to consider reordering material or adding new information.  For now, we'd like to point out how a few of the requests for expansion can be addressed with existing material in the paper, possibly in the context of some brief new clarifications.

- Review A asks for more detailed comparison with other approaches.  First, on the side of performance of related code, competing approaches involve writing code by hand, and they generally copy quite faithfully from the best known implementations (e.g., OpenSSL), so our comparison with best-known versions is a strong proxy for the performance elements of HACL*, Jasmin, and Vale.  On the side of developer effort, we hope the message came across that, once a new algorithm is implemented in our framework, it becomes almost free to recompile for a new prime modulus or a new hardware-architecture class, while as far as we know, no high-performance past project had implemented/verified more than one configuration of that kind per algorithm.  Furthermore, we don't know of any prior high-performance verification of NISTP256 or Montgomery reduction, which are used by the vast majority of TLS connections today.
- Review B asks how developers use the tool and what comes out in the end.  We should perhaps have been clearer that our story involves at least three layers: the framework, generic implementations of algorithms, and instantiations of those algorithms to new parameters.  Targeting a new algorithm involves new coding and proving work (often nontrivial).  However, compiling an existing algorithm with new parameters really does only require the level of effort seen in the "Input" part of Figure 1 (no proving, etc.).  Much of the PLDI audience probably takes that sort of thing for granted, but as far as we know, all past approaches have required days of work to "recompile" an existing algorithm for new parameters.  And, yes, the "Output" part of Figure 1 is representative of what our tool produces.  It can be copied-and-pasted into a project, compiled and linked as a library, etc.
- Review C expresses skepticism that crypto-library authors will use this tool.  We're not sure what to say beyond our original (blinded) description of how one of the most used crypto libraries has already adopted our framework (and more has happened since paper submission).  It seems there is a nontrivial segment of the crypto world where stronger security guarantees can outweigh the last 10% of performance.
- Review D says that we failed to discuss side-channel resistance, though we believe the issue was treated fully (but perhaps the presentation deserves rearranging).  In addition to several other mentions throughout the paper, see the paragraph starting on line 951.  Essentially, our target language only exposes instructions that preserve constant time.
- Review D asks how the compiler chooses a representation.  The main part of our framework is designed to take the representation as a parameter, and it is hard to predict analytically which choice will perform best.  Thus, the experiments of Figure 3 involve heuristic generation of several representations per configuration (prime modulus and hardware target), compiling with each choice, and keeping the best-performing version.  Currently we only bother generating 2 representations per configuration, but it is easy to add new heuristics to the mix, with no new formal-methods work.
- Review E asks who performs CPS translation and makes important design choices.  Currently the programmer implementing a new generic algorithm codes and proves a CPS version manually.  We would like to apply CPS translation automatically, though our initial efforts were flummoxed by the dependent types used by our direct-style algorithms.  Design choices like data-representation changes for points are also made manually by the programmer implementing a new algorithm.  From this question and parts of other reviews, we see that one point we failed to make clearly is that nontrivial manual work goes into implementing a new verified, generic algorithm, while we then provide a good automation story for instantiating that algorithm with new parameters.

More responses past the 700-word limit
======================================

We see two main use cases for the artifact produced in this work.
First it can be used in a plug-and-play fashion to generate modular arithmetic code using the already-implemented strategies.
This use case is rather narrow but very expedient when it is applicable.
More importantly, we think there is a lot going for implementing new optimized algorithms in the style we propose.
In particular, the code reuse made possible by our library (relying on a predictable specializing compiler) allows new operations (e.g. polynomial multiplication) and tricks (e.g. Karatsuba's multiplication, interleaved carrying and reduction) to be added with less effort than using any other methodology we are aware of.
For example, we implemented (and proved correct) Goldilocks Karatsuba multiplication at a conference lunch break when the author of the technique inquired whether our framework could support it.

Review D
--------

We wholly agree that going from the C code our pipeline generates to speed-record-setting assembly code requires a whole new set of techniques in addition to those described in this paper.
In our opinion, our output language is a natural interface between domain-specific and microarchitecture-specific optimizations, so that the C-to-assembly translation would not need to understand modular arithmetic or limbed representations to produce the best assembly we know to wish for.
As the first reviewer points out, a simple translation validation should be sufficient to connect the assembly code that includes low-level optimizations to the output of our pipeline it was generated from.
In fact, as seen in libsecp256k-c64, it is even possible to improve on our performance within the C language by munging source code (at the same abstraction layer as our generated code) into a form that happens to persuade a particular compiler to produce the desired register allocation and instruction schedule.

Asymmetric cryptography implementations (ours and others') use very simple CPU instructions and cannot benefit from the AES/SHA implementations found on some popular CPUs.

We view producing straight-line code as a strength rather than a limitation.
Any conventional compiler (e.g., gcc, ocamlc) could turn code roughly at the same abstraction layer as our templates into machine code that uses loops and branches, but so far it has taken significant amounts of (duplicative) human effort to produce specialized constant-time code.
We do not generate loops or complicated control-flow logic (e.g., as seen in elliptic-curve scalar multiplication routines that use precomputed tables) because writing (and verifying) the code with loops in the first place is pleasant and reliable when compared to doing the same for specialized field-arithmetic implementations.
Our library already includes verified code with iteration and conditionals, and we have integrated it (with proofs) with our generated field arithmetic.
There is just nothing novel about it.

Review E
--------

The question "In Section 2.3, are we assuming $p < 2^{64}$?" may have been caused by our typo of $\mathbb N_p$ in place of plain $\mathbb N$ on line 333.  Sorry!

On guarantees throughout our pipline: each run of the entire pipeline, from a specification like "a * b mod m" to C code, generates a proof certificate for functional correctness.
For the early phases, the proof is generated anew using tactics on each execution.
The flattening and word-size-selection transformations (the "certified compiler") are proven correct once and for all, and this proof is appropriately linked into the end-to-end proof.

The hand-modified version in section 6.2 is proven equivalent to the generated code it replaces before word-size selection using the `ring` tactic from Coq's standard library.
It is indeed ad-hoc but has an end-to-end Coq-checkable correctness proof against the same specification as the rest of the library, and the proof is very automated, in the tradition of translation validation.
We also intended that manual transformation as an experiment to zero in on the nature of our performance gap with other code; we are not suggesting manual code tweaking as part of the intended usage of a future, improved version of our framework (and it is easily skipped now if being within 10% of best-known performance is acceptable).
