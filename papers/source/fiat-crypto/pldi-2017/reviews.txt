===========================================================================
                           PLDI '17 Review #39A
---------------------------------------------------------------------------
Paper #39: Systematic Synthesis of Elliptic Curve Cryptography
           Implementations
---------------------------------------------------------------------------

                 Reviewer expertise: Z. Some familiarity
                      Overall merit: B. OK paper, but I will not champion
                                        it

                           ===== Strengths =====

+ Strong motivation.
+ Well-written paper, with a good amount of background
+ Strong evaluation

                      ===== Comments to authors =====

The paper is well-motivated. Cryptography code is sophisticated as well as safety-critical, and manually refactoring complex crypto code for new parameter values seems like a recipe for disaster. This makes elliptic cryptography a good candidate for correct-by-construction program development. 

The technical contribution is impressive, in that it evidently took a large amount of work and careful thought. The paper sets good goals for itself -- rigor, effort, and generality -- and adequately evaluates the framework it presents with respect to these goals. 

The question is whether this paper is a good fit for PLDI. The paper is well-written, and gives a good amount of background. However, at the end, much of the paper is about the technical ins and outs of how to build a specific class of verified systems, and that too one that most of the PLDI audience is likely to be unfamiiar with. It would be one thing if there were some reusable insights here that can be used for the verification/development of other kinds of systems, even other kinds of cryptography code. But I am not sure that this is the case. None of this is to denigrate the work, which I find impressive. The question is only whether the paper is a better fit for a domain (crypto) conference.

===========================================================================
                           PLDI '17 Review #39B
---------------------------------------------------------------------------
Paper #39: Systematic Synthesis of Elliptic Curve Cryptography
           Implementations
---------------------------------------------------------------------------

                 Reviewer expertise: Y. Knowledgeable
                      Overall merit: C. Weak paper, though I will not fight
                                        strongly against it

                           ===== Strengths =====

- Clear motivation: writing fast crypto-implementations is difficult.
- Impressive results: within factor of 6 of real-world implementations (at least for handshake)

                          ===== Weaknesses =====

- Lack of detail of the techniques used (e.g., what synthesis techniques were used?  What exactly was synthesized, and from what specification?).  This makes it impossible to evaluate or reproduce.
- No attacker model, no clear indication of the trusted code base

                      ===== Comments to authors =====

The paper identifies implementations of elliptic curve cryptography as an important part of the security infrastructure of the Internet, with implementations required to provide excellent performance.  To achieve these high-performance implementations, complicated low-level strategies are employed that are highly specific to the particular elliptic curve in question, crafted with great effort manually in low-level languages.  Unsurprisingly, these are difficult to get right and a number of bugs have been found historically.  The paper proposes to synthesize these operations from a high-level specification, by using the same implementation tricks as the fastest implementations, but proving them correct in Coq.

The paper explains in detail how modern implementations achieve high performance, through the use of clever variable-base integers, management of carry chains, canonical representations, Barret reduction, etc.  The approach uses a number of abstractions, and eventually produces an "assembly-like" functional program based on let bindings and simple arithmetic operations.  These operations are guaranteed to be free of overflows thanks to a bounds-checker.

Unfortunately, even though the paper gives detailed and interesting insights into what existing implementations do and the kind of tricks they employ, the paper is light on details of the approach taken.  This makes it difficult to evaluate the paper.  For instance, the paper says to **synthesize** an efficient implementation, without ever saying what synthesis approach is used, or even what the specification is that is given to the synthesis procedure.  In fact, the paper reads more like the approach uses a very general hand-crafted implementation and then translates it (through partial evaluation and other techniques to concretely instantiate the general implementation) into a "low-level" functional program.  It does not appear to use synthesis, which is the process of finding an implementation from some form of specification (at least, this is the commonly assumed definition of synthesis, or program synthesis, in the area of programming languages).  In what sense does this paper synthesize an implementation?

Given that the paper is talking about security, it requires an explicit attacker-model.  It appears that the papers implementation gives the same functional guarantees as a pen-and-paper version of the underlying cryptographic algorithms and protects against timing-attacks, but does not defend against other side-channel attacks.  Is this correct?  Which of these properties are formally proven (for instance, is the fact that the implementation is constant-time w.r.t. secret inputs proven formally?)?

The paper also does not say what the trusted code base is.  The paper argues that it has a "smaller trusted code base than in past work, as all of our formal reasoning is done within Coq."  However, it appears that the code that is run is compiled by GHC, so GHC and the Haskell runtime system are presumably also part of the trusted code base.  What else is trusted?  Stating these assumptions explicitly would be useful.

The evaluation talks about three aspects: safety, performance and effort.  The safety subsection can be summarized as "safe by proving so formally" and it is not clear why so many bugs in other systems are described in the paper (it is similar to earlier sections: interesting information, but does not explain the system from the paper).  During the development of the system, two small discrepancies in other systems have been uncovered.  The performance section compares against a number of existing systems, and outperforms other formally verified systems significantly.  For the handshake, the papers system is within a factor of 6 of state-of-the-art implementations, which is impressive.  For signing, however, there is still a 80x difference between the paper and the fastest implementations available.  This 80x slowdown seems to be swept under the rug (e.g., the abstract only mentions the 6x figure).


More questions to the authors:

- The paper highlights several times how currently implementations have to be written from scratch for every set of new parameters.  How often do these parameters actually change?  Would they change more often if the cost of reimplementing high-performance implementations was lower?

- If a new curve wants to be supported using the system described in the paper, how much manual effort is necessary and how much can be done automatically.  The paper seems to indicate it is mostly automatic, but scattered throughout the paper are places that look like manual intervention is necessary (e.g., 4.4 "choosing which formulas to use is not within the scope of our library", or 4.2 "guessing the right bounds is beyond the scope of this paper").

===========================================================================
                           PLDI '17 Review #39C
---------------------------------------------------------------------------
Paper #39: Systematic Synthesis of Elliptic Curve Cryptography
           Implementations
---------------------------------------------------------------------------

                 Reviewer expertise: Y. Knowledgeable
                      Overall merit: C. Weak paper, though I will not fight
                                        strongly against it

                           ===== Strengths =====

- framework for synthesis of efficient code for elliptic curve cryptography optimized at low level for 
efficient use of CPU registers; it is written in Coq

- this framework should enable easy generation of optimized implementation for any set of specific
parameters which change the field that is used and the representation of elements in it

- the authors provide evaluation with the EdDSA signature algorithm and demonstrate that their
synthesis tool would avoid several existing bugs in knows implementations of the algorithm

                          ===== Weaknesses =====

- the presentation of the paper does not clearly present the actual contributions and optimization techniques
that are leveraged in the synthesis tool

- the synthesized code for EdDSA is slower than some manually optimized implementations.

                      ===== Comments to authors =====

Cryptographic operation are often some of the most computationally expensive primitives in existing implementations 
for secure an authenticated communication. That is why optimizing implementation of cryptographic algorithms is an
important task especially when these algorithms will be used many times in the execution of an application. The goal
of this paper is to construct a synthesis tool that starts from a high level specification of a cryptographic algorithm and
provides highly optimized low level specification of an implementation for a specific set of parameters. This tool will replace
a lot of manual effort for optimizing implementations and will facilitate the instantiation of a particular algorithm with
many different parameter settings, which may require different representation of the underlying structures. 

The main focus of the work is to build a proof-generating synthesis for big-integer representation and arithmetic. For the
representation of big integers the paper looks at a decomposition of the value into digits where each digit is a different base,
i.e. it carries different numbers of bits information. In the paper these are called limbs and limb width. Since each limb will
be stored in a separate register the difference between the width of the register and the limb width can remove the need to carry
bits across registers at each computation step, hence can allow parallel execution. Reasoning about the need of carry bits and
how information should be distributed across different limbs can be kept in carry chain, which can be applied all at once.
These, however, creates issues related several possible representations for the same number. The authors point out that efficient
conversion to a canonical form is complicated but it remains unclear to me how they handle this issue. In fact it is also not quite
clear how they reason about carry chains and use these tools together with the settings of limb length in their optimization 
techniques. 

Section four in paper which describes how low level code is synthesized is less than two pages long and I wish it provided
greater level of detail. For example, the paragraph on partial evaluation mentions how a recursive multiplication becomes a
chain of simple operations, but it remains unclear whether this is all that it is used for. The authors mention that they had further
engineering challenges without much elaborations, which makes it hard to evaluate the contribution.The rest of the section talks
about bound checking; at one point there is a mention that since guessing correct bounds is hard they will be given as input, does
 this mean that the bounds need to be manually computed? This section also mentions double word operations that allow to construct operations on n-bit integers from operations on n/2 bit integers as well as Barrett and Montgomery form reductions which allow for efficient modulo reductions. I assume that these techniques were all incorporated as synthesis rules but I wish I 
saw some discussion when they were useful, did they constitute all major techniques for improvement of modular operations, how 
different components fit together.

The case study for the synthesis tools is the EdDSA signature algorithm. The authors analyzed deployed software with different
implementation of the algorithms as well as several functions-correctness bugs that have been identified in these implementations.
They argue that their synthesis tools provides provisions that guarantee that the produced implementation specification could not
have contained the same types of bugs. The verification that tool from this paper offers was used to discover subtle issue in a
proposed modification the Barrett-style modulo reduction. It also has identified an issue with an implementation of the algorithm 
with a particular curve that has points of different orders while some computation optimizations work only using points with one 
of the orders. In the evaluation the authors also compare the efficiency of the code produced by the synthesis tool
compared to other existing implementations. From the results presented in Table 2 and 3 it seems that the tool produces code
for Diffie-Hellman handshake that runs better than some other implementations while it is still slower than the best implementations. In the case EdDSA signing the code from the tool seems to be substantially slower than other implementation 
but I did not find much discussion about this in the paper.

I think the goal of this paper of facilitating the synthesis of optimized and verified code specifications for cryptographic algorithms
is very important. The evaluations results for efficiency seem concerning since they do not seem to be on par with other 
implementation while the main motivation for the work is improving the efficiency of existing implementations without requiring
manual efforts.

===========================================================================
                           PLDI '17 Review #39D
---------------------------------------------------------------------------
Paper #39: Systematic Synthesis of Elliptic Curve Cryptography
           Implementations
---------------------------------------------------------------------------

                 Reviewer expertise: Y. Knowledgeable
                      Overall merit: B. OK paper, but I will not champion
                                        it

                           ===== Strengths =====

+ Captures the whole early phase of producing an optimized
implementation, including many tasks that are cumbersome and
error-prone by hand

+ Does not require any code specific to the choice of a prime or
elliptic curve within standard families

+ Coq-based approach gives a small TCB for the verification steps

                          ===== Weaknesses =====

- "Synthesis" is mostly templating and partial evaluation, and does
not automate some complex parameter choices like carry chains

- There is still a significant gap, in both format and performance,
between the output and the practical implementations it aims to
replace

- The practical advance over previous similar efforts (gfverif [10],
[16], ECC-star [35]) is fairly modest

                      ===== Comments to authors =====

This paper presents an approach for generating high-performance
implementations of ECC primitives by a verified derivation process in
Coq which contains parametric implementations of standard
optimizations. The end result is code in a restricted subset of
Haskell close to C or machine code, which in some cases gets the
better part of the way in performance between unoptimized reference
implementations and the hand-optimized code currently used in
practice. What the paper describes as "synthesis" does not require
significant search, but a programmer merely supplies a couple of lines
of numeric parameters, which the tool transforms into complex code
which is specialized for those parameters. The paper surveys a number
of implementation bugs which would have been avoided by its technique,
and gives performance comparisons for X25519 Diffe-Hellman and Ed25519
signing. The better result for X25519 is about 5.5 times slower than
the best hand-written code; the factor for Ed25519 is larger (about
80x) due to more missing optimizations.

Cryptographic primitives are fairly small pieces of code where even
subtle mistakes can have wide-ranging consequences, so they represent
a natural target for research on techniques to support correctness
such as verification and synthesis. The asymmetric ECC primitives
considered here require complex optimizations to get good performance,
but the optimizations are mostly mathematical so they are a good match
to the capabilities of decision procedures and proof assistants.
Several previous research projects have targeted some of these same
ECC operations, but this paper advances the state of the art
particularly for what you might call the earlier phases of the
derivation, from the simplest mathematical spec through detailed
algorithm design. The paper encodes many of the key optimization ideas
used in such implementations as program equivalences, and applies them
together with partial evaluation to automate the derive the
implementation of a provably correct optimized implementation from
just a few parameters. However there are two key senses in which the
work does not yet automate the end-to-end implementation process.
First, the final program format the work currently produces is Haskell
code extracted from Coq. The code is in a fragment of Haskell that
doesn't use many more features than those available in the mixture of
C and assembly that is commonly used in practical implementations, but
some low-level optimizations and security concerns can't be expressed
in this format. Second, of the many optimzations needed to get high
performance, not all are yet implemented, leaving performance about
5x-80x behind the state of the art.

Similar primitives have been used as case studies in several previous
verification projects. The results of this paper seem to me to be an
improvement over the state of the art in several ways. The most
relevant point of comparison is the ECC-star library [35]: that work
is recent, and has a similar focus on the higher-level phases of
derivation using a functional language instead of C or assembly. I
would agree that the paper's work represents the following advances:

+ By implementing more mid-level optimizations and using a
lower-overhead functional compiler, this work shows important speedups
relative to [35] (around 50x for X25519), albeit still short of fully
practical performance.

+ By automating some optimizations in the form of code transformations
rather than just a library, this paper's approach avoids any need for
writing code specific to a particular-size elliptic curve. By
comparison [35]'s design reduced and refactored but did not eliminate
curve-specific code. In this paper's approach a developer must still
choose some parameters that represent somewhat similar design effort
to the curve-specific code in [35], but the effort is less and the
declarative parameters are not trusted for security: they're only
relevant for performance.

+ By building on Coq instead of the F* verification stack, this
paper's security assurance for the high level stages of derivation has
a much smaller TCB. However there the approaches are similar in the
low-level stages, as both depend on functional language compilers and
runtimes (OCaml vs. GHC).

On the other hand, there are still a number of ways in which this work
falls short of what we would eventually want in such a derivation:

- Though this paper implements a number of algorithmic optimizations,
some important ones are still missing, particularly as seen in the
Ed25519 case study. So some significant effort will still be needed
before this approach is within shooting distance of practical usage.

- The derivation does not reach all the way to C or assembly language,
which detracts from both performance and security assurance. The paper
estimates that about a 40% performance gap derives from
instruction-level optimizations which cannot be expressed in their
functional language. Also, it is crucial for security against
side-channel attacks that the running time of an implementation be
independent of the secret key material. Though some of the algorithms
the paper uses are based on this criterion, the functional language is
again not suitable to express it and the derivation process cannot
provide a guarantee. Several other previous projects do verification
directly at the level of low-level code and so get results that are
stronger in this respect, though as a trade-off the properties
verified are currently less general. Executing the implementations in
a functional language also means that language's infrastructure is
part of the TCB. C code compiled by a mainstream compiler would have a
similar issue; ultimately either a verified compiler or assembly-level
verification would give the best assurance.

Another factor that seems less of a practical issue but still feels
like a disappointment compared to the paper's title is that the
described "synthesis" does sound like it involves search or automates
the design steps that require the most insight. There is perhaps not a
big gap between an advance compiler or specialized code generation
system, and a program synthesis system; it's somewhat a choice of
terminology, and "synthesis" is currently in vogue. But in my usage I
would prefer to reserve "synthesis" for situations where the automated
system performs a fairly undirected search within a large space of
programs to replace a design step that we would think of as requiring
insight or creativity when performed by a human developer.
Optimizations are also searches through the space of programs, but
they embody a more directed strategy which is designed by the
optimization implementor. The code transformations currently performed
by this system seem more like optimizations or domain-specific code
generation than what I would call synthesis; but that's not a critcism
if they achieve the results we want. Some of the side parameters that
are currently supplied by a developer, such as the choice of a carry
chain, seem like natural targets for synthesis of a more general
nature, which the paper argues would be required because no general
methodology for choosing them is known. In a sense the paper already
represents part of such an implementation, since it can generate the
implementation from a given set of parameters: one would just need to
add automatic benchmarking and a search over the parameter space.
Practically speaking this would not be a high-impact improvement
because good parameters are already known for the most important
curves, though the same could be said about the paper's generality
over curve parameters.

I like that the way the paper in section 6.1 surveys historical bugs
in practical implementations: it helps confirm with some empirical
data the improvements in security we can reasonably hope for from more
rigorous development approaches. Among the extra material in the
overflow PDF, I found appendix B on proof techniques more interesting
that appendix A on ECC background. Though there is some value in
making the paper more self-contained, I found the more limited amount
of background currently in the paper proper to be sufficient for
understanding its contributions.

One more superficial point: I think the use of a fractional value of
"limbwidths" in tables 2 and 3 could use some more explanation, given
that one would normally expect that the width of a limb is always an
integer. From the context that appears elsewhere in the paper, I
presume the idea is that 25.5 is the average limbwidth, and the intent
is that the width are as close as possible to equal subject to the
constraint of being integers, for instance the example given
explicitly elsewhere as [26, 25, 26, 25, 26, 25, 26, 25, 26, 25].
Perhaps you could use a more explicit notation: for instance I think
even keeping the table in one column and not expanding the font size
you could say "(51)^5" or "(26, 25)^5", where the latter is then
distinct from "(25, 26)^5", "(26)^5, (25)^5", or "15, (16)^3, (32)^6",
which would also have average limbwidth 25.5.

I don't have any specific questions to highlight for the author(s) to
answer in a response.

===========================================================================
                Response by Jade Philipoom <jadep@mit.edu>
---------------------------------------------------------------------------
# General Clarifications

## "Synthesis"

It's true that our pipeline is "synthesis" in only a limited sense, and that
was evidently an unfortunate choice of wording for a PLDI audience. We process
code in two steps: (1) verifying optimizations in a generic way and (2)
compiling code (with specific parameters instantiated) that applies those
optimizations. The first is done mostly manually, but is done once for wide
classes of curves. The second is done mostly automatically, with minimal human
input (specifically, a JSON file like the one shown in section 6.3). There is
no search process involved.

## Haskell

Our output was a syntax tree of simple, assembly-like operations[0]. The
performance-critical leaf functions could easily be pretty-printed to C or
any other language that supports fixed-size integers. The final product does not
depend on any specifics of Haskell or GHC, though for convenience we used them
for the performance experiments reported in the paper. Writing a verified compiler that
performs efficient register allocation and produces code for a sufficient subset
of architectures would be another project in itself.

## Scope

We'd like to clarify which formal claims we aim to establish for particular
generated pieces of code.  Given a cryptography implementation, there are two main
kinds of security vulnerabilities to worry about.

1. The protocol being implemented might not have the properties it claims.
2. The implementation may fail to correctly implement the protocol.

We are focused entirely on the second question. Implementation correctness is a
significant source of bugs and, due to intense optimization efforts, contains
complex reasoning. However, there is no real "attacker model" needed for this
problem.

## "Side channels"

We do not currently verify that our code runs in constant time, or that it does
not expose its secret inputs through some other means. Such verification would
be fundamentally different from ours, primarily because it would heavily depend
on the details of the deployment scenario. For example, processors implementing
the same instruction set can differ in which instructions leak their inputs
into timing, and CPU vendors as a rule do not publish documentation regarding
this. However, pretty-printing our low-level functions as C instead of Haskell
and running ct-verif to analyze that output would indeed clear it as "constant
time" in a common timing model.

## Performance

Some reviewers pointed out an 80x gap between our ed25519 signing implementation
and the best manual ones, which is much larger than the 6x gap for the handshake
benchmark that we focused on. This is because our ed25519 code consisted of
plugging our optimized code for subroutines into a verified but completely naive
implementation of the high-level signature scheme. We included the ed25519
implementation to show that the formal specifications of our code allow for
verified generic composition; we did not make any attempt at optimizing the
signature-specific high-level code. That said, the missing optimizations are
conceptually very similar to the ones we did implement, and we believe that it
would be rather straightforward to extend our library to include them.

Another comment was that the 6x gap for the handshake benchmark is still too big
for practical use. This is true for some cases of "practical use," but we are
very confident that our approach has not hit a wall at this speed. There are
clear improvements to make that would close this gap. Furthermore, the 6x gap is
between our implementation and the very fastest of the handwritten ones. Our
code is only 40% slower than the code used in Chrome, in part because the
developers have significantly more confidence in the correctness of that code
compared to the fastest assembly code, whose slight variant "64-24k" contained
an elusive arithmetic bug as shown in table 1.

# Answers to Remaining Questions

**What reusable insights does this paper provide for similar future tools?**

We believe our strategy generalizes to any arithmetic-heavy domain (including
those listed in the end of section 8), but as we have no evidence, just an
(informed) guess, we leave the determination of whether our method would suit
some particular application to the reader interested in that application.

**What exactly is the TCB?**

Coq and our whiteboard-level specs. As explained above, we require some kind
of tool that does register allocation and does the final translation to a
machine-specific assembly. So, if the tool used for that task is unverified,
it is part of the TCB for that particular instantiation of code. Although we
use GHC for this purpose in our benchmarks, it's not the case in general that
GHC is part of our TCB. For a realistic use of our code, we would recommend
pretty-printing it into the language used by the overall project.

**How often do curve parameters change, and would they change more often if
there were a lower cost to reimplement high-performance implementations?**

The mathematical parameters have changed once in TLS (from a set of 4 to a set
of 2), and gaining adoption required the proposer to write optimized
implementations for every conceivable platform. The performance-oriented
parameters like carry chains and word size change when code is ported from one
CPU microarchitecture to another -- OpenSSL currently contains several different
implementations of arithmetic modulo P256.


[0] Full definition of output syntax tree as found in the submitted code
tarball (the type variable `tZ` is instantiated with a type representing
machine words in the final output):

In `src/Reflection/Syntax.v`, lines 186-191:
~~~
Inductive exprf : flat_type -> Type :=
| Const {t : flat_type} : interp_type t -> exprf t
| Var {t} : var t -> exprf t
| Op {t1 tR} : op t1 tR -> exprf t1 -> exprf tR
| LetIn : forall {tx}, exprf tx -> forall {tC}, (interp_flat_type_gen var tx -> exprf tC) -> exprf tC
| Pair : forall {t1}, exprf t1 -> forall {t2}, exprf t2 -> exprf (Prod t1 t2).
~~~

In `src/Reflection/Z/Syntax.v`, lines 34-44:
~~~
Inductive op : flat_type base_type -> flat_type base_type -> Type :=
| Add : op (tZ * tZ) tZ
| Sub : op (tZ * tZ) tZ
| Mul : op (tZ * tZ) tZ
| Shl : op (tZ * tZ) tZ
| Shr : op (tZ * tZ) tZ
| Land : op (tZ * tZ) tZ
| Lor : op (tZ * tZ) tZ
| Neg (int_width : Z) : op tZ tZ
| Cmovne : op (tZ * tZ * tZ * tZ) tZ
| Cmovle : op (tZ * tZ * tZ * tZ) tZ.
~~~

