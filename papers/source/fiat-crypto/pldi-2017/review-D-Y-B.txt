===========================================================================
                           PLDI '17 Review #39D
---------------------------------------------------------------------------
Paper #39: Systematic Synthesis of Elliptic Curve Cryptography
           Implementations
---------------------------------------------------------------------------

                 Reviewer expertise: Y. Knowledgeable
                      Overall merit: B. OK paper, but I will not champion
                                        it

                           ===== Strengths =====

+ Captures the whole early phase of producing an optimized
implementation, including many tasks that are cumbersome and
error-prone by hand

+ Does not require any code specific to the choice of a prime or
elliptic curve within standard families

+ Coq-based approach gives a small TCB for the verification steps

                          ===== Weaknesses =====

- "Synthesis" is mostly templating and partial evaluation, and does
not automate some complex parameter choices like carry chains

- There is still a significant gap, in both format and performance,
between the output and the practical implementations it aims to
replace

- The practical advance over previous similar efforts (gfverif [10],
[16], ECC-star [35]) is fairly modest

                      ===== Comments to authors =====

This paper presents an approach for generating high-performance
implementations of ECC primitives by a verified derivation process in
Coq which contains parametric implementations of standard
optimizations. The end result is code in a restricted subset of
Haskell close to C or machine code, which in some cases gets the
better part of the way in performance between unoptimized reference
implementations and the hand-optimized code currently used in
practice. What the paper describes as "synthesis" does not require
significant search, but a programmer merely supplies a couple of lines
of numeric parameters, which the tool transforms into complex code
which is specialized for those parameters. The paper surveys a number
of implementation bugs which would have been avoided by its technique,
and gives performance comparisons for X25519 Diffe-Hellman and Ed25519
signing. The better result for X25519 is about 5.5 times slower than
the best hand-written code; the factor for Ed25519 is larger (about
80x) due to more missing optimizations.

Cryptographic primitives are fairly small pieces of code where even
subtle mistakes can have wide-ranging consequences, so they represent
a natural target for research on techniques to support correctness
such as verification and synthesis. The asymmetric ECC primitives
considered here require complex optimizations to get good performance,
but the optimizations are mostly mathematical so they are a good match
to the capabilities of decision procedures and proof assistants.
Several previous research projects have targeted some of these same
ECC operations, but this paper advances the state of the art
particularly for what you might call the earlier phases of the
derivation, from the simplest mathematical spec through detailed
algorithm design. The paper encodes many of the key optimization ideas
used in such implementations as program equivalences, and applies them
together with partial evaluation to automate the derive the
implementation of a provably correct optimized implementation from
just a few parameters. However there are two key senses in which the
work does not yet automate the end-to-end implementation process.
First, the final program format the work currently produces is Haskell
code extracted from Coq. The code is in a fragment of Haskell that
doesn't use many more features than those available in the mixture of
C and assembly that is commonly used in practical implementations, but
some low-level optimizations and security concerns can't be expressed
in this format. Second, of the many optimzations needed to get high
performance, not all are yet implemented, leaving performance about
5x-80x behind the state of the art.

Similar primitives have been used as case studies in several previous
verification projects. The results of this paper seem to me to be an
improvement over the state of the art in several ways. The most
relevant point of comparison is the ECC-star library [35]: that work
is recent, and has a similar focus on the higher-level phases of
derivation using a functional language instead of C or assembly. I
would agree that the paper's work represents the following advances:

+ By implementing more mid-level optimizations and using a
lower-overhead functional compiler, this work shows important speedups
relative to [35] (around 50x for X25519), albeit still short of fully
practical performance.

+ By automating some optimizations in the form of code transformations
rather than just a library, this paper's approach avoids any need for
writing code specific to a particular-size elliptic curve. By
comparison [35]'s design reduced and refactored but did not eliminate
curve-specific code. In this paper's approach a developer must still
choose some parameters that represent somewhat similar design effort
to the curve-specific code in [35], but the effort is less and the
declarative parameters are not trusted for security: they're only
relevant for performance.

+ By building on Coq instead of the F* verification stack, this
paper's security assurance for the high level stages of derivation has
a much smaller TCB. However there the approaches are similar in the
low-level stages, as both depend on functional language compilers and
runtimes (OCaml vs. GHC).

On the other hand, there are still a number of ways in which this work
falls short of what we would eventually want in such a derivation:

- Though this paper implements a number of algorithmic optimizations,
some important ones are still missing, particularly as seen in the
Ed25519 case study. So some significant effort will still be needed
before this approach is within shooting distance of practical usage.

- The derivation does not reach all the way to C or assembly language,
which detracts from both performance and security assurance. The paper
estimates that about a 40% performance gap derives from
instruction-level optimizations which cannot be expressed in their
functional language. Also, it is crucial for security against
side-channel attacks that the running time of an implementation be
independent of the secret key material. Though some of the algorithms
the paper uses are based on this criterion, the functional language is
again not suitable to express it and the derivation process cannot
provide a guarantee. Several other previous projects do verification
directly at the level of low-level code and so get results that are
stronger in this respect, though as a trade-off the properties
verified are currently less general. Executing the implementations in
a functional language also means that language's infrastructure is
part of the TCB. C code compiled by a mainstream compiler would have a
similar issue; ultimately either a verified compiler or assembly-level
verification would give the best assurance.

Another factor that seems less of a practical issue but still feels
like a disappointment compared to the paper's title is that the
described "synthesis" does sound like it involves search or automates
the design steps that require the most insight. There is perhaps not a
big gap between an advance compiler or specialized code generation
system, and a program synthesis system; it's somewhat a choice of
terminology, and "synthesis" is currently in vogue. But in my usage I
would prefer to reserve "synthesis" for situations where the automated
system performs a fairly undirected search within a large space of
programs to replace a design step that we would think of as requiring
insight or creativity when performed by a human developer.
Optimizations are also searches through the space of programs, but
they embody a more directed strategy which is designed by the
optimization implementor. The code transformations currently performed
by this system seem more like optimizations or domain-specific code
generation than what I would call synthesis; but that's not a critcism
if they achieve the results we want. Some of the side parameters that
are currently supplied by a developer, such as the choice of a carry
chain, seem like natural targets for synthesis of a more general
nature, which the paper argues would be required because no general
methodology for choosing them is known. In a sense the paper already
represents part of such an implementation, since it can generate the
implementation from a given set of parameters: one would just need to
add automatic benchmarking and a search over the parameter space.
Practically speaking this would not be a high-impact improvement
because good parameters are already known for the most important
curves, though the same could be said about the paper's generality
over curve parameters.

I like that the way the paper in section 6.1 surveys historical bugs
in practical implementations: it helps confirm with some empirical
data the improvements in security we can reasonably hope for from more
rigorous development approaches. Among the extra material in the
overflow PDF, I found appendix B on proof techniques more interesting
that appendix A on ECC background. Though there is some value in
making the paper more self-contained, I found the more limited amount
of background currently in the paper proper to be sufficient for
understanding its contributions.

One more superficial point: I think the use of a fractional value of
"limbwidths" in tables 2 and 3 could use some more explanation, given
that one would normally expect that the width of a limb is always an
integer. From the context that appears elsewhere in the paper, I
presume the idea is that 25.5 is the average limbwidth, and the intent
is that the width are as close as possible to equal subject to the
constraint of being integers, for instance the example given
explicitly elsewhere as [26, 25, 26, 25, 26, 25, 26, 25, 26, 25].
Perhaps you could use a more explicit notation: for instance I think
even keeping the table in one column and not expanding the font size
you could say "(51)^5" or "(26, 25)^5", where the latter is then
distinct from "(25, 26)^5", "(26)^5, (25)^5", or "15, (16)^3, (32)^6",
which would also have average limbwidth 25.5.

I don't have any specific questions to highlight for the author(s) to
answer in a response.

