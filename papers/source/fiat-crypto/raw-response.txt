1.

>~ why is OpenSSL X25519 so slow?

Because it does not use 64x64->128-bit multiplication, for portability to machines where that is not natively available.

2.

> Where is the compiler used?

They provide the inputs shown in figure 1, receiving the outputs shown in figure 1. A lower-level interface roughly corresponding to the structure of section 4 is available for implementation of new arithmetic strategies.

> What is figure 1 showing? Section 2 talks...

Figure 1 belongs with section 1 and shows the fastest implementation our end-to-end pipeline chose for NIST-P256. It uses the "word-by-word Montgomery reduction" strategy described in section 4.3.1.

3. 

-- Expresses skepticism that crypto library hackers will use our code.

4.

-- C compiler bad: increases TCB, too generic.

We wholly agree that going from the C code our pipeline generates to speed-record-setting assembly code requires a whole new set of techniques in addition to those described in this paper.
However, we believe that our compiler encompasses all fundamentally domain-specific optimizations: for example, the C-to-assembly translation would not need to understand modular arithmetic or limbed representations to produce the best assembly we know to wish for.
In fact, as seen in libsecp256k-c64, it is even possible to improve on our performance within the C language by munging source code (at the same abstraction layer as our generated code) into a form that happens to be cause good register allocation and instruction scheduling in a particular compiler.

All operations in the output language of the compiler are implemented in
constant time on many commodity processors (stated on line 953 in the paper).

We are not aware of any popular ISA providing support for public-key cryptography.
The AES/SHA instructions available on some platforms are not useful here.

The measurements in this paper were reported for the optimization configuration that produced the (empirically) fastest code, using a simple, conservative heuristic for carry chain selection.
A lower-level API that allows manual selection of optimization parameters is available as well.
The true challenge of the optimizations lies in defining the correct parameter space and using it to generate code (section 4), not in finding the optimal parameters.

5.

For code submission accompanying this paper we manually wrote the arithmetic templates in continuation-passing style.
This was because we (in hindsight, foolishly) used dependent types, and it is an open problem to reflect syntax-trees into dependently-typed code inside Coq, which we had needed to do to integrate a verified CPS transformation.

The choice of elliptic curve representation is not automated because it involves an outward-facing trade-off between performance, the size of precomputed tables, and how many functions can share these tables.
If our compiler was operating on an entire cryptographic library at once and took as input a code size constraint, it would be simple (and feasible) to choose the fastest representation by trying all of them.

We agree that the highest-level API presented in this work is not widely
applicable -- its main purpose is to demonstrate the level of automation
involved.
Underlying it is a library of generic implementation strategy
templates, enabling reuse far beyond the current practice in cryptographic implementation.
We hope that the reuse this library enables makes it an
attractive tool for experimenting with alternate implementations even for crypto implementation experts with limited Coq experience.

In section 2.3, "list N_p" should have been "list N"?

> * Line 153: "The only input is a prime number". Compilers usually also take code.  Please address Q2 in the Questions for Authors.

> I'd like to know more about what primitives you actually generate. I looked at your artifact and saw that it asked to generate primitives like "feadd". And you write that your example code is in "some unspecified functional language". But I don't have a good sense as to what you can take as input code. The functions aren't hardcoded, I hope.

The first pass of the compiler is to specialize a Gallina function to concrete parameters so that only integer operations remain.
The "input language" is thus "templates of integer arithmetic, expressed in Gallina".
When we said that the input is a prime number, we meant that we meant that we compile our modular arithmetic template specialized to that prime number.

The hand-modified version in section 6.2 is proven equivalent to the generated code it replaces before word size selection using the "ring" tactic from Coq standard library.
It indeed ad-hoc, but has an end-to-end Coq-checkable correctness proof against the same specification as the rest of the library.

The each run of the entire pipeline, from the specification "a * b mod m" to C code generates a functional correctness proof certificate.
For the early phases, the certificate is generate anew using tactics on each execution, the compiler phases are proven correct once and forall (and this proof is appropriately linked into the end-to-end proof).
