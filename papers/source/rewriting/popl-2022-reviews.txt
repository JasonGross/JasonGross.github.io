POPL 2022 Paper #19 Reviews and Comments
===========================================================================
Paper #19 Accelerating Verified-Compiler Development with a Verified
Rewriting Engine


Review #19A
===========================================================================

Overall merit
-------------
A. Strong Accept

Reviewer expertise
------------------
Y. Knowledgeable

Paper summary
-------------
The paper presents the development and implementation of a verified (compiler) compiler from a high-level functional language to an intermediate representation suitable for translation to low-level languages.
It uses rewrite rules and NbE as its main mechanics and is extensively tested against other such developments in the literature. The integration of the compiler into Fiat Cryptography significantly sped up the code generation, and thus made it possible to generate more primitives.

Strengths
---------
- The tool is modular, in the sense that compilers generated by the tool can be extended effortlessly. Providing new rewrite rules does not require one to change the correctness proof.
- The tool has made it possible to use Fiat Cryptography for more primitives, which have been deployed in industry. This has had a big impact on the industry.
- While the main application presented is to Fiat Cryptography, the tool is general enough to be used for efficient partial evaluation of Gallina terms in other contexts.
- Based on the micro- and macrobenchmarks, the work seems to scale much better than previous work in the same direction.

Weaknesses
----------
- The only use-case presented is Fiat Cryptography (and a small motivating example in section 2)

Comments for author
-------------------
line 232: This sentence seems to be missing a word.
line 749: You refer to the developers of Fiat Crypto as 'we'; this is inconsistent with other places in the paper.
line 798: Please define IR (I suppose it is 'intermediate representation')

Questions for the response period
---------------------------------
- The authors claim that the tool is general purpose, but the only two examples are either very small (section 2) or very large (Fiat Cryptography). Could you elaborate on how this tool can be used by a general Coq-user and how efficiently?



Review #19B
===========================================================================

Overall merit
-------------
C. Weak Reject

Reviewer expertise
------------------
Y. Knowledgeable

Paper summary
-------------

The paper describes an efficient, compiled implementation of rewriting within the Coq proof assistant, based on reification of the rewrite rules (initially proved as Coq lemmas), precomputation of a decision tree, and normalization by evaluation using a parametric HOAS (PHOAS) representation.  Application to the Fiat Cryptography code generation library demonstrates major reductions in code generation times (10-1000x).

Strengths
---------

+ Intriguing approach to efficient rewriting "within" Coq, based on reflection and specialization.
+ Impressive speedups when applied to the Fiat Crypto system.

Weaknesses
----------

- The paper is poorly written, with too much hype and high-level design points and not enough description of the actual algorithms.
- As a consequence, I don't think it is possible to implement the same ideas independently just from the description given in the paper.
- The conceptual advance, if any, should be better stated.  Right now the paper feels like a combination of known approaches.

Comments for author
-------------------

- p.1 "compilers from functional languages to C".  This is oddly specific and exclude both CompCert and CakeML.  Certicoq (by Appel et al) is a compiler from a functional language to C, but your approach doesn't seem applicable to Certicoq either.

- Section 2, line 217, and elsewhere in the paper: it is unclear what strategy the rewriter follows, generally speaking, and more specifically why "do_again" is needed.

- Section 3.3: the motivating example could be treated by first beta-reducing to $z + 0$, then rewriting as usual.  I thought you wanted to demonstrate rewriting under lambdas?  In which case, $\lambda x. x + 0$ would be a better example.

- Section 4.2: yes, subterm sharing is crucial, but sometimes terms must be unshared so that rewriting / reduction can proceed.  (That's the optimal reduction dilemma.)  Strategies such as "let-reduce when the bound term is a constant" are simplistic.  There are many other heuristics in the literature on compiling functional languages, e.g. Appel's book
"Compiling with continuations", but maybe what is needed is ways for users to control "let" reductions?

- Section 4.3: what about conditional rewriting rules with arbitrary Boolean guards (like "when" patterns in OCaml)?  or Agda's "with abstractions"?

- Section 4.4 and section 5.1.4: it is really unclear how bit-width of parameters and intermediate results is being tracked.  I would have expected types "bits(N)" to be used for this purpose, with specific passes to infer appropriate bit widths, but this is not what happens, apparently.

Questions for the response period
---------------------------------

- What rewriting strategies are implemented? How are they controlled by the user?

- What is the generality of the approach?  Have you considered other applications beyond the Fiat Crypto system?

- What about conditional rewriting rules in the style of OCaml's "when" or Agda's "with abstractions"?



Review #19C
===========================================================================

Overall merit
-------------
C. Weak Reject

Reviewer expertise
------------------
Z. No familiarity

Paper summary
-------------
This paper describes a range of improvements to the "verified rewriting" approach to compiler development. The idea of phrasing a compiler as a set of rewrite rules has been explored before; it is an appealing idea because each rewrite can be proven correct individually, and those rewrites can then be applied by a generic rewriting engine in any order.

What's new in this paper is that a previous incarnation of the idea (from Aehlig et al.) has been improved so much that it is now feasible (and indeed, attractive) to apply the verified rewriting approach to a "real-world" compiler -- in this case, the Fiat Cryptography compiler from Erbsen et al.

The key improvements are:
- Aehlig et al.'s technique involves growing the trusted computing base (I think because the rewriting engine itself is not verified?) whereas this new approach does not.
- Aehlig et al.'s approach does not handle partial evaluation, which is handy in the Fiat Cryptography setting because it can be used to specialise the compiler for particular elliptic curves. The new approach handles partial evaluation.

The approach is applied to a handful of microbenchmarks to demonstrate its scalability, and also to the Fiat Cryptography compiler, which becomes about 10x-1000x faster as a result.

Strengths
---------
- The paper is immaculately written. I found just a single typo -- there's a missing space on line 890. (Other tiny gripes: line 7 doesn't need a comma after "implement", Web -> web on line 52, you might want an "and" before "conclude" on line 259, and there's some inconsistent hyphenation between "frontend" and "middle-end".)

- Very much on-topic for POPL -- there is a lot of interest in verified compiler development these days, and I like the way that this paper takes more of a "meta" view of the topic by not just _building_ another verified compiler, but reflecting on how the process of building verified compilers can be improved.

Weaknesses
----------
- The paper is very technical. There's not much here for the non-expert. For that reason, I'm wondering whether the paper would be a better fit in a venue that's more tightly focused on theorem provers and/or verified compilers.

- It's quite hard for me as a relative outsider to separate this contribution from those that have gone before. For instance, the paper compares against Malecha and Bengtson [2016] by saying that rewriting rules can now be "written in natural Coq form" -- is this a big deal? It's hard for me to judge whether that matters.

- Relatedly, I'm not sure about the claim (line 116) that Aehlig et al. do not grow the trusted computing base. Aehlig et al. claim (page 2 of their paper) that their approach does not grow the trusted base "if the compiler used at runtime is the same as the one compiling the theorem prover". For your setting, I guess this is `ocamlc` in both cases, and so the condition holds? (Perhaps you could clarify in your author response?)

- I think my biggest concern about this paper is how closely coupled it is with the specific case-study, Fiat Cryptography. The paper is presented as a general technique for building verified compilers that has been successfully applied to Fiat Cryptography, but the impression I get is kinda the reverse: that really the work is about improving the Fiat Cryptography compiler. For instance, I'm not sure that there would be so much focus on partial evaluation if the focus were on verified compiler development in general, because I think partial evaluation is unusually important in the cryptography setting. And the focus on "compilers from functional languages to C" (line 25) similarly feels a bit arbitrary (but makes sense if you put "improving Fiat Cryptography" as the paper's primary contribution). The impact of all this is that I worry about the applicability of the techniques outside of Fiat Cryptography.

Comments for author
-------------------
- I was quite confused by the third paragraph of the intro. The paragraph is explaining the scalability challenges of _proof-producing_ compilers. But the next paragraph goes on to say that this paper is about a _proven-correct_ compiler, which doesn't have these challenges. So I didn't understand why that third paragraph is necessary. The reader has already been primed to expect proven-correct compilers from the very first line of the introduction, so can't proof-producing compilers be deferred to Related Work?

- I was a bit surprised that Hickey and Nogin's [2006] work on compilers built using rewrite rules is criticised for its rewriting engine being _unproven_. As I understand Hickey and Nogin's work, that's kinda the point -- once the rewrites are proven, it doesn't matter which order they're applied in, so the rewriting engine can be unproven (and hence easier to change, etc). But your point about them not supporting side-conditions seems believable.

- The graphs in Figure 3 are pretty ugly because they have so many data points packed together. It is nonetheless possible to distinguish the various series involved though, with a bit of geometrical deduction. And I suppose you'd lose some information if you thinned out the data points or plotted lines only. So I don't really have a concrete suggestion for improving the graphs; I just don't find them very pleasing to look at.

- Line 99: I'd drop the "it was simultaneously true that".

- Line 312: I'm not sure what "nearly simply typed" means.

- Line 876: "on the facing page" -- neat bit of LaTeXing there!

Questions for the response period
---------------------------------
1. A claimed advantage over Aehlig et al. is that the trusted computing base is not grown. However, Aehlig et al. claim (page 2 of their paper) that their approach does not grow the trusted base either, "if the compiler used at runtime is the same as the one compiling the theorem prover". Could you respond to this claim?



Review #19D
===========================================================================
* Updated: 8 Sep 2021 7:11:02am AoE

Overall merit
-------------
C. Weak Reject

Reviewer expertise
------------------
Y. Knowledgeable

Paper summary
-------------
This paper presents an efficient system for optimizing code, inside the Coq
proof assistant.

The source code and the optimized code are expressed directly in Gallina,
Coq's language (a shallow embedding). The desired optimizations are expressed
as rewrite rules; each rewrite rule takes the form of a lemma, whose statement
is an equality (possibly with side conditions).

A Coq plugin accepts the rewrite rules that the user wants to exploit,
collects the toplevel identifiers that are involved in these rules, defines an
explicit (deeply-embedded) syntax for terms, sets up some magic to perform
conversion (both ways) between shallowly- and deeply-embedded syntax, and
defines a rewriting function (a variant of normalization by evaluation) which
efficiently rewrites a deeply-embedded term, until a normal form is reached.
(My description is probably inaccurate, reflecting my limited understanding of
the details.)

The end user does not see all of this machinery; she can just establish a set
of equalities (lemmas), write a program (in Gallina), and ask for the
optimized version of her program to be computed. This computation takes place
inside Coq. The optimized program is provably equal to the original program.
The trusted computing base is not enlarged; it is just Coq. Apparently, the
optimizer can also be extracted to OCaml, compiled by OCaml, and executed as a
standalone program, which is about 10x faster; however, in this scenario, I do
not understand what (source and target) language the optimizer operates on.

The system is described at a rather high level of abstraction. Many details
are omitted, which is good to some extent (we do not want to be drowned in
gory details and Coq idiosyncrasies), but the authors have perhaps gone too
far; it is difficult for the reader to understand what is going on.
Furthermore, in the beginning (Section 3), the system seems to be described as
a delta with respect to Aehlig et al.'s earlier work; this is very unhelpful
for readers who are not familiar with this work. Although I have read about
NbE before, I would much prefer to see a direct self-contained presentation of
the authors' work, relegating a discussion of the related work to a later
section in the paper. Section 3.2, which describes the compilation of pattern
matching, seems too informal to really understand what is going on. There
seems to be a nice modularity in the split between first-order rewriting
(Section 3.2, which defines the function `rewrite_head`) and the treatment of
higher-order terms (Section 3.3, which defines NbE). Unfortunately, Section
3.3 is also informal (I am not sure I can understand Figure 1 without knowing
precisely how terms are represented). This is remedied on page 11, where the
representation of terms is finally presented; I think this could be done
earlier. Ideally, the paper should show "real" code and explain, piece by
piece, so that the reader can hope to understand and imitate the results of
the paper. I don't think this is the case here. E.g., although the authors
pay attention to "subterm sharing" (dealing with `let` definitions), some
pieces seem to be missing: the `LetIn` case in `reduce` is not shown (line
560), and the "NbE strategy that outputs UnderLets telescopes" in Section 4.2
is never fully defined, as far as I can tell.

Although the optimizer is always correct, and always terminates, the question
of its completeness is never discussed. Does it always produce a normal form
with respect to the rewrite rules chosen by the user? I would imagine not, since
these rules might not even form a terminating rewrite system. They also might
not be confluent. A discussion of these problems (and how to debug them in
practice) might be welcome.

The paper contains a quite thorough experimental evaluation, containing both
micro-benchmarks (with performance measurements and comparisons against
competing approaches) and a macro-benchmark, the Fiat Cryptography compiler.
The appendices also contain a detailed analysis of the complexity bottlenecks,
which could be quite valuable to a narrow audience.

In summary, the paper's achievement seems quite impressive. The gains in
performance, in modularity, in simplicity (direct style instead of CPS) that
have been achieved should go a long way towards making a system like Fiat
Cryptography much more usable and maintainable in practice. Furthermore, on
the Coq side, the ability to automatically transform a set of equality lemmas
into an efficient, verified rewriting engine sounds great, and could open up
the way to many new applications. So, I like the results presented in the
paper very much.

However, I don't like the paper itself very much. Its presentation is often
unclear, and seems to rely on a heavy amount of background knowledge. I wish
the ideas were presented in a more self-contained way, and with sufficient
detail so that the reader can hope to reproduce the result. I feel that the
paper should almost be rewritten from scratch. For this reason, I lean towards
*not* accepting it this time, hoping that it can be much improved in the
future.

Strengths
---------
+ New approach to the (automated, modular) construction of verified
  rewriting engines in Coq.

+ Use of Coq in an industrial setting, with promising results.

Weaknesses
----------
- The presentation is not great, not self-contained enough,
  not gentle enough. The comparisons with the related work
  are very technical and come too early.

- The technical presentation is incomplete: reification is not
  described at all, I think; the use of decision trees is only
  briefly described; the treatment of `LetIn` constructs in the
  NbE algorithm is barely described; etc.

Comments for author
-------------------
lines 29-31, "most desired iteration on the compiler can be achieved through
iteration on the rewrite rules". The authors seem to suggest that all changes
to the compiler can take the form of new rewrite rules, or modifications to
existing rewrite rules. However, much of the "foresight" that a compiler
writer must display resides in the designing of appropriate intermediate
languages, separated by conceptual gaps that must be just large enough to be
interesting and small enough to be tractable; and in the ordering of these
intermediate languages. One might argue that, once the design of these ILs has
been frozen, some optimisations may become difficult to express, as they might
require extending the definition of one or more ILs, or (worse) they might
require revisiting the order in which certain transformations take place.

line 33, "Proof-producing compilers usually operate on the functional
languages of the proof assistants that they are written in". Perhaps I am
missing something, but I don't see why that is or must be the case. In fact,
there is probably a spectrum of "proof-producing compilers", where the "proof"
takes the form of a witness that can be checked by a certified verifier; the
more complex the verifier, the closer we are to a "certified" compiler, and
the simpler the verifier, the closer we are to a "certifying" compiler. When
the authors write "Proof-producing compilers usually operate [...]", which
end of the spectrum do they have in mind? (Perhaps the extreme where there
is no user-defined verifier and the witness must be a term of type Prop?)

line 60, "Their execution timed out": the execution of what?

Section 1.1, it may be a little early at this point to discuss the related
work, as I don't think it is quite clear yet exactly what problem this paper
solves. From the first 1.5 pages, I understand that the Fiat Cryptography
compiler has been improved, but it is not clear yet how this is achieved and
what are the merits or demerits of the approach presented in this paper.

line 90, "preserving subterm sharing". It is not clear what kind of sharing
the authors have in mind. Sharing of program terms, or sharing of proof terms?
Why are there important opportunities for sharing? Why cannot R_tac preserve
sharing?

line 114, "Our variant on the technique of Aehlig et al. [...]". The authors
have not yet recalled what is Aehlig et al.'s technique, nor explained how
this work differs from it, so it is difficult to appreciate the list of
claimed advantages at this point.

line 135, "we expect it to be performance-prohibitive to perform bookkeeping
operations on first-order-encoded terms". Why is that? Is it because the
authors assume that de Bruijn indices are encoded as unary numbers? Or would
the cost be prohibitive even with an efficient representation of de Bruijn
indices (either as natural integers in binary notation or as machine
integers)?

line 142, "Fiat Cryptography's arithmetic routines rely on significant
sharing". It is still not clear what is meant. If sharing matters, why
not explicitly represent it via an object-level `let` construct? Does
the difficulty stem from the fact that a shallow embedding is used,
therefore it is difficult to control whether and when Coq decides to
unfold its meta-level `let` construct?

line 178, I suppose I could have understood this earlier, but it is becoming
clear only now that the paper is about partial evaluation, and about a form
of supercompilation.

line 203, the role of `ident.eagerly` is unclear. Is it an identity function?
I assume that it serves as a marker. It would be good to point precisely to
the place in the paper where the recognition of this marker is explained. Why
is `ident.eagerly` *not* used in the lemmas `eval_combine` and `eval_length`?

lines 220-225, most of the details mentioned in this paragraph remain
mysterious. Why would "extra bottom-up passes" be needed? (Doesn't rewriting
produce a normal form?) What is a "permitted" identifier? What is `seq`? etc.

line 232, "even more pragmatic is to extract the compiler". What are the
source and target languages of the extracted compiler? This seems unclear,
since the transformations described so far transform Gallina terms to Gallina
terms.

line 243, "We are [...] but made [...]". Stylistically, I suggest sticking
with present tense everywhere. See also "a type is defined [...] they also
wrote [...]" on the next page.

The description of Aehlig et al.'s approach on page 6 is difficult to follow
(for a non-expert reader, at least). Why not describe your approach first
and contrast it with Aehlig et al.'s approach in the Related Work section?

line 290, what is "the former"?

line 292, "Automating this step allows rewrite rules to be proven in terms of
their shallow embedding". I don't know what this means.

line 312, "limited support". Limited in what ways?

line 325, "we instead build our rewriter in Coq’s logic". Not sure what this
means. Is your rewrite a program expressed in Gallina? Is it something else?

line 330, you adopt Maranget's approach, because you fear that a simpler
approach would involve "duplicate work". Have you compared the two approaches
in practice?

line 335, "There are three steps to turn a set of rewrite rules into a
functional program that takes in an expression and reduces according to the
rules." Is the goal even well-defined? What if the set of rewrite rules is not
confluent or not terminating?

line 379, what is "Coq's logic"? What is "Coq's normal partial evaluation"?

line 433, "First, [...]" but there is no "Second, [...]"?

line 440, is this an amendment to the code in Figure 1? It would be clearer
to have just one version of the code.

line 482, I don't understand the implied opposition between "closed programs"
and "thousands of lines of low-level code". One aspect is whether the program
has free variables; another aspect is the size of the program.

line 496, "it is difficult to integrate arrays soundly in a logic". This is
a naïve question, but could Coq's primitive arrays be used here?

line 504, "Here is the actual inductive definition". Could this definition
come earlier? The definition of reify & reflect would be clearer if it did.

line 517, "A good example of encoding adequacy". I believe I know what
"adequacy" means in this context, but I don't think every reader will.

line 528, "the suspicious function-abstraction clause". I believe I would
prefer to see correct code up front.

line 577, "we needed to convince Coq that the function terminates". Coq is
convinced already, if it accepts your definition. The phrasing is somewhat
odd.

line 581, "the Coq kernel is ready to run our Rewrite procedure during
checking." During checking of what? If you mean that the Coq type-checker
performs reduction during type-checking, that is a well-known fact which
perhaps does not need to be recalled here.

line 584, "These strategies have". Not clear which strategies are meant
here.

line 590, "“known-good” goals". I have no idea what this means.

line 616, "Then we modified reduction to inline let binders instead of
preserving them, at which point the reduction job terminated with an
out-of-memory error [...]". I don't really get the point. Even if the compiler
had been able to terminate, the resulting "optimized" code would be
practically unusable, as it would perform many redundant computations. Right?

line 650, why is `with_lets` needed? Is it used to implement the heuristics
discussed on line 658?

line 672, "We added a variant of pattern variable [...]". This passage is
rather obscure.

I believe the authors could perhaps better explain why the rewriting rule
at line 706 is preferable (somehow more tractable) than the rewriting rule
at line 696. At first sight, they seem similar. I guess the point is that
the variable `n` is unknown (i.e., *not* a compile-time constant) whereas
the variable `u` is known (a compile-time constant), so the second rule
falls within the scope of the technology described in Section 4.3, whereas
the first rule does not.

The `clip` trick seems to be a nice way of attaching statically computable
information to variables. It may be worth discussing how general this trick
is (can it be used to carry arbitrary information, not just an interval?)
and whether there might be other ways of achieving the same result (could
the information be stored in some sort of environment?).

lines 721 and 725, "Recall from section 2 [...]". This was never really
explained. All of Section 4.5 is relatively obscure.

line 736, "when converting the interpretation of the reified term with the
prereified term". I don't know what this means.

line 755, I don't understand the meaning of "iterating on the Fiat
Cryptography compiler".

Section 5.1.2 seems relatively uninteresting.

line 808, "designating Z.combine_at_bitwidth as an identifier that should be
inlined rather than let-bound". I am not sure at which point it should be
inlined away. Certainly not too soon, otherwise the rewrite rules mentioned
at lines 805-806 would never fire.

line 834-847, I think I understand this paragraph (more or less), but I do not
see how it explains the quartic behavior discussed in the previous paragraph.

line 852, the forward pointer to section 5.2.4 is somewhat unfortunate; the
text cannot be fully understood without looking ahead, it seems. We do not
know what the parameters `n` and `m` represents. On line 853, it is not clear
what "cubic" means (cubic in what parameter?). On line 858, what is `make`?
What is `map_dbl`? The whole paragraph (848-861) is extremely technical. It
does not clearly explain why it is the fundamentally difficult to perform
let-lifting in an efficient way. It also does not explain its opening
sentence, "Another essential pass to fuse with rewriting and partial
evaluation is let-lifting". Why is it essential? Is it more efficient
if fused than if performed separately? Why is that?

In Figure 2, is `m` in fact a known constant? Otherwise I am not sure
what the ellipsis means.

line 958 (and elsewhere), "continuation-passing style [...] allows standard
VM-based reduction to achieve good performance". Is it folklore knowledge why
writing in CPS style helps? It might be worth briefly recalling why this is so.

Questions for the response period
---------------------------------
* Could you explain how the compiler works (and is used) when it is extracted
  to OCaml code? In this scenario, what source language and target language
  does it use?
