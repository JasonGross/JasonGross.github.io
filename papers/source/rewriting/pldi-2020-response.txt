Thank you all for your feedback and comments.  We'll start by
clarifying a few most crucial elements of our approach and its
advantages, after which we'll return to answer further questions on a
review-by-review basis (some of which are forward-referenced by our
initial highlights).

First, a quick response to Review A's questions about what the user
experience is really like.  Section 1.1 does provide a complete
example; we are not eliding any additional user overhead of applying
our rewrite engine.  (Abstract interpretation is a separate activity
that we say a bit more about later, but this paper is not about
abstract interpretation, but rather a flexible framework that can
interface with abstract intepretation.)

Next: Review D revealed a number of places where we left it unclear
how we avoid growing the trusted base, beyond what one accepts anyway
in using Coq, and we're glad to have the opportunity to clarify.
Briefly:
- *Must one trust that the registered rewrite rules are correct*?
   No, the verified rewriting engine requires a proof that the
   interpretation of the AST of each rewrite rule is a correct
   proposition in the logic.
- *Must one trust the translations back and forth between normal Coq and our ASTs?*
  No, the translation *into* ASTs is inherently untrusted, as we ask
  the Coq kernel to "run it backwards" through interpretation of ASTs,
  for instance in formulating the semantic correctness condition for a
  rewrite rule. The kernel naturally checks that interpretation takes
  us back to the terms (e.g., rewrite-rule statements) we started
  with.
- *Are the central NbE function definitions (lines 701-704) trusted?*
  No, they receive full proofs of functional correctness (which are
  used as lemmas in the overall engine proof) and thus need not be
  trusted.
Summing up, in any particular use of our rewrite engine, the final
theorem statement proved truly does not mention or depend on any part
of our framework; only the *proof* depends on our framework.  Since we
do not add any axioms in this project, the trusted base is not
expanded.  And see our more detailed Review D response below for more.

Finally, a few brief remarks on performance, which indeed is at the
heart of our project's motivation. Review D expresses disappointment
that, in comparison to [11], our project is only about improving the
performance of a compilation pipeline; we should have emphasized that
the tool from [11] timed out (running longer than 100 hours) on many
important inputs, while our new version handles all known real-world
elliptic curves usually within a few minutes or at most two hours. Also,
Review A summarizes Section 5.2 as concluding that our engine's
performance is poor. We want to emphasize again that there we compare
against reduction on programs (from [11]) that have been
tailor-written in CPS just to improve reduction performance, while
our engine works on much more readable, direct-style code, in addition
to integrating flexible rewriting with reduction.  Still, our
within-Coq evaluation doesn't pass 90 seconds of running time until
primes grow to about 10 times as many bits as native machine words.

Responses to Review A
=====================

Regarding termination checking, indeed, normalization by evaluation in
PHOAS is, in some sense, specifically crafted to appease the
termination checker in an elegant way.

Broadly, none of the individual techniques are new.  What's new is
that we combine all of them to get a flexible verified tool that
operates efficiently at scale, without extending the trusted code
base.

> However, Section 5.2 concedes that actually, the performance of the
> partial evaluator running inside Coq is pretty poor.

While the performance is poor compared to the extracted version and is
not stellar compared to the "rewrite all the code in CPS" version
(which is inflexible), the performance is still quite good relative to
the prior state-of-the-art equational reasoning in Coq of
`setoid_rewrite` / `rewrite_strat`, which take around 90 seconds for
primes equal to the bitwidth, and about 3 hours for primes with twice
as many bits as the machine architecture.  By contrast, even within
Coq, our approach only approaches 90 seconds for primes with about 10
times as many bits as the machine architecture.  We'll aim to
emphasize this more in future versions of the paper.

> It seems like if we're going to make that concession, we don't need
> to worry much about the efficiency of the pattern matching engine --
> we can just delegate to OCaml's, just like [1]?

If we delegate to OCaml's pattern matching engine, then we can't do
conditional rewrite rules, because there's not currently a way to
extract Coq pattern matches to OCaml's conditional matches.
Additionally, we tried eliminating pattern-matching compilation from
our engine.  This results in out-of-memory errors during compile-time
(because we end up being quadratic in the number of rewrite rules for
reasons that are slightly hard to explain), and if we disable the
partial evaluation of rewrite rule selection (trusting in OCaml's
pattern matching engine), we lose approximately a factor of 8 in
performance of extracted code.


The intended user experience for a tool like this is detailed at the
end of section 1.1.  The couple lines of code described there are all
that is necessary for equational reasoning.  The work of abstract
interpretation was required for Fiat Cryptography to handle automatic
specialization from unbounded integers to fixed-width integers.  This
is more than just equational reasoning, and, indeed, was a somewhat
laborious process.  We have yet to find any better alternative to
this, however, when abstract interpretation is, in fact, required.


Responses to Review B
=====================

Thank you for your detailed comments about places where we were
unclear or misstating things.

Responding to the questions:

> p.2 [213] “reduction included in .. need not be requested
> explicitly.” What does it mean for a reduction to be “requested”?

Imagine a proof assistant without any computational reduction rules,
only axioms.  In such a system, every reduction step needs to
correspond to a use of an axiom in the proof term.  This is what we
mean by "requested".

> p.5 [457] “rely on no such embeddings” — it’s perhaps besides the
> point, but Coq’s extraction mechanism must rely on the semantics of
> OCaml, and probably any attempt to build a certified extraction
> procedure will need to have such embedding, right?

Yes.

> p.5 [498] “set of inductive codes” — what are these codes? Is it
> just an implementation detail?

The inductive codes are the inductive types from step (2), "Inductive
types enumerating all available primitive types and functions are
emitted."  For example, we might have the inductive codes for types
`Inductive base := nat_code | bool_code | Z_code` and the inductive
codes for functions `Inductive ident := nat_add | Z_add | bool_if`.
Details beyond this are basically just implementation details.

> p.5 [545] “does not need to look into definitions” — so this
> rewriting scheme never unfolds the definitions of symbols? Why is
> that an acceptable limitation?

The rewriter itself has no native support for unfolding definitions,
but you can include a rewrite rule that corresponds to an "unfolding",
such as `?n + S ?m = S (n + m)` or `fst (?x, ?y) = x`.

> p.6 [630] “Wildcard”. The holes are not named nor indexed. Is it
> irrelevant or are they implicitly indexed left-to-right?

Holes are indexed left-to-right, but this is an implementation detail;
the machinery for reifying Gallina rewrite rules into patterns and
replacements takes care of indexing.

> p.6 [636] “App” — I am guessing the edge in the decision tree should
> read “Switch” (as in [609])? Or is “Switch” supposed to be written
> inside the circle?

We should have been more clear that every un-labeled node is a
"Switch" node.  The "App" edges are for the `app_case` on [609], and
the identifier-labeled edges are part of `icases`.

> p.6 [654] “match e with etc.” — Where did the vectors go in this
> expression? Were they all eliminated by Coq’s Compute or similar?

Yes, they are all eliminated under `cbv` / `cbn` during the partial
evaluation described on lines [644]-[646].

> p.11 [1169] “including reification, cbv, etc.” — It is not clear how
> important these are, and whether the reader should consider them as
> part of the running time or not. Surely reification is a vital part
> of the process, so what is the significance of “only rewriting”?

The response to this is hinted at on [1120]-[1123]: "As the
reification method was not especially optimized, and there exist fast
reification methods [13]".  However, refication is not actually the
bottleneck.  The bottleneck is the final cbv at the end, and this is
only due to a bug in Coq's implementation of NbE reduction that makes
substitution quadratic in the number of
binders (we wish we could link to the bug tracker, but it might reveal
something about our identities).
While we felt it would be misleading to give only the "only rewriting"
numbers, we do believe this is really the benchmark that matters, Coq
bugs aside.  (And we hope this bug in Coq will be fixed soon.)


Responses to Review C
=====================

Thank you for your points; we will aim to incorporate your feedback in
the next revision of the paper.

One clarification we'd like to make is that the abstract interpreter
itself is beyond the scope of this project; we merely want to
demonstrate that it is possible to integrate separate abstract
interpretation passes with the partial evaluator and rewriting
framework.


Responses to Review D
=====================

> Why not implement, once and for all in Coq, the efficient rewriting
> engine algorithms, based on indexing and pattern match automata,
> that the term rewriting community has developed over the years and
> implemented in truly fast rewrite engines, performing millions, or
> even tens of millions, or rewrite steps per second?  That would
> benefit the entire Coq community.

Indeed, this would be a great next step.  We started with a somewhat
more modest goal, and already found the proof effort significant.

> Why should one trust the rewrite rules as correct? Don't they become
> part of the trust base anyway?

To be accepted by the engine, the rewrite rules must be proven in Coq.
So no unsound rewrites are permitted (unless you use unsound axioms to
prove them). The checking happens as part of the step from line 502 of
the paper, where we bring all the rewrite rules together in a list of
dependently typed records, each one pairing the syntax of a rewrite
rule with a required proof of its soundness, stated in terms of
interpretation of the reified syntax tree.

> Also, isn't your term translation back and forth to Coq the same as
> like giving a semantics to an ML-like language like in [1]?  One
> still has to trust these translations, including the fact that the
> ML-like language and Coq share the semantics of their similar
> constructs., which to me counts as the same level of trust that [1]
> requires from their users.  You should clarify that in the paper.

The translations in our tool are *not* trusted. This is because every
time our code reifies a term, the Coq kernel checks that the embedded
AST our code produced has the same semantics as the original term by
unfolding the interpreter applied to the AST and observing that it
results in the original term. Our rewriting transformation is proven to
be semantics-preserving, so the theorem we get at the end is that the
new term, post-rewriting, has the same semantics as the original term.
Since the kernel checks the correctness of the semantics of the initial
term, there's nothing additional here to trust. We should perhaps
clarify this in the paper.

> Lines 701-704: this becomes part of the trust base, no?

Thankfully, it does not.  In this case, reify and reflect are internal
operations which are not exposed in the end-to-end theorem (any moreso
than code for CompCert's intermediate phases appears in a generated
theorem witnessing correct compilation of a concrete C program), and
we prove the properties that we need of these operations.

> How is the approach in [11] different from the approach here?

> Is it only a performance improvement?

[11] relies on a delicate ad-hoc combination of hand-writing all of
the code in CPS to take advantage of built-in reduction for partial
evaluation, and then using a hand-rolled domain-specific rewriter to do
the necessary algebraic simplifications. The authors of [11] describe
the need to write all algorithms in CPS as "arbitrary and unfortunate."
Further, this two-stage approach requires reifying large blocks of code,
and reifying repeatedly for every prime, which was prohibitively slow in
many cases ([11] Appendix B).

The approach described here lets us reify the generic code once, and
combine all of the rewriting and partial evaluation into the same
generic framework. We believe this is much easier to use. Probably the
clearest example of this is that our rewriting and partial evaluation
can be applied to code that was written without considering our tool as
a target, whereas the code repository accompanying [11] contains two
versions of many functions: one human-readable, and one for partial
evaluation (which they prove equivalent each time).

The performance improvement is also essential, in the sense that some of
the primes that did not finish even after waiting nearly a thousand
hours in [11] finished for us in seconds, minutes, or, at most, an hour
or two.

> How about the trust-base?

Running our rewriter in Coq has the same trusted base as [11].  The
faster version of our code also relies on extraction to OCaml, but this
is not essential.

> I was a bit disappointed to read that you generate essentially the
> same code as [11] and you only care about the performance of the
> partial evaluator; it would have been a really nice application
> where correctness is guaranteed through Coq, but if the application
> already had all these guarantees, then the work in this paper feels
> less critical and more incremental.

Again, we would like to emphasize that many primes could not be
handled at all by [11] even with hundreds of hours and 64 GB of RAM.
The "performance improvement" here allows us to not just handle the
same primes faster, but also allows us to handle primes that could not
be handled at all.
