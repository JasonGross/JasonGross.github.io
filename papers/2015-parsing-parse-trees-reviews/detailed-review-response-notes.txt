Reviewer A

> Splitting oracles: the essential algorithmic component of parsing is
  the association of the input into meaningful trees. Deferring the
  split choice to an "oracle" means that all non-trivial computation
  is in it. So I would not say that you're performing parsing.

This is a good point.  We are intentionally not doing anything interesting with regard to algorithmic parsing theory; that is reserved for future papers on currently in-progress work on synthesizing efficient parsers.  We picked the simplest, most obvious parsing algorithm we could think of, as a starting point.  That you claim our algorithm contains only trivial computation speaks to our success in picking a simple starting point.  Allow me to add a trivial splitter, which allows us to really perform parsing, trivially: the brute-force splitter, which returns the sequence of numbers [0, 1, ..., length of string] is sufficient for correctness, because we filter the list of splits to accept only valid ones.  We believe that the idea of parsing parse trees is complicated enough as it stands, and does not need extra complexity in exposition introduced by chosing a non-trivial parsing algorithm as a starting point.

> The work that remains appear to be mostly about detection of
  recursion in the grammar, which is easier than the above step.

Certainly not easier, when good performance is not demanded (as it is not, for code that is never run and only compiled).  It is straightforward, bordering on trivial, to prove that returning every possible split of the string does not miss any splits (and is thus complete), and it does not add any complexity to the completeness proof to do it this way.

> The textbook approach to detect loops is by reduction to Chomsky
  Normal Form. (This can be done statically on the grammar.) In
  comparison your approach feels awkward. At the minimum, discuss
  your choice!

This seems more complicated, but I am not sure.  Computing the Chompsky Normal Form of a grammar in a manifestly terminating way, proving that every grammar has a Chompsky Normal Form, that a grammar has a parse iff its CNF has a parse, this seems more complicated than proving that our way of keeping track of which nonterminals have been seen does not lose us any valid parses.  Perhaps it is not more complicated, but it seems less ovbious, and not obviously simpler nor more straightforward.

> I don't think that your interface to the parser is sensible. Why is
  it so long (fig 1)? In my mind I should just supply the grammar
  rules. (And perhaps your oracle.) I can also not reconcile your
  explanation (reading it off the minimal parse tree) with fig 1.

The interface is not a user-facing one; it is the internal one, to be partially instantiated to generate minimal parse trees, or parse trees, or booleans.  Once you fix the type of the return, indeed, all that is left is to specify the rules of the grammar, and, perhaps, the splitter.  The length of the interface results from the fact that the algorithm, a priori, knows nothing about minimal parse trees; the interface needs to have enough generality to specify exactly how to build a minimal parse tree, and how to prove that no such parse tree exists.  Hence it has enough cases for all the constructors, and enough cases to prove absurdity from impossible parse trees.

>   1. Define a "ParseTreeOf nt s" (the spec)
  2. Write an algorithm to decide the relation:

     parse : (nt : NT) -> (s : String) -> Decide (MinParseTreeOf ns t)

  And to get to the ParseTreeOf version only the de-loop lemma is
  needed (in one direction).

The de-loop lemma would need to be strengthened to do this; the current version only de-loops at one level, which is not enough to get ParseTreeOf -> MinParseTreeOf.  When proving correctness of the parser returning booleans directly, the full delooping construction is longer than the entire rest of the soundness and completeness proofs, combined!  Note that the full delooping construction includes the generation of ParseTreeOf from MinParseTreeOf as a way of getting well-foundedness evidence.



Reviewer B
> I had problems in matching the paper to the accompanying Coq codes.

I agree, this is a problem.  The accompanying code is a half-assed proof-of-concept, not an elegant match for the description.

I agree, this is a problem.  We did not find enough time to clean up the code.  It was an explicit goal in writing the paper to hide as many of the warts of our code-base as we could: long names, long lines, costly mistakes that we didn't have time to fix, unenlightening details forced on us by Coq.

> E.g., grep could not find has_parse or min_parse -- I was interested in a proof of min_parse.)

We apologize.  [min_parse] is named [minimal_parse_nonterminal__of__parse] in our code base.  We should have done more to make the code match the paper, and intend to do so, should the paper be accepted.
[FIXME: Adam: Is this okay to promise?]

> I miss technical comparisons
with the work by Ridge in HOL4. The authors say their approach is different from Ridge's in that they use dependent types which have a different aesthetic appeal 
-- the statement is too general to be informative.

FIXME: How to respond?



Reviewer C:
> what I was wanting to see was a set of versions of increasing complexity (with diffs indicated) that gave more of an indication of the way in which the development had proceeded.

To a first approximation, there is the code in 1.1, the slightly more complicated code in 1.2, and the fuller code of Figure 2.  Perhaps we could have indicated diffs more clearly, or rewritten 1.1 and 1.2 when it came time to introduce Figure 2, perhaps indicating which bits were already there in 1.1, which bits where there to prevent infinite regress from 1.2, and which bits are just there to handle the more complicated type signature.  In the actual code, there is BooleanRecognizer.v, which corresponds to the grammar-independent version of the code in 1.2 (equivalently, to the code in Figure 2 specialized to booleans), while DependentlyTyped.v gives the full code of the interface and implementation of Figures 1 and 2.

> there is a substantial amount to do on defining splitters and their effect on performance (more discussion on splitters below); this is reported as planned work;

Yes, this paper is about parsers that can prove their own correctness.  We should have mentioned that we believe this can be extended to other parsing algorithms.

> when splitting strings, this is not the case and it was not clear (at least to me) that for every grammar a splitter of the right kind exists

The brute-force splitter, which returns the sequence of numbers [0, 1, ..., length of string] is sufficient for correctness, because we filter the list of splits to accept only valid ones. 

> did editors and any other tools support the development, or not?

We used ProofGeneral, with it's interactive display of coqtop feedback, syntax highlighting, and saving capabilities.  But most of these features are built into coqtop, and are assumed when developing Coq code; an interactive compiler seems essential when developing proof scripts and dependently typed code.  Is it worth mentioning explicitly?


Reviewer D

> The biggest criticism is about the code. The presented code in the
paper is not intact with the real development which makes it
impossible to really see if the proposed approach is as simple and
elegant as authors claim. I was unable to find functions like
"has_parse", "has_no_parse", "min_parse", "parse_full", 
"parse_sound", "parse_complete", etc.

Yes, this needs to be fixed, thank you for the suggestions.

> You are using provocative statement that parser is proving its
correctness and this is not what is happening.

I am tempted to try to do this precisely, rather than approximately, as I suggest in the last section.

> At page 2 you say that your strategy works only with finite sets of
nonterminals.  However, set of nonterminals could be infinite, but
then you should add the step of extracting the "used" nonterminals
from the grammar.

Consider A := B, B := C, C := D, and so on, ad infinitum.  There is no finite set of "used" nonterminals to be extracted, not even a decision strategy to see if it terminates.  The strategy really only works for manifestly finite sets of explicitly given "used nonterminals", which is what is meant by "finite sets of nonterminals".

> In subsection 2.3 it would be nice if you would make precise which
parse tree is returned by the parser for ambiguous grammars. In that
sense your notion of "completeness" is very special.

It depends on the splitter.  It returns the first loopless parse tree that the splitter provides it.  It would be relatively easy to make it return all parse trees, roughly by replacing a "fold" with a "flat_map".