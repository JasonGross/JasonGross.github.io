===========================================================================
                           PLDI '17 Review #39C
---------------------------------------------------------------------------
Paper #39: Systematic Synthesis of Elliptic Curve Cryptography
           Implementations
---------------------------------------------------------------------------

                 Reviewer expertise: Y. Knowledgeable
                      Overall merit: C. Weak paper, though I will not fight
                                        strongly against it

                           ===== Strengths =====

- framework for synthesis of efficient code for elliptic curve cryptography optimized at low level for 
efficient use of CPU registers; it is written in Coq

- this framework should enable easy generation of optimized implementation for any set of specific
parameters which change the field that is used and the representation of elements in it

- the authors provide evaluation with the EdDSA signature algorithm and demonstrate that their
synthesis tool would avoid several existing bugs in knows implementations of the algorithm

                          ===== Weaknesses =====

- the presentation of the paper does not clearly present the actual contributions and optimization techniques
that are leveraged in the synthesis tool

- the synthesized code for EdDSA is slower than some manually optimized implementations.

                      ===== Comments to authors =====

Cryptographic operation are often some of the most computationally expensive primitives in existing implementations 
for secure an authenticated communication. That is why optimizing implementation of cryptographic algorithms is an
important task especially when these algorithms will be used many times in the execution of an application. The goal
of this paper is to construct a synthesis tool that starts from a high level specification of a cryptographic algorithm and
provides highly optimized low level specification of an implementation for a specific set of parameters. This tool will replace
a lot of manual effort for optimizing implementations and will facilitate the instantiation of a particular algorithm with
many different parameter settings, which may require different representation of the underlying structures. 

The main focus of the work is to build a proof-generating synthesis for big-integer representation and arithmetic. For the
representation of big integers the paper looks at a decomposition of the value into digits where each digit is a different base,
i.e. it carries different numbers of bits information. In the paper these are called limbs and limb width. Since each limb will
be stored in a separate register the difference between the width of the register and the limb width can remove the need to carry
bits across registers at each computation step, hence can allow parallel execution. Reasoning about the need of carry bits and
how information should be distributed across different limbs can be kept in carry chain, which can be applied all at once.
These, however, creates issues related several possible representations for the same number. The authors point out that efficient
conversion to a canonical form is complicated but it remains unclear to me how they handle this issue. In fact it is also not quite
clear how they reason about carry chains and use these tools together with the settings of limb length in their optimization 
techniques. 

Section four in paper which describes how low level code is synthesized is less than two pages long and I wish it provided
greater level of detail. For example, the paragraph on partial evaluation mentions how a recursive multiplication becomes a
chain of simple operations, but it remains unclear whether this is all that it is used for. The authors mention that they had further
engineering challenges without much elaborations, which makes it hard to evaluate the contribution.The rest of the section talks
about bound checking; at one point there is a mention that since guessing correct bounds is hard they will be given as input, does
 this mean that the bounds need to be manually computed? This section also mentions double word operations that allow to construct operations on n-bit integers from operations on n/2 bit integers as well as Barrett and Montgomery form reductions which allow for efficient modulo reductions. I assume that these techniques were all incorporated as synthesis rules but I wish I 
saw some discussion when they were useful, did they constitute all major techniques for improvement of modular operations, how 
different components fit together.

The case study for the synthesis tools is the EdDSA signature algorithm. The authors analyzed deployed software with different
implementation of the algorithms as well as several functions-correctness bugs that have been identified in these implementations.
They argue that their synthesis tools provides provisions that guarantee that the produced implementation specification could not
have contained the same types of bugs. The verification that tool from this paper offers was used to discover subtle issue in a
proposed modification the Barrett-style modulo reduction. It also has identified an issue with an implementation of the algorithm 
with a particular curve that has points of different orders while some computation optimizations work only using points with one 
of the orders. In the evaluation the authors also compare the efficiency of the code produced by the synthesis tool
compared to other existing implementations. From the results presented in Table 2 and 3 it seems that the tool produces code
for Diffie-Hellman handshake that runs better than some other implementations while it is still slower than the best implementations. In the case EdDSA signing the code from the tool seems to be substantially slower than other implementation 
but I did not find much discussion about this in the paper.

I think the goal of this paper of facilitating the synthesis of optimized and verified code specifications for cryptographic algorithms
is very important. The evaluations results for efficiency seem concerning since they do not seem to be on par with other 
implementation while the main motivation for the work is improving the efficiency of existing implementations without requiring
manual efforts.
